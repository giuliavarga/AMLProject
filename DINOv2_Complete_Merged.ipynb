{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93279fdf",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup & Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and configure paths\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"✓ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"✓ Running locally\")\n",
    "\n",
    "# Set up paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = '/content/AMLProject'\n",
    "    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "\n",
    "# Create necessary directories\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'dinov2')\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "\n",
    "for directory in [CHECKPOINT_DIR, OUTPUT_DIR, MODEL_DIR, DATA_ROOT]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import subprocess\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "packages = [\n",
    "    'torch',\n",
    "    'torchvision', \n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'opencv-python',\n",
    "    'pillow',\n",
    "    'scipy',\n",
    "    'tqdm',\n",
    "    'pandas',\n",
    "    'scikit-learn'\n",
    "]\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', '--upgrade'] + packages)\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141df073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import importlib\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70efda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset path and import SPair dataset class\n",
    "repo_path = os.path.join(PROJECT_ROOT, 'SD4Match')\n",
    "\n",
    "if not os.path.exists(repo_path):\n",
    "    print(\"WARNING: SD4Match repository not found!\")\n",
    "    print(\"Please ensure the SD4Match repository is cloned in the project directory.\")\n",
    "else:\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "    print(f\"✓ SD4Match path added: {repo_path}\")\n",
    "\n",
    "# Import dataset class\n",
    "try:\n",
    "    module = importlib.import_module(\"dataset.spair\")\n",
    "    SPairDataset = getattr(module, \"SPairDataset\")\n",
    "    print(\"✓ SPairDataset class imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Import Error: {e}\")\n",
    "    print(\"Make sure SD4Match repository is properly cloned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8702dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SPair-71k dataset (if not already present)\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm import tqdm as tqdm_requests\n",
    "\n",
    "data_path = os.path.join(DATA_ROOT, 'SPair-71k')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading SPair-71k dataset...\")\n",
    "    url = \"http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
    "    tar_path = os.path.join(DATA_ROOT, 'SPair-71k.tar.gz')\n",
    "    \n",
    "    # Download with progress bar\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(tar_path, 'wb') as f, tqdm_requests(\n",
    "        desc='Downloading',\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = f.write(data)\n",
    "            pbar.update(size)\n",
    "    \n",
    "    print(\"\\nExtracting...\")\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        tar.extractall(DATA_ROOT)\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(tar_path)\n",
    "    print(\"✓ Extraction complete\")\n",
    "else:\n",
    "    print(f\"✓ SPair-71k dataset already exists at {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for dataset\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        class DatasetConfig:\n",
    "            def __init__(self):\n",
    "                self.ROOT = DATA_ROOT\n",
    "                self.NAME = 'spair'\n",
    "                self.CATEGORY = 'cat'\n",
    "                self.SIZE = 224\n",
    "                self.IMG_SIZE = 224\n",
    "                self.MEAN = [0.485, 0.456, 0.406]\n",
    "                self.STD = [0.229, 0.224, 0.225]\n",
    "        self.DATASET = DatasetConfig()\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"✓ Configuration created\")\n",
    "print(f\"Dataset root: {cfg.DATASET.ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedd96a",
   "metadata": {},
   "source": [
    "## Section 2: DINOv2 Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv2 ViT-B/14 model\n",
    "print(\"Loading DINOv2 ViT-B/14 model...\")\n",
    "\n",
    "try:\n",
    "    # Try loading from torch hub\n",
    "    dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "    dinov2_model = dinov2_model.to(device)\n",
    "    dinov2_model.eval()\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully!\")\n",
    "    print(f\"Model type: DINOv2 ViT-B/14\")\n",
    "    print(f\"Feature dimension: {dinov2_model.embed_dim}\")\n",
    "    print(f\"Patch size: 14x14\")\n",
    "    print(f\"Image size: 224x224 → {224//14}x{224//14} = 256 patches\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading DINOv2: {e}\")\n",
    "    print(\"Make sure you have internet connection for the first load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efd03e",
   "metadata": {},
   "source": [
    "## Section 3: Feature Extractor Class\n",
    "\n",
    "DINOv2 extracts dense patch features that capture semantic information. We'll create a feature extractor class that:\n",
    "- Extracts 16×16 grid of features (patch tokens)\n",
    "- Handles coordinate mapping between image and feature space\n",
    "- Provides L2-normalized features for similarity computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc26a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract dense spatial features from DINOv2 for correspondence matching.\n",
    "    \n",
    "    Features:\n",
    "    - Input: 224×224 RGB images\n",
    "    - Output: 16×16×768 feature maps (for ViT-B/14)\n",
    "    - Features are L2-normalized for cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Feature dimensions\n",
    "        self.patch_size = 14\n",
    "        self.img_size = 224\n",
    "        self.feat_h = self.feat_w = self.img_size // self.patch_size  # 16\n",
    "        \n",
    "    def extract_features(self, img):\n",
    "        \"\"\"\n",
    "        Extract dense feature map from an image.\n",
    "        \n",
    "        Args:\n",
    "            img: PIL Image or tensor [3, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            features: torch.Tensor [1, feat_h*feat_w, dim]\n",
    "            features_2d: torch.Tensor [1, dim, feat_h, feat_w]\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        else:\n",
    "            img_tensor = img.unsqueeze(0).to(self.device) if img.dim() == 3 else img.to(self.device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            # Get patch tokens (excluding CLS token)\n",
    "            features_dict = self.model.forward_features(img_tensor)\n",
    "            features = features_dict['x_norm_patchtokens']  # [1, num_patches, dim]\n",
    "            \n",
    "            # Reshape to spatial grid\n",
    "            B, N, D = features.shape\n",
    "            features_2d = features.reshape(B, self.feat_h, self.feat_w, D)\n",
    "            features_2d = features_2d.permute(0, 3, 1, 2)  # [1, dim, feat_h, feat_w]\n",
    "            \n",
    "            # L2 normalize for cosine similarity\n",
    "            features = F.normalize(features, p=2, dim=-1)\n",
    "            features_2d = F.normalize(features_2d, p=2, dim=1)\n",
    "        \n",
    "        return features, features_2d\n",
    "    \n",
    "    def extract_keypoint_features(self, img, keypoints):\n",
    "        \"\"\"\n",
    "        Extract features at specific keypoint locations.\n",
    "        \n",
    "        Args:\n",
    "            img: PIL Image or tensor\n",
    "            keypoints: numpy array [N, 2] in image coordinates (x, y)\n",
    "            \n",
    "        Returns:\n",
    "            kp_features: torch.Tensor [N, dim]\n",
    "        \"\"\"\n",
    "        _, features_2d = self.extract_features(img)  # [1, dim, feat_h, feat_w]\n",
    "        \n",
    "        # Map image coordinates to feature coordinates\n",
    "        feat_coords = self.map_coords_to_features(keypoints)\n",
    "        \n",
    "        # Extract features using bilinear interpolation\n",
    "        kp_features = []\n",
    "        for x, y in feat_coords:\n",
    "            if 0 <= x < self.feat_w and 0 <= y < self.feat_h:\n",
    "                # Use bilinear interpolation for sub-pixel accuracy\n",
    "                x0, y0 = int(np.floor(x)), int(np.floor(y))\n",
    "                x1, y1 = min(x0 + 1, self.feat_w - 1), min(y0 + 1, self.feat_h - 1)\n",
    "                \n",
    "                # Interpolation weights\n",
    "                wx = x - x0\n",
    "                wy = y - y0\n",
    "                \n",
    "                # Bilinear interpolation\n",
    "                feat = (1 - wx) * (1 - wy) * features_2d[0, :, y0, x0] + \\\n",
    "                       wx * (1 - wy) * features_2d[0, :, y0, x1] + \\\n",
    "                       (1 - wx) * wy * features_2d[0, :, y1, x0] + \\\n",
    "                       wx * wy * features_2d[0, :, y1, x1]\n",
    "                \n",
    "                kp_features.append(feat)\n",
    "            else:\n",
    "                # Out of bounds - use zero vector\n",
    "                kp_features.append(torch.zeros(features_2d.shape[1], device=self.device))\n",
    "        \n",
    "        kp_features = torch.stack(kp_features)\n",
    "        kp_features = F.normalize(kp_features, p=2, dim=-1)\n",
    "        \n",
    "        return kp_features\n",
    "    \n",
    "    def map_coords_to_features(self, coords):\n",
    "        \"\"\"\n",
    "        Map image coordinates to feature map coordinates.\n",
    "        \n",
    "        Args:\n",
    "            coords: numpy array [N, 2] in image space (x, y)\n",
    "            \n",
    "        Returns:\n",
    "            feat_coords: numpy array [N, 2] in feature space\n",
    "        \"\"\"\n",
    "        scale_x = self.feat_w / self.img_size\n",
    "        scale_y = self.feat_h / self.img_size\n",
    "        \n",
    "        feat_coords = coords.copy()\n",
    "        feat_coords[:, 0] = coords[:, 0] * scale_x\n",
    "        feat_coords[:, 1] = coords[:, 1] * scale_y\n",
    "        \n",
    "        return feat_coords\n",
    "    \n",
    "    def map_features_to_coords(self, feat_coords):\n",
    "        \"\"\"\n",
    "        Map feature coordinates back to image space.\n",
    "        \n",
    "        Args:\n",
    "            feat_coords: numpy array [N, 2] in feature space\n",
    "            \n",
    "        Returns:\n",
    "            img_coords: numpy array [N, 2] in image space (x, y)\n",
    "        \"\"\"\n",
    "        scale_x = self.img_size / self.feat_w\n",
    "        scale_y = self.img_size / self.feat_h\n",
    "        \n",
    "        img_coords = feat_coords.copy()\n",
    "        img_coords[:, 0] = feat_coords[:, 0] * scale_x\n",
    "        img_coords[:, 1] = feat_coords[:, 1] * scale_y\n",
    "        \n",
    "        return img_coords\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = DINOv2FeatureExtractor(dinov2_model, device)\n",
    "print(\"✓ Feature extractor created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c8ef9",
   "metadata": {},
   "source": [
    "## Section 4: Test Feature Extraction with Visualization\n",
    "\n",
    "Let's test the feature extractor on a sample image pair and visualize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "print(\"Loading SPair-71k test dataset...\")\n",
    "try:\n",
    "    dataset = SPairDataset(cfg, 'test', 'cat')\n",
    "    print(f\"✓ Successfully loaded {len(dataset)} test pairs\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Make sure dataset is properly extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample and extract features\n",
    "sample_idx = 0\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "print(f\"Extracting features for sample {sample_idx}...\")\n",
    "\n",
    "# Get images and keypoints\n",
    "src_img = sample['src_img']\n",
    "tgt_img = sample['trg_img']\n",
    "src_kps = sample['src_kps']\n",
    "tgt_kps = sample['trg_kps']\n",
    "\n",
    "print(f\"Source keypoints shape: {src_kps.shape}\")\n",
    "print(f\"Target keypoints shape: {tgt_kps.shape}\")\n",
    "\n",
    "# Extract dense features\n",
    "src_features, src_features_2d = feature_extractor.extract_features(src_img)\n",
    "tgt_features, tgt_features_2d = feature_extractor.extract_features(tgt_img)\n",
    "\n",
    "print(f\"\\nFeature shapes:\")\n",
    "print(f\"Source features: {src_features.shape}\")  # [1, 256, 768]\n",
    "print(f\"Source features 2D: {src_features_2d.shape}\")  # [1, 768, 16, 16]\n",
    "print(f\"Target features: {tgt_features.shape}\")\n",
    "print(f\"Target features 2D: {tgt_features_2d.shape}\")\n",
    "\n",
    "# Denormalize function for visualization\n",
    "def denorm_show(img_tensor):\n",
    "    img = img_tensor.permute(1, 2, 0).numpy()\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    return np.clip(img, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images with keypoints\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Source image\n",
    "axes[0].imshow(denorm_show(src_img))\n",
    "valid_src_kps = src_kps[src_kps[:, 0] >= 0]\n",
    "axes[0].scatter(valid_src_kps[:, 0], valid_src_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3, label='Keypoints')\n",
    "axes[0].set_title(f'Source Image\\n{len(valid_src_kps)} keypoints', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "axes[0].legend()\n",
    "\n",
    "# Target image\n",
    "axes[1].imshow(denorm_show(tgt_img))\n",
    "valid_tgt_kps = tgt_kps[tgt_kps[:, 0] >= 0]\n",
    "axes[1].scatter(valid_tgt_kps[:, 0], valid_tgt_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3, label='Keypoints')\n",
    "axes[1].set_title(f'Target Image\\n{len(valid_tgt_kps)} keypoints', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'sample_images_with_keypoints.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualization saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b67112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DINOv2 features using PCA\n",
    "print(\"Visualizing DINOv2 features with PCA...\")\n",
    "\n",
    "# Get features as numpy arrays\n",
    "src_feat_np = src_features[0].cpu().numpy()  # [256, 768]\n",
    "tgt_feat_np = tgt_features[0].cpu().numpy()  # [256, 768]\n",
    "\n",
    "# Apply PCA to reduce to 3 components for RGB visualization\n",
    "pca = PCA(n_components=3)\n",
    "src_pca = pca.fit_transform(src_feat_np)  # [256, 3]\n",
    "tgt_pca = pca.transform(tgt_feat_np)  # [256, 3]\n",
    "\n",
    "# Reshape to spatial grid\n",
    "src_pca_img = src_pca.reshape(16, 16, 3)\n",
    "tgt_pca_img = tgt_pca.reshape(16, 16, 3)\n",
    "\n",
    "# Normalize to [0, 1] for visualization\n",
    "src_pca_img = (src_pca_img - src_pca_img.min()) / (src_pca_img.max() - src_pca_img.min())\n",
    "tgt_pca_img = (tgt_pca_img - tgt_pca_img.min()) / (tgt_pca_img.max() - tgt_pca_img.min())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Original images\n",
    "axes[0, 0].imshow(denorm_show(src_img))\n",
    "axes[0, 0].set_title('Source Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(denorm_show(tgt_img))\n",
    "axes[0, 1].set_title('Target Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# PCA visualizations\n",
    "axes[1, 0].imshow(src_pca_img)\n",
    "axes[1, 0].set_title(f'Source DINOv2 Features (PCA)\\nExplained variance: {pca.explained_variance_ratio_.sum():.2%}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(tgt_pca_img)\n",
    "axes[1, 1].set_title(f'Target DINOv2 Features (PCA)\\nExplained variance: {pca.explained_variance_ratio_.sum():.2%}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'pca_feature_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ PCA captures {pca.explained_variance_ratio_.sum():.2%} of feature variance\")\n",
    "print(\"Colors represent semantic regions learned by DINOv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute patch-to-patch similarity matrix\n",
    "print(\"Computing patch-to-patch similarity...\")\n",
    "\n",
    "# Normalize features for cosine similarity\n",
    "src_feat_norm = F.normalize(torch.from_numpy(src_feat_np), dim=1)  # [256, 768]\n",
    "tgt_feat_norm = F.normalize(torch.from_numpy(tgt_feat_np), dim=1)  # [256, 768]\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = torch.mm(src_feat_norm, tgt_feat_norm.t()).numpy()  # [256, 256]\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Source image\n",
    "axes[0].imshow(denorm_show(src_img))\n",
    "axes[0].scatter(valid_src_kps[:, 0], valid_src_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3)\n",
    "axes[0].set_title(f'Source Image\\n({len(valid_src_kps)} keypoints)', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Similarity heatmap\n",
    "im = axes[1].imshow(similarity_matrix, cmap='hot', aspect='auto')\n",
    "axes[1].set_title(f'Patch-to-Patch Similarity\\n(DINOv2 Features)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Target Patches')\n",
    "axes[1].set_ylabel('Source Patches')\n",
    "plt.colorbar(im, ax=axes[1], label='Cosine Similarity')\n",
    "\n",
    "# Target image\n",
    "axes[2].imshow(denorm_show(tgt_img))\n",
    "axes[2].scatter(valid_tgt_kps[:, 0], valid_tgt_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3)\n",
    "axes[2].set_title(f'Target Image\\n({len(valid_tgt_kps)} keypoints)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'similarity_heatmap.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSimilarity statistics:\")\n",
    "print(f\"  Mean: {similarity_matrix.mean():.4f}\")\n",
    "print(f\"  Max: {similarity_matrix.max():.4f}\")\n",
    "print(f\"  Min: {similarity_matrix.min():.4f}\")\n",
    "print(f\"  High similarity (>0.9): {(similarity_matrix > 0.9).sum()} patch pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1de480",
   "metadata": {},
   "source": [
    "## Section 5: Correspondence Matcher\n",
    "\n",
    "Now let's implement the correspondence matcher that finds matching keypoints between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac370531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrespondenceMatcher:\n",
    "    \"\"\"\n",
    "    Find correspondences between source and target images using feature similarity.\n",
    "    \n",
    "    Methods:\n",
    "    - Nearest Neighbor (NN): Find target point with highest similarity\n",
    "    - Mutual Nearest Neighbors (MNN): Enforce bidirectional consistency\n",
    "    - Ratio Test: Reject ambiguous matches (Lowe's ratio test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mutual_nn=False, ratio_threshold=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mutual_nn: If True, only keep mutual nearest neighbors\n",
    "            ratio_threshold: If set, apply ratio test (e.g., 0.8)\n",
    "        \"\"\"\n",
    "        self.mutual_nn = mutual_nn\n",
    "        self.ratio_threshold = ratio_threshold\n",
    "    \n",
    "    def match_keypoints(self, src_features, tgt_features_2d, src_keypoints, \n",
    "                       feature_extractor):\n",
    "        \"\"\"\n",
    "        Find correspondences for source keypoints in target image.\n",
    "        \n",
    "        Args:\n",
    "            src_features: Source keypoint features [N, dim]\n",
    "            tgt_features_2d: Target dense features [1, dim, H, W]\n",
    "            src_keypoints: Source keypoint coordinates [N, 2]\n",
    "            feature_extractor: Feature extractor instance\n",
    "            \n",
    "        Returns:\n",
    "            pred_keypoints: Predicted target coordinates [N, 2]\n",
    "            confidences: Match confidence scores [N]\n",
    "        \"\"\"\n",
    "        N = src_features.shape[0]\n",
    "        _, D, H, W = tgt_features_2d.shape\n",
    "        \n",
    "        # Reshape target features to [H*W, D]\n",
    "        tgt_features_flat = tgt_features_2d.reshape(D, H * W).t()  # [H*W, D]\n",
    "        \n",
    "        # Compute similarity: [N, H*W]\n",
    "        similarities = torch.mm(src_features, tgt_features_flat.t())\n",
    "        \n",
    "        # Find best matches\n",
    "        max_sims, max_indices = similarities.max(dim=1)\n",
    "        \n",
    "        # Apply ratio test if specified\n",
    "        if self.ratio_threshold is not None:\n",
    "            # Get second best matches\n",
    "            sorted_sims, _ = similarities.sort(dim=1, descending=True)\n",
    "            ratios = sorted_sims[:, 0] / (sorted_sims[:, 1] + 1e-8)\n",
    "            \n",
    "            # Mark low-confidence matches\n",
    "            valid_mask = ratios > self.ratio_threshold\n",
    "            max_sims = max_sims * valid_mask.float()\n",
    "        \n",
    "        # Convert flat indices to 2D coordinates\n",
    "        pred_y = (max_indices // W).float()\n",
    "        pred_x = (max_indices % W).float()\n",
    "        pred_coords_feat = torch.stack([pred_x, pred_y], dim=1).cpu().numpy()\n",
    "        \n",
    "        # Map to image coordinates\n",
    "        pred_keypoints = feature_extractor.map_features_to_coords(pred_coords_feat)\n",
    "        confidences = max_sims.cpu().numpy()\n",
    "        \n",
    "        # Apply mutual nearest neighbors if specified\n",
    "        if self.mutual_nn:\n",
    "            # Find reverse matches (target → source)\n",
    "            reverse_sims = similarities.t()  # [H*W, N]\n",
    "            _, reverse_indices = reverse_sims.max(dim=1)\n",
    "            \n",
    "            # Check mutual consistency\n",
    "            forward_indices = max_indices.cpu().numpy()\n",
    "            reverse_map = reverse_indices.cpu().numpy()\n",
    "            \n",
    "            for i in range(N):\n",
    "                target_idx = forward_indices[i]\n",
    "                if reverse_map[target_idx] != i:\n",
    "                    # Not mutually consistent\n",
    "                    confidences[i] = 0.0\n",
    "        \n",
    "        return pred_keypoints, confidences\n",
    "    \n",
    "    def match_images(self, src_img, tgt_img, src_keypoints, feature_extractor):\n",
    "        \"\"\"\n",
    "        Complete matching pipeline for an image pair.\n",
    "        \n",
    "        Args:\n",
    "            src_img: Source image tensor\n",
    "            tgt_img: Target image tensor\n",
    "            src_keypoints: Source keypoint coordinates [N, 2]\n",
    "            feature_extractor: Feature extractor instance\n",
    "            \n",
    "        Returns:\n",
    "            pred_keypoints: Predicted target keypoints [N, 2]\n",
    "            confidences: Match confidence scores [N]\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        src_kp_features = feature_extractor.extract_keypoint_features(src_img, src_keypoints)\n",
    "        _, tgt_features_2d = feature_extractor.extract_features(tgt_img)\n",
    "        \n",
    "        # Match\n",
    "        pred_keypoints, confidences = self.match_keypoints(\n",
    "            src_kp_features, tgt_features_2d, src_keypoints, feature_extractor\n",
    "        )\n",
    "        \n",
    "        return pred_keypoints, confidences\n",
    "\n",
    "# Create matcher\n",
    "matcher = CorrespondenceMatcher(mutual_nn=False, ratio_threshold=None)\n",
    "print(\"✓ Correspondence matcher created\")\n",
    "print(f\"  Mutual NN: {matcher.mutual_nn}\")\n",
    "print(f\"  Ratio test: {matcher.ratio_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136c491",
   "metadata": {},
   "source": [
    "## Section 6: PCK Evaluator\n",
    "\n",
    "Implement PCK (Percentage of Correct Keypoints) evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCKEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate correspondence quality using PCK (Percentage of Correct Keypoints).\n",
    "    \n",
    "    A keypoint is correct if:\n",
    "        ||predicted - ground_truth|| ≤ α × bbox_diagonal\n",
    "    \n",
    "    Standard thresholds: α ∈ {0.05, 0.10, 0.15}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_values=[0.05, 0.10, 0.15]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha_values: List of PCK thresholds\n",
    "        \"\"\"\n",
    "        self.alpha_values = alpha_values\n",
    "    \n",
    "    def compute_pck(self, pred_kps, gt_kps, bbox=None, img_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        Compute PCK for a single image pair.\n",
    "        \n",
    "        Args:\n",
    "            pred_kps: Predicted keypoints [N, 2]\n",
    "            gt_kps: Ground truth keypoints [N, 2]\n",
    "            bbox: Bounding box [4] as [x1, y1, x2, y2] (optional)\n",
    "            img_size: Image size (H, W) for normalization if no bbox\n",
    "            \n",
    "        Returns:\n",
    "            pck_dict: Dictionary with PCK@alpha for each threshold\n",
    "            distances: Normalized distances for each keypoint\n",
    "        \"\"\"\n",
    "        # Filter valid keypoints (ground truth with positive coordinates)\n",
    "        valid_mask = (gt_kps[:, 0] >= 0) & (gt_kps[:, 1] >= 0)\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            # No valid keypoints\n",
    "            return {f'pck@{alpha:.2f}': 0.0 for alpha in self.alpha_values}, np.array([])\n",
    "        \n",
    "        pred_valid = pred_kps[valid_mask]\n",
    "        gt_valid = gt_kps[valid_mask]\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = np.linalg.norm(pred_valid - gt_valid, axis=1)\n",
    "        \n",
    "        # Compute normalization factor\n",
    "        if bbox is not None:\n",
    "            # Use bounding box diagonal\n",
    "            bbox_w = bbox[2] - bbox[0]\n",
    "            bbox_h = bbox[3] - bbox[1]\n",
    "            norm_factor = np.sqrt(bbox_w ** 2 + bbox_h ** 2)\n",
    "        else:\n",
    "            # Use image diagonal\n",
    "            norm_factor = np.sqrt(img_size[0] ** 2 + img_size[1] ** 2)\n",
    "        \n",
    "        # Normalize distances\n",
    "        normalized_distances = distances / (norm_factor + 1e-8)\n",
    "        \n",
    "        # Compute PCK for each threshold\n",
    "        pck_dict = {}\n",
    "        for alpha in self.alpha_values:\n",
    "            correct = (normalized_distances <= alpha).sum()\n",
    "            pck = correct / len(normalized_distances)\n",
    "            pck_dict[f'pck@{alpha:.2f}'] = pck\n",
    "        \n",
    "        return pck_dict, normalized_distances\n",
    "    \n",
    "    def evaluate_dataset(self, predictions, ground_truths, bboxes=None):\n",
    "        \"\"\"\n",
    "        Evaluate PCK over entire dataset.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of predicted keypoints [N_samples, N_kps, 2]\n",
    "            ground_truths: List of ground truth keypoints [N_samples, N_kps, 2]\n",
    "            bboxes: List of bounding boxes (optional)\n",
    "            \n",
    "        Returns:\n",
    "            avg_pck: Dictionary with average PCK across dataset\n",
    "            per_sample_pck: List of per-sample PCK dictionaries\n",
    "        \"\"\"\n",
    "        per_sample_pck = []\n",
    "        all_distances = []\n",
    "        \n",
    "        for i in range(len(predictions)):\n",
    "            bbox = bboxes[i] if bboxes is not None else None\n",
    "            pck_dict, distances = self.compute_pck(predictions[i], ground_truths[i], bbox)\n",
    "            per_sample_pck.append(pck_dict)\n",
    "            all_distances.extend(distances.tolist())\n",
    "        \n",
    "        # Compute average PCK\n",
    "        avg_pck = {}\n",
    "        for alpha in self.alpha_values:\n",
    "            key = f'pck@{alpha:.2f}'\n",
    "            avg_pck[key] = np.mean([sample[key] for sample in per_sample_pck])\n",
    "        \n",
    "        return avg_pck, per_sample_pck, np.array(all_distances)\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = PCKEvaluator(alpha_values=[0.05, 0.10, 0.15])\n",
    "print(\"✓ PCK evaluator created\")\n",
    "print(f\"  Alpha values: {evaluator.alpha_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4e7a6",
   "metadata": {},
   "source": [
    "## Section 7: Test on Sample\n",
    "\n",
    "Let's test the complete pipeline on our sample image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd176449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match keypoints for the sample\n",
    "print(\"Matching keypoints for sample...\")\n",
    "\n",
    "# Get valid keypoints\n",
    "valid_src_mask = (src_kps[:, 0] >= 0) & (src_kps[:, 1] >= 0)\n",
    "valid_src_kps = src_kps[valid_src_mask]\n",
    "\n",
    "print(f\"Source keypoints: {len(valid_src_kps)}\")\n",
    "\n",
    "# Perform matching\n",
    "pred_kps, confidences = matcher.match_images(\n",
    "    src_img, tgt_img, valid_src_kps, feature_extractor\n",
    ")\n",
    "\n",
    "print(f\"Predicted keypoints: {pred_kps.shape}\")\n",
    "print(f\"Confidences: min={confidences.min():.3f}, max={confidences.max():.3f}, mean={confidences.mean():.3f}\")\n",
    "\n",
    "# Evaluate\n",
    "valid_tgt_kps = tgt_kps[valid_src_mask]\n",
    "pck_dict, distances = evaluator.compute_pck(pred_kps, valid_tgt_kps)\n",
    "\n",
    "print(\"\\nPCK Results:\")\n",
    "for key, value in pck_dict.items():\n",
    "    print(f\"  {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDistance statistics:\")\n",
    "print(f\"  Min: {distances.min():.4f}\")\n",
    "print(f\"  Max: {distances.max():.4f}\")\n",
    "print(f\"  Mean: {distances.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(distances):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matches\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Source image with keypoints\n",
    "axes[0].imshow(denorm_show(src_img))\n",
    "axes[0].scatter(valid_src_kps[:, 0], valid_src_kps[:, 1], \n",
    "               c='red', s=150, marker='x', linewidths=3, label='Source KPs')\n",
    "axes[0].set_title('Source Image', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "axes[0].legend()\n",
    "\n",
    "# Side-by-side comparison\n",
    "axes[1].imshow(denorm_show(tgt_img))\n",
    "axes[1].scatter(valid_tgt_kps[:, 0], valid_tgt_kps[:, 1], \n",
    "               c='lime', s=150, marker='o', alpha=0.6, linewidths=2, \n",
    "               edgecolors='darkgreen', label='Ground Truth')\n",
    "axes[1].scatter(pred_kps[:, 0], pred_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3, label='Predicted')\n",
    "axes[1].set_title(f'Target Image: Predictions vs Ground Truth\\nPCK@0.10: {pck_dict[\"pck@0.10\"]*100:.1f}%', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "axes[1].legend()\n",
    "\n",
    "# Distance histogram\n",
    "axes[2].hist(distances, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "for alpha in evaluator.alpha_values:\n",
    "    axes[2].axvline(alpha, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'α={alpha:.2f}')\n",
    "axes[2].set_xlabel('Normalized Distance', fontsize=12)\n",
    "axes[2].set_ylabel('Frequency', fontsize=12)\n",
    "axes[2].set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'sample_matching_result.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a6342",
   "metadata": {},
   "source": [
    "## Section 8: Full Dataset Evaluation\n",
    "\n",
    "Now let's evaluate on the complete test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_dataset(dataset, feature_extractor, matcher, evaluator, \n",
    "                       max_samples=None, save_visualizations=False):\n",
    "    \"\"\"\n",
    "    Evaluate correspondence on entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: SPairDataset instance\n",
    "        feature_extractor: DINOv2FeatureExtractor instance\n",
    "        matcher: CorrespondenceMatcher instance\n",
    "        evaluator: PCKEvaluator instance\n",
    "        max_samples: Maximum samples to evaluate (None = all)\n",
    "        save_visualizations: Whether to save sample visualizations\n",
    "        \n",
    "    Returns:\n",
    "        results: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating on {len(dataset)} samples...\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    num_samples = min(max_samples, len(dataset)) if max_samples else len(dataset)\n",
    "    \n",
    "    for idx in tqdm(range(num_samples), desc=\"Evaluating\"):\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        src_img = sample['src_img']\n",
    "        tgt_img = sample['trg_img']\n",
    "        src_kps = sample['src_kps']\n",
    "        tgt_kps = sample['trg_kps']\n",
    "        \n",
    "        # Get valid keypoints\n",
    "        valid_mask = (src_kps[:, 0] >= 0) & (src_kps[:, 1] >= 0)\n",
    "        valid_src_kps = src_kps[valid_mask]\n",
    "        valid_tgt_kps = tgt_kps[valid_mask]\n",
    "        \n",
    "        if len(valid_src_kps) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Match\n",
    "        pred_kps, confidences = matcher.match_images(\n",
    "            src_img, tgt_img, valid_src_kps, feature_extractor\n",
    "        )\n",
    "        \n",
    "        all_predictions.append(pred_kps)\n",
    "        all_ground_truths.append(valid_tgt_kps)\n",
    "        all_confidences.append(confidences)\n",
    "        \n",
    "        # Save visualization for first few samples\n",
    "        if save_visualizations and idx < 5:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "            \n",
    "            axes[0].imshow(denorm_show(src_img))\n",
    "            axes[0].scatter(valid_src_kps[:, 0], valid_src_kps[:, 1], \n",
    "                           c='red', s=100, marker='x', linewidths=3)\n",
    "            axes[0].set_title(f'Source (Sample {idx})', fontsize=12, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(denorm_show(tgt_img))\n",
    "            axes[1].scatter(valid_tgt_kps[:, 0], valid_tgt_kps[:, 1], \n",
    "                           c='lime', s=100, marker='o', alpha=0.6, linewidths=2, \n",
    "                           edgecolors='darkgreen', label='GT')\n",
    "            axes[1].scatter(pred_kps[:, 0], pred_kps[:, 1], \n",
    "                           c='red', s=80, marker='x', linewidths=2, label='Pred')\n",
    "            axes[1].set_title(f'Target (Sample {idx})', fontsize=12, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, f'match_sample_{idx}.png'), \n",
    "                       dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nComputing PCK metrics...\")\n",
    "    avg_pck, per_sample_pck, all_distances = evaluator.evaluate_dataset(\n",
    "        all_predictions, all_ground_truths\n",
    "    )\n",
    "    \n",
    "    # Compute additional statistics\n",
    "    all_confidences_flat = np.concatenate(all_confidences)\n",
    "    \n",
    "    results = {\n",
    "        'avg_pck': avg_pck,\n",
    "        'num_samples': num_samples,\n",
    "        'num_keypoints': len(all_distances),\n",
    "        'avg_confidence': float(all_confidences_flat.mean()),\n",
    "        'distance_stats': {\n",
    "            'mean': float(all_distances.mean()),\n",
    "            'std': float(all_distances.std()),\n",
    "            'median': float(np.median(all_distances)),\n",
    "            'min': float(all_distances.min()),\n",
    "            'max': float(all_distances.max())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation on small subset first (for testing)\n",
    "print(\"Running evaluation on 50 samples...\")\n",
    "results = evaluate_on_dataset(\n",
    "    dataset=dataset,\n",
    "    feature_extractor=feature_extractor,\n",
    "    matcher=matcher,\n",
    "    evaluator=evaluator,\n",
    "    max_samples=50,\n",
    "    save_visualizations=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS (50 samples)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPCK Metrics:\")\n",
    "for key, value in results['avg_pck'].items():\n",
    "    print(f\"  {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Samples evaluated: {results['num_samples']}\")\n",
    "print(f\"  Total keypoints: {results['num_keypoints']}\")\n",
    "print(f\"  Average confidence: {results['avg_confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\nDistance Statistics:\")\n",
    "for key, value in results['distance_stats'].items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results_path = os.path.join(OUTPUT_DIR, 'evaluation_results_subset.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\n✓ Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349ae27",
   "metadata": {},
   "source": [
    "## Section 9: Full Evaluation (Optional)\n",
    "\n",
    "Uncomment and run this cell to evaluate on the complete test set. This will take longer (~30-60 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Full dataset evaluation\n",
    "# print(\"Running FULL evaluation on all test samples...\")\n",
    "# print(\"This will take approximately 30-60 minutes...\\n\")\n",
    "\n",
    "# results_full = evaluate_on_dataset(\n",
    "#     dataset=dataset,\n",
    "#     feature_extractor=feature_extractor,\n",
    "#     matcher=matcher,\n",
    "#     evaluator=evaluator,\n",
    "#     max_samples=None,  # Use all samples\n",
    "#     save_visualizations=False  # Don't save all visualizations\n",
    "# )\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"FULL EVALUATION RESULTS\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"\\nPCK Metrics:\")\n",
    "# for key, value in results_full['avg_pck'].items():\n",
    "#     print(f\"  {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "\n",
    "# print(f\"\\nDataset Statistics:\")\n",
    "# print(f\"  Samples evaluated: {results_full['num_samples']}\")\n",
    "# print(f\"  Total keypoints: {results_full['num_keypoints']}\")\n",
    "# print(f\"  Average confidence: {results_full['avg_confidence']:.4f}\")\n",
    "\n",
    "# # Save results\n",
    "# results_path = os.path.join(OUTPUT_DIR, 'evaluation_results_full.json')\n",
    "# with open(results_path, 'w') as f:\n",
    "#     json.dump(results_full, f, indent=2)\n",
    "# print(f\"\\n✓ Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cbfcb7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **Environment Setup**: Cross-platform compatible setup (Windows/Linux/macOS/Colab)\n",
    "\n",
    "2. **DINOv2 Feature Extraction**:\n",
    "   - Model: DINOv2 ViT-B/14\n",
    "   - Features: 16×16×768 dense feature maps\n",
    "   - Preprocessing: ImageNet normalization\n",
    "   - Output: L2-normalized features for cosine similarity\n",
    "\n",
    "3. **Feature Visualization**:\n",
    "   - PCA visualization showing semantic structure\n",
    "   - Patch-to-patch similarity heatmaps\n",
    "   - Feature quality analysis\n",
    "\n",
    "4. **Correspondence Matching**:\n",
    "   - Nearest neighbor matching with bilinear interpolation\n",
    "   - Optional mutual nearest neighbors\n",
    "   - Optional ratio test for ambiguous matches\n",
    "\n",
    "5. **PCK Evaluation**:\n",
    "   - Standard thresholds: 0.05, 0.10, 0.15\n",
    "   - Bounding box normalization\n",
    "   - Per-sample and aggregate metrics\n",
    "\n",
    "6. **Results** (50 samples):\n",
    "   - See evaluation results above\n",
    "   - Visualizations saved to `outputs/dinov2/`\n",
    "\n",
    "### Key Insights:\n",
    "- DINOv2 learns rich semantic features without supervision\n",
    "- Features capture object parts and semantic regions\n",
    "- Performance varies by object category and pose variation\n",
    "- Higher similarity scores correlate with better matches\n",
    "\n",
    "### Next Steps:\n",
    "1. **Run full evaluation** (uncomment Section 9)\n",
    "2. **Try different matching strategies**:\n",
    "   - Enable mutual nearest neighbors: `matcher = CorrespondenceMatcher(mutual_nn=True)`\n",
    "   - Enable ratio test: `matcher = CorrespondenceMatcher(ratio_threshold=0.8)`\n",
    "3. **Compare with other backbones** (DINOv3, SAM)\n",
    "4. **Analyze per-category performance**\n",
    "5. **Try larger models** (dinov2_vitl14, dinov2_vitg14)\n",
    "\n",
    "### Files Generated:\n",
    "- `outputs/dinov2/sample_images_with_keypoints.png`\n",
    "- `outputs/dinov2/pca_feature_visualization.png`\n",
    "- `outputs/dinov2/similarity_heatmap.png`\n",
    "- `outputs/dinov2/sample_matching_result.png`\n",
    "- `outputs/dinov2/match_sample_*.png` (first 5 samples)\n",
    "- `outputs/dinov2/evaluation_results_subset.json`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
