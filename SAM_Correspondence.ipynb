{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27a8f54",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb402999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and configure paths\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"✓ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"✓ Running locally\")\n",
    "\n",
    "# Set up paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = '/content/AMLProject'\n",
    "    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "\n",
    "# Create necessary directories\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints', 'sam')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'sam')\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "\n",
    "for directory in [CHECKPOINT_DIR, OUTPUT_DIR, MODEL_DIR, DATA_ROOT]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def window_soft_argmax(similarity, H, W, window=7, tau=0.05):\n",
    "    \"\"\"\n",
    "    Window soft-argmax for sub-pixel coordinate prediction.\n",
    "    \n",
    "    Args:\n",
    "        similarity: [N, H*W] or [N, H, W] similarity scores\n",
    "        H, W: Grid dimensions\n",
    "        window: Window size around peak (odd number)\n",
    "        tau: Temperature for softmax (lower = sharper)\n",
    "    \n",
    "    Returns:\n",
    "        [N, 2] tensor with (y, x) coordinates in patch space\n",
    "    \"\"\"\n",
    "    if similarity.dim() == 2:\n",
    "        N = similarity.size(0)\n",
    "        sim2d = similarity.view(N, H, W)\n",
    "    elif similarity.dim() == 3:\n",
    "        N = similarity.size(0)\n",
    "        sim2d = similarity\n",
    "    else:\n",
    "        raise ValueError(\"similarity must be [N,H*W] or [N,H,W]\")\n",
    "    \n",
    "    r = window // 2\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        s = sim2d[i]  # [H, W]\n",
    "        \n",
    "        # Find peak with argmax\n",
    "        idx = torch.argmax(s)\n",
    "        y0 = (idx // W).item()\n",
    "        x0 = (idx % W).item()\n",
    "        \n",
    "        # Extract window around peak\n",
    "        y1, y2 = max(y0 - r, 0), min(y0 + r + 1, H)\n",
    "        x1, x2 = max(x0 - r, 0), min(x0 + r + 1, W)\n",
    "        \n",
    "        sub = s[y1:y2, x1:x2]\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.arange(y1, y2, device=s.device, dtype=torch.float32),\n",
    "            torch.arange(x1, x2, device=s.device, dtype=torch.float32),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        \n",
    "        # Soft-argmax within window\n",
    "        wts = torch.softmax(sub.flatten() / tau, dim=0).view_as(sub)\n",
    "        y_hat = (wts * yy).sum()\n",
    "        x_hat = (wts * xx).sum()\n",
    "        \n",
    "        preds.append(torch.stack([y_hat, x_hat]))\n",
    "    \n",
    "    return torch.stack(preds, dim=0)  # [N, 2]\n",
    "\n",
    "\n",
    "def unfreeze_last_k_blocks(model, k, blocks_attr='blocks'):\n",
    "    \"\"\"\n",
    "    Unfreeze the last k transformer blocks of a model.\n",
    "    For SAM, use 'image_encoder.blocks' to access encoder blocks.\n",
    "    \n",
    "    Args:\n",
    "        model: The backbone model (SAM image_encoder)\n",
    "        k: Number of last blocks to unfreeze\n",
    "        blocks_attr: Attribute path for blocks (default 'blocks')\n",
    "    \n",
    "    Returns:\n",
    "        List of trainable parameters\n",
    "    \"\"\"\n",
    "    # Freeze all parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Navigate to blocks (handle nested attributes like 'image_encoder.blocks')\n",
    "    obj = model\n",
    "    for attr in blocks_attr.split('.'):\n",
    "        obj = getattr(obj, attr)\n",
    "    blocks = obj\n",
    "    \n",
    "    # Unfreeze last k blocks\n",
    "    for block in blocks[-k:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "    \n",
    "    # Return trainable parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f\"Unfroze last {k} blocks: {len(trainable_params)} trainable parameters\")\n",
    "    \n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "def compute_keypoint_loss(sim2d, H, W, gt_xy_px, patch_size, use_soft=True, window=7, tau=0.05):\n",
    "    \"\"\"\n",
    "    Compute loss from similarity map to ground truth keypoint.\n",
    "    \n",
    "    Args:\n",
    "        sim2d: [H, W] similarity map\n",
    "        H, W: Grid dimensions\n",
    "        gt_xy_px: [2] ground truth coordinates in pixels (y, x)\n",
    "        patch_size: Patch size for coordinate conversion\n",
    "        use_soft: Use soft-argmax (True) or argmax (False)\n",
    "        window, tau: Soft-argmax parameters\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss\n",
    "    \"\"\"\n",
    "    if use_soft:\n",
    "        pred_xy_patch = window_soft_argmax(sim2d[None], H, W, window, tau)[0]\n",
    "    else:\n",
    "        idx = sim2d.argmax()\n",
    "        pred_xy_patch = torch.stack([idx // W, idx % W]).float()\n",
    "    \n",
    "    pred_xy_px = (pred_xy_patch + 0.5) * patch_size\n",
    "    \n",
    "    return F.smooth_l1_loss(pred_xy_px, gt_xy_px)\n",
    "\n",
    "\n",
    "print(\"✓ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c898a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    \"\"\"SPair-71k dataset with keypoint annotations.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='trn', category=None, image_size=1024, subset=None):\n",
    "        # SAM uses 1024x1024 by default\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.category = category\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.pairs = self._load_pairs()\n",
    "        if subset is not None:\n",
    "            self.pairs = self.pairs[:subset]\n",
    "        \n",
    "        # SAM preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(f\"SPair-71k {split} dataset: {len(self.pairs)} pairs loaded\")\n",
    "    \n",
    "    def _load_pairs(self):\n",
    "        pairs = []\n",
    "        layout_dir = os.path.join(self.root_dir, 'Layout', self.split)\n",
    "        \n",
    "        if not os.path.exists(layout_dir):\n",
    "            print(f\"Warning: Layout directory not found: {layout_dir}\")\n",
    "            return pairs\n",
    "        \n",
    "        if self.category:\n",
    "            categories = [self.category]\n",
    "        else:\n",
    "            categories = [d for d in os.listdir(layout_dir) \n",
    "                         if os.path.isdir(os.path.join(layout_dir, d))]\n",
    "        \n",
    "        for cat in categories:\n",
    "            cat_dir = os.path.join(layout_dir, cat)\n",
    "            if not os.path.exists(cat_dir):\n",
    "                continue\n",
    "            \n",
    "            for fname in os.listdir(cat_dir):\n",
    "                if not fname.endswith('.json'):\n",
    "                    continue\n",
    "                \n",
    "                json_path = os.path.join(cat_dir, fname)\n",
    "                try:\n",
    "                    with open(json_path, 'r') as f:\n",
    "                        pair_data = json.load(f)\n",
    "                    \n",
    "                    pair = {\n",
    "                        'category': cat,\n",
    "                        'src_img': pair_data['src_imname'],\n",
    "                        'tgt_img': pair_data['trg_imname'],\n",
    "                        'src_kps': np.array(pair_data['src_kps']).reshape(-1, 2),\n",
    "                        'tgt_kps': np.array(pair_data['trg_kps']).reshape(-1, 2),\n",
    "                        'src_bbox': pair_data.get('src_bndbox', None),\n",
    "                        'tgt_bbox': pair_data.get('trg_bndbox', None),\n",
    "                    }\n",
    "                    pairs.append(pair)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        src_img_path = os.path.join(self.root_dir, 'JPEGImages', \n",
    "                                    pair['category'], pair['src_img'])\n",
    "        tgt_img_path = os.path.join(self.root_dir, 'JPEGImages',\n",
    "                                    pair['category'], pair['tgt_img'])\n",
    "        \n",
    "        src_img_pil = Image.open(src_img_path).convert('RGB')\n",
    "        tgt_img_pil = Image.open(tgt_img_path).convert('RGB')\n",
    "        \n",
    "        src_w, src_h = src_img_pil.size\n",
    "        tgt_w, tgt_h = tgt_img_pil.size\n",
    "        \n",
    "        src_kps = pair['src_kps'].copy().astype(float)\n",
    "        tgt_kps = pair['tgt_kps'].copy().astype(float)\n",
    "        \n",
    "        src_kps[:, 0] *= self.image_size / src_w\n",
    "        src_kps[:, 1] *= self.image_size / src_h\n",
    "        tgt_kps[:, 0] *= self.image_size / tgt_w\n",
    "        tgt_kps[:, 1] *= self.image_size / tgt_h\n",
    "        \n",
    "        src_img = self.transform(src_img_pil)\n",
    "        tgt_img = self.transform(tgt_img_pil)\n",
    "        \n",
    "        if pair['src_bbox'] is not None:\n",
    "            src_bbox = np.array(pair['src_bbox'])\n",
    "            src_bbox[0::2] *= self.image_size / src_w\n",
    "            src_bbox[1::2] *= self.image_size / src_h\n",
    "            src_bbox_wh = np.array([src_bbox[2] - src_bbox[0], src_bbox[3] - src_bbox[1]])\n",
    "        else:\n",
    "            src_bbox_wh = np.array([self.image_size, self.image_size])\n",
    "        \n",
    "        if pair['tgt_bbox'] is not None:\n",
    "            tgt_bbox = np.array(pair['tgt_bbox'])\n",
    "            tgt_bbox[0::2] *= self.image_size / tgt_w\n",
    "            tgt_bbox[1::2] *= self.image_size / tgt_h\n",
    "            tgt_bbox_wh = np.array([tgt_bbox[2] - tgt_bbox[0], tgt_bbox[3] - tgt_bbox[1]])\n",
    "        else:\n",
    "            tgt_bbox_wh = np.array([self.image_size, self.image_size])\n",
    "        \n",
    "        return {\n",
    "            'src_img': src_img,\n",
    "            'tgt_img': tgt_img,\n",
    "            'src_kps': torch.from_numpy(src_kps).float(),\n",
    "            'tgt_kps': torch.from_numpy(tgt_kps).float(),\n",
    "            'src_bbox_wh': torch.from_numpy(src_bbox_wh).float(),\n",
    "            'tgt_bbox_wh': torch.from_numpy(tgt_bbox_wh).float(),\n",
    "            'category': pair['category'],\n",
    "            'pair_id': idx\n",
    "        }\n",
    "\n",
    "\n",
    "def create_spair_dataloaders(root_dir, batch_size=1, num_workers=2, \n",
    "                             train_subset=None, val_subset=None):\n",
    "    train_dataset = SPairDataset(root_dir, split='trn', subset=train_subset)\n",
    "    val_dataset = SPairDataset(root_dir, split='val', subset=val_subset)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                           num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "print(\"✓ SPair-71k dataloader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11da02f",
   "metadata": {},
   "source": [
    "## SPair-71k Dataloader\n",
    "\n",
    "Complete dataloader for SPair-71k with keypoint annotations for finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40607e3c",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Window soft-argmax for sub-pixel refinement and finetuning utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15036f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION FLAGS ==========\n",
    "# Set these flags to control behavior\n",
    "ENABLE_FINETUNING = False  # Set True to enable light finetuning of last layers\n",
    "USE_SOFT_ARGMAX = False    # Set True to use window soft-argmax instead of argmax\n",
    "\n",
    "# Finetuning hyperparameters (only used if ENABLE_FINETUNING=True)\n",
    "FINETUNE_K_LAYERS = 2      # Number of last transformer blocks to unfreeze {1, 2, 4}\n",
    "FINETUNE_LR = 1e-5         # Learning rate\n",
    "FINETUNE_WD = 1e-4         # Weight decay\n",
    "FINETUNE_EPOCHS = 3        # Number of training epochs\n",
    "FINETUNE_BATCH_SIZE = 1    # Batch size for training\n",
    "FINETUNE_TRAIN_SUBSET = None  # None for full training set, or int for subset\n",
    "\n",
    "# Soft-argmax hyperparameters (only used if USE_SOFT_ARGMAX=True)\n",
    "SOFT_WINDOW = 7            # Window size around peak (odd number: 5, 7, 9)\n",
    "SOFT_TAU = 0.05            # Softmax temperature (lower = sharper)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  ENABLE_FINETUNING = {ENABLE_FINETUNING}\")\n",
    "print(f\"  USE_SOFT_ARGMAX = {USE_SOFT_ARGMAX}\")\n",
    "if ENABLE_FINETUNING:\n",
    "    print(f\"  Finetuning: k={FINETUNE_K_LAYERS}, lr={FINETUNE_LR}, epochs={FINETUNE_EPOCHS}\")\n",
    "if USE_SOFT_ARGMAX:\n",
    "    print(f\"  Soft-argmax: window={SOFT_WINDOW}, tau={SOFT_TAU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ddaaa6",
   "metadata": {},
   "source": [
    "## Configuration Flags\n",
    "\n",
    "Set these flags to control the pipeline behavior:\n",
    "- `ENABLE_FINETUNING`: Enable light finetuning of last transformer blocks in image encoder\n",
    "- `USE_SOFT_ARGMAX`: Use window soft-argmax instead of argmax for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (robust, platform-aware with clear errors)\n",
    "import subprocess, sys, platform\n",
    "print(\"Installing required packages...\")\n",
    "# Install Segment Anything first (from git)\n",
    "try:\n",
    "    print(\"Installing Segment Anything (segment-anything)...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'git+https://github.com/facebookresearch/segment-anything.git'])\n",
    "    print(\"✓ segment-anything installed\")\n",
    "except Exception as e:\n",
    "    print(\"You can install it manually with:\")\n",
    "    print(\"  pip install git+https://github.com/facebookresearch/segment-anything.git\")\n",
    "\n",
    "# Common Python packages (install separately to isolate failures)\n",
    "common_packages = [\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'opencv-python',\n",
    "    'pillow',\n",
    "    'scipy',\n",
    "    'tqdm',\n",
    "    'pandas',\n",
    "    'scikit-learn'\n",
    "]\n",
    "try:\n",
    "    print(\"Installing common Python packages...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade'] + common_packages)\n",
    "    print(\"✓ Common packages installed\")\n",
    "except Exception as e:\n",
    "    print(\"You can install them manually, e.g.:\")\n",
    "    print(\"  pip install \")\n",
    " \n",
    "\n",
    "print(\"Installing PyTorch (torch, torchvision, torchaudio)...\")\n",
    "try:\n",
    "    if platform.system() == 'Darwin':\n",
    "        # macOS: pip usually installs the correct (CPU/MPS) wheel or user can use conda\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'torch', 'torchvision', 'torchaudio'])\n",
    "    else:\n",
    "        # Linux/Windows: prefer official CUDA wheel index (adjust if you need a different CUDA version)\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--index-url', 'https://download.pytorch.org/whl/cu121', 'torch', 'torchvision', 'torchaudio'])\n",
    "    print(\"✓ PyTorch packages installed\")\n",
    "except Exception as e:\n",
    "    print(\"Attempting CPU-only PyTorch installation as a fallback...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--index-url', 'https://download.pytorch.org/whl/cpu', 'torch', 'torchvision', 'torchaudio'])\n",
    "        print(\"✓ CPU-only PyTorch installed\")\n",
    "    except Exception as e2:\n",
    "        print(\"Please follow the official instructions at https://pytorch.org/get-started/locally/ to install a compatible wheel for your system.\")\n",
    "\n",
    "import sys, subprocess\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"scikit-learn\"])\n",
    "    print(\"✓ scikit-learn installed (or already up-to-date)\")\n",
    "except Exception as e:\n",
    "    print(\"✗ pip install failed:\", e)\n",
    "\n",
    "print(\"\\nInstallation step finished. If any package failed, rerun the cell without '--quiet' or install manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_FINETUNING:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LIGHT FINETUNING ENABLED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Unfreeze last k blocks of SAM's image encoder\n",
    "    trainable_params = unfreeze_last_k_blocks(sam.image_encoder, FINETUNE_K_LAYERS, blocks_attr='blocks')\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=FINETUNE_LR, weight_decay=FINETUNE_WD)\n",
    "    \n",
    "    print(f\"Finetuning configuration:\")\n",
    "    print(f\"  k={FINETUNE_K_LAYERS} layers (image encoder)\")\n",
    "    print(f\"  lr={FINETUNE_LR}, wd={FINETUNE_WD}\")\n",
    "    print(f\"  epochs={FINETUNE_EPOCHS}\")\n",
    "    print(f\"  batch_size={FINETUNE_BATCH_SIZE}\")\n",
    "    \n",
    "    print(\"\\n⚠️  Finetuning code structure ready but requires SPair-71k training dataloader\")\n",
    "    print(\"   Implement the dataloader to enable full finetuning\")\n",
    "    print(\"   See DINOv2 notebook for training loop example\")\n",
    "    \n",
    "else:\n",
    "    print(\"Finetuning disabled. Using pretrained weights only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee6e49b",
   "metadata": {},
   "source": [
    "## Light Finetuning (Optional)\n",
    "\n",
    "If `ENABLE_FINETUNING=True`, this section finetunes the last k transformer blocks of SAM's image encoder on SPair-71k with keypoint supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Import SAM\n",
    "try:\n",
    "    from segment_anything import sam_model_registry, SamPredictor\n",
    "    from segment_anything.utils.transforms import ResizeLongestSide\n",
    "    print(\"✓ SAM imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Error importing SAM: {e}\")\n",
    "    print(\"  Please ensure segment-anything is installed\")\n",
    "    raise\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a960a6d",
   "metadata": {},
   "source": [
    "## Section 2: Download and Load SAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SAM checkpoint\n",
    "import urllib.request\n",
    "\n",
    "SAM_CHECKPOINT_URL = 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth'\n",
    "SAM_CHECKPOINT_NAME = 'sam_vit_b_01ec64.pth'\n",
    "sam_checkpoint_path = os.path.join(CHECKPOINT_DIR, SAM_CHECKPOINT_NAME)\n",
    "\n",
    "print(\"Checking SAM checkpoint...\")\n",
    "if os.path.exists(sam_checkpoint_path):\n",
    "    print(f\"✓ Checkpoint already exists: {sam_checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Downloading SAM ViT-B checkpoint...\")\n",
    "    print(f\"URL: {SAM_CHECKPOINT_URL}\")\n",
    "    print(f\"This may take a few minutes (~375 MB)...\")\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(SAM_CHECKPOINT_URL, sam_checkpoint_path)\n",
    "        print(f\"✓ Downloaded successfully to: {sam_checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Download failed: {e}\")\n",
    "        print(\"\\nPlease download manually:\")\n",
    "        print(f\"  URL: {SAM_CHECKPOINT_URL}\")\n",
    "        print(f\"  Save to: {sam_checkpoint_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24164fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM model\n",
    "print(\"Loading SAM ViT-B model...\")\n",
    "\n",
    "try:\n",
    "    sam_model = sam_model_registry['vit_b'](checkpoint=sam_checkpoint_path)\n",
    "    sam_model = sam_model.to(device)\n",
    "    sam_model.eval()\n",
    "    \n",
    "    # Create predictor (optional, for segmentation tasks)\n",
    "    sam_predictor = SamPredictor(sam_model)\n",
    "    \n",
    "    print(\"✓ SAM model loaded successfully!\")\n",
    "    print(f\"  - Architecture: ViT-B (Base)\")\n",
    "    print(f\"  - Image encoder output: 64×64 feature map\")\n",
    "    print(f\"  - Feature dimension: 256\")\n",
    "    print(f\"  - Input size: 1024×1024 (longest side)\")\n",
    "    print(f\"  - Device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading SAM: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe3286",
   "metadata": {},
   "source": [
    "## Section 3: Dense Feature Extraction\n",
    "\n",
    "### SAM Feature Extraction Strategy\n",
    "\n",
    "SAM's image encoder produces high-quality dense features:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Resize longest side to 1024 pixels\n",
    "   - Pad to square (1024×1024)\n",
    "   - Normalize with ImageNet statistics\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - Extract from image encoder (ViT backbone)\n",
    "   - Output: 64×64×256 feature map\n",
    "   - Features encode both semantic and spatial information\n",
    "\n",
    "3. **Key Differences from DINO**:\n",
    "   - Larger input resolution (1024 vs 224)\n",
    "   - Lower feature dimensionality (256 vs 768)\n",
    "   - Designed for segmentation (may be better for spatial tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fa8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract dense spatial features from SAM's image encoder.\n",
    "    \n",
    "    SAM is optimized for dense prediction, making it potentially\n",
    "    excellent for correspondence tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.image_encoder = model.image_encoder\n",
    "        self.img_size = self.image_encoder.img_size  # 1024\n",
    "        self.feat_dim = 256  # SAM feature dimension\n",
    "        \n",
    "        # SAM's preprocessing transform\n",
    "        self.transform = ResizeLongestSide(self.img_size)\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        Preprocess image following SAM's requirements: resize + pad to square.\n",
    "        Returns:\n",
    "            input_tensor: torch.Tensor shaped [1, 3, 1024, 1024] on device with correct dtype\n",
    "            original_size: (H, W) of original image\n",
    "        \"\"\"\n",
    "        # Convert to numpy\n",
    "        if isinstance(image, Image.Image):\n",
    "            image_np = np.array(image)\n",
    "            original_size = (image.height, image.width)\n",
    "        else:\n",
    "            image_np = image\n",
    "            original_size = (image_np.shape[0], image_np.shape[1])\n",
    "\n",
    "        # Apply SAM's transform (resize longest side to 1024)\n",
    "        input_image = self.transform.apply_image(image_np)  # HxWxC, uint8\n",
    "        \n",
    "        # Convert to tensor [C, H, W]\n",
    "        input_tensor = torch.as_tensor(input_image, dtype=torch.float32).permute(2, 0, 1).contiguous()\n",
    "        \n",
    "        # Pad to square (1024x1024) - CRITICAL for SAM\n",
    "        h, w = input_tensor.shape[-2:]\n",
    "        padh = self.img_size - h\n",
    "        padw = self.img_size - w\n",
    "        input_tensor = F.pad(input_tensor, (0, padw, 0, padh))  # pad right and bottom\n",
    "        \n",
    "        # Add batch dimension [1, 3, 1024, 1024]\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Move to device and match model dtype\n",
    "        try:\n",
    "            param_dtype = next(self.image_encoder.parameters()).dtype\n",
    "        except StopIteration:\n",
    "            param_dtype = torch.float32\n",
    "        \n",
    "        input_tensor = input_tensor.to(self.device).to(param_dtype)\n",
    "        \n",
    "        return input_tensor, original_size\n",
    "    \n",
    "    def extract_features(self, image, normalize=True):\n",
    "        \"\"\"\n",
    "        Extract dense feature map from image.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or numpy array\n",
    "            normalize: Apply L2 normalization\n",
    "            \n",
    "        Returns:\n",
    "            features: [H, W, D] numpy array (64×64×256)\n",
    "            info: Metadata dictionary\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        img_tensor, original_size = self.preprocess_image(image)\n",
    "        \n",
    "        # Extract features from image encoder\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.image_encoder(img_tensor)  # [1, 256, 64, 64]\n",
    "        \n",
    "        # Rearrange to [H, W, D]\n",
    "        features = image_embedding[0].permute(1, 2, 0)  # [64, 64, 256]\n",
    "        \n",
    "        # L2 normalize\n",
    "        if normalize:\n",
    "            features = F.normalize(features, p=2, dim=-1)\n",
    "        \n",
    "        features = features.cpu().numpy()\n",
    "        \n",
    "        h, w = features.shape[0], features.shape[1]\n",
    "        \n",
    "        # Get original image size\n",
    "        if isinstance(image, Image.Image):\n",
    "            orig_w, orig_h = image.size\n",
    "        else:\n",
    "            orig_h, orig_w = image.shape[:2]\n",
    "        \n",
    "        info = {\n",
    "            'original_size': (orig_w, orig_h),\n",
    "            'feature_size': (w, h),\n",
    "            'processed_size': original_size,\n",
    "            'scale_x': w / orig_w,\n",
    "            'scale_y': h / orig_h\n",
    "        }\n",
    "        \n",
    "        return features, info\n",
    "    \n",
    "    def map_coords_to_features(self, coords, info):\n",
    "        \"\"\"Map image coordinates to feature space.\"\"\"\n",
    "        coords = np.array(coords).astype(float)\n",
    "        feat_coords = coords.copy()\n",
    "        feat_coords[:, 0] *= info['scale_x']\n",
    "        feat_coords[:, 1] *= info['scale_y']\n",
    "        return feat_coords\n",
    "    \n",
    "    def extract_keypoint_features(self, image, keypoints):\n",
    "        \"\"\"\n",
    "        Extract features at specific keypoint locations.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            keypoints: [N, 2] array of (x, y) coordinates\n",
    "            \n",
    "        Returns:\n",
    "            kp_features: [N, D] feature vectors\n",
    "        \"\"\"\n",
    "        features, info = self.extract_features(image, normalize=True)\n",
    "        h, w, d = features.shape\n",
    "        \n",
    "        # Map to feature space\n",
    "        feat_kps = self.map_coords_to_features(keypoints, info)\n",
    "        \n",
    "        # Clip and round\n",
    "        feat_kps[:, 0] = np.clip(feat_kps[:, 0], 0, w - 1)\n",
    "        feat_kps[:, 1] = np.clip(feat_kps[:, 1], 0, h - 1)\n",
    "        feat_kps = np.round(feat_kps).astype(int)\n",
    "        \n",
    "        # Extract features\n",
    "        kp_features = features[feat_kps[:, 1], feat_kps[:, 0], :]\n",
    "        \n",
    "        return kp_features\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = SAMFeatureExtractor(sam_model, device=device)\n",
    "print(\"✓ SAM feature extractor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62208668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature extraction\n",
    "print(\"Testing SAM feature extraction...\")\n",
    "test_image = Image.new('RGB', (480, 640), color=(128, 128, 128))\n",
    "\n",
    "features, info = feature_extractor.extract_features(test_image)\n",
    "print(f\"\\n✓ Feature extraction successful!\")\n",
    "print(f\"  Input image size: {info['original_size']}\")\n",
    "print(f\"  Feature map size: {info['feature_size']} = {features.shape[0]}×{features.shape[1]}\")\n",
    "print(f\"  Feature dimension: {features.shape[2]}\")\n",
    "print(f\"  Features normalized: {np.allclose(np.linalg.norm(features[0, 0, :]), 1.0)}\")\n",
    "print(f\"\\n  Note: SAM uses 64×64 feature grid (higher resolution than DINO's 16×16)\")\n",
    "\n",
    "# Test keypoint features\n",
    "test_kps = np.array([[100, 150], [200, 300], [400, 500]])\n",
    "kp_features = feature_extractor.extract_keypoint_features(test_image, test_kps)\n",
    "print(f\"\\n✓ Keypoint feature extraction successful!\")\n",
    "print(f\"  Number of keypoints: {len(test_kps)}\")\n",
    "print(f\"  Feature shape: {kp_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881fcd5",
   "metadata": {},
   "source": [
    "## Section 3: Correspondence Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrespondenceMatcher:\n",
    "    \"\"\"\n",
    "    Match keypoints between images using dense feature similarity.\n",
    "    \n",
    "    SAM's higher spatial resolution (64×64 vs 16×16) may provide\n",
    "    more accurate localization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mutual_nn=False, ratio_threshold=None, use_soft_argmax=False, \n",
    "                 soft_window=7, soft_tau=0.05):\n",
    "        self.mutual_nn = mutual_nn\n",
    "        self.ratio_threshold = ratio_threshold\n",
    "        self.use_soft_argmax = use_soft_argmax\n",
    "        self.soft_window = soft_window\n",
    "        self.soft_tau = soft_tau\n",
    "    \n",
    "    def match(self, src_features, tgt_features_map, return_scores=True):\n",
    "        \"\"\"Match source features to target feature map.\"\"\"\n",
    "        h, w, d = tgt_features_map.shape\n",
    "        tgt_flat = tgt_features_map.reshape(-1, d)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = src_features @ tgt_flat.T  # [N, h*w]\n",
    "        \n",
    "        if self.use_soft_argmax:\n",
    "            # Convert to torch for soft-argmax\n",
    "            sim_torch = torch.from_numpy(similarity).float()\n",
    "            \n",
    "            # Use window soft-argmax\n",
    "            pred_coords_patch = window_soft_argmax(\n",
    "                sim_torch, h, w,\n",
    "                window=self.soft_window,\n",
    "                tau=self.soft_tau\n",
    "            )  # [N, 2] in (y, x) patch coordinates\n",
    "            \n",
    "            matched_x = pred_coords_patch[:, 1].cpu().numpy()\n",
    "            matched_y = pred_coords_patch[:, 0].cpu().numpy()\n",
    "            \n",
    "            # Get confidence from peak similarity\n",
    "            best_scores = np.max(similarity, axis=1)\n",
    "        else:\n",
    "            # Standard argmax\n",
    "            best_indices = np.argmax(similarity, axis=1)\n",
    "            best_scores = np.max(similarity, axis=1)\n",
    "            \n",
    "            # Convert to coordinates\n",
    "            matched_y = best_indices // w\n",
    "            matched_x = best_indices % w\n",
    "        \n",
    "        # Ratio test\n",
    "        if self.ratio_threshold is not None:\n",
    "            sorted_sim = np.sort(similarity, axis=1)[:, ::-1]\n",
    "            ratios = sorted_sim[:, 0] / (sorted_sim[:, 1] + 1e-8)\n",
    "            valid_mask = ratios > self.ratio_threshold\n",
    "            if not self.use_soft_argmax:\n",
    "                matched_x[~valid_mask] = -1\n",
    "                matched_y[~valid_mask] = -1\n",
    "            best_scores[~valid_mask] = 0.0\n",
    "        \n",
    "        # Mutual nearest neighbor (only for argmax)\n",
    "        if self.mutual_nn and not self.use_soft_argmax:\n",
    "            reverse_sim = tgt_flat @ src_features.T\n",
    "            reverse_best = np.argmax(reverse_sim, axis=1)\n",
    "            \n",
    "            best_indices = matched_y * w + matched_x\n",
    "            for i, tgt_idx in enumerate(best_indices):\n",
    "                if tgt_idx >= 0 and reverse_best[int(tgt_idx)] != i:\n",
    "                    matched_x[i] = -1\n",
    "                    matched_y[i] = -1\n",
    "        \n",
    "        matched_coords = np.stack([matched_x, matched_y], axis=1).astype(float)\n",
    "        \n",
    "        if return_scores:\n",
    "            return matched_coords, best_scores\n",
    "        return matched_coords\n",
    "    \n",
    "    def match_images(self, src_img, tgt_img, src_keypoints, feature_extractor):\n",
    "        \"\"\"\n",
    "        Complete matching pipeline for an image pair.\n",
    "        \n",
    "        Args:\n",
    "            src_img: Source PIL Image\n",
    "            tgt_img: Target PIL Image\n",
    "            src_keypoints: [N, 2] array of (x, y) coordinates\n",
    "            feature_extractor: SAMFeatureExtractor instance\n",
    "            \n",
    "        Returns:\n",
    "            matched_coords: [N, 2] array in target image coordinates\n",
    "            scores: [N] confidence scores\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        src_kp_feats = feature_extractor.extract_keypoint_features(src_img, src_keypoints)\n",
    "        tgt_feats, tgt_info = feature_extractor.extract_features(tgt_img)\n",
    "        \n",
    "        # Match in feature space\n",
    "        matched_feat_coords, scores = self.match(src_kp_feats, tgt_feats, return_scores=True)\n",
    "        \n",
    "        # Map back to image space\n",
    "        matched_img_coords = matched_feat_coords.copy()\n",
    "        matched_img_coords[:, 0] /= tgt_info['scale_x']\n",
    "        matched_img_coords[:, 1] /= tgt_info['scale_y']\n",
    "        \n",
    "        return matched_img_coords, scores\n",
    "\n",
    "# Initialize matcher with configuration\n",
    "matcher = CorrespondenceMatcher(\n",
    "    mutual_nn=False, \n",
    "    ratio_threshold=None,\n",
    "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
    "    soft_window=SOFT_WINDOW if USE_SOFT_ARGMAX else 7,\n",
    "    soft_tau=SOFT_TAU if USE_SOFT_ARGMAX else 0.05\n",
    ")\n",
    "print(f\"✓ Correspondence matcher initialized (soft-argmax={'enabled' if USE_SOFT_ARGMAX else 'disabled'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7d102",
   "metadata": {},
   "source": [
    "## Section 3: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCKEvaluator:\n",
    "    \"\"\"PCK (Percentage of Correct Keypoints) evaluator.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_values=[0.05, 0.10, 0.15], use_bbox=True):\n",
    "        self.alpha_values = alpha_values\n",
    "        self.use_bbox = use_bbox\n",
    "    \n",
    "    def compute_pck(self, predicted_kps, gt_kps, image_size=None, bbox=None):\n",
    "        \"\"\"Compute PCK for single image pair.\"\"\"\n",
    "        valid_mask = ~np.isnan(predicted_kps).any(axis=1) & ~np.isnan(gt_kps).any(axis=1)\n",
    "        if valid_mask.sum() == 0:\n",
    "            return {f'PCK@{alpha:.2f}': 0.0 for alpha in self.alpha_values}\n",
    "        \n",
    "        pred = predicted_kps[valid_mask]\n",
    "        gt = gt_kps[valid_mask]\n",
    "        \n",
    "        distances = np.linalg.norm(pred - gt, axis=1)\n",
    "        \n",
    "        if self.use_bbox and bbox is not None and len(bbox) >= 4:\n",
    "            norm_factor = np.sqrt(bbox[2]**2 + bbox[3]**2)\n",
    "        elif image_size is not None:\n",
    "            norm_factor = np.sqrt(image_size[0]**2 + image_size[1]**2)\n",
    "        else:\n",
    "            norm_factor = 1.0\n",
    "        \n",
    "        pck_dict = {}\n",
    "        for alpha in self.alpha_values:\n",
    "            threshold = alpha * norm_factor\n",
    "            correct = (distances <= threshold).sum()\n",
    "            pck = correct / len(distances) if len(distances) > 0 else 0.0\n",
    "            pck_dict[f'PCK@{alpha:.2f}'] = pck\n",
    "        \n",
    "        return pck_dict\n",
    "    \n",
    "    def evaluate_batch(self, predictions, ground_truths, image_sizes=None, bboxes=None):\n",
    "        \"\"\"Evaluate multiple image pairs.\"\"\"\n",
    "        all_pck = {f'PCK@{alpha:.2f}': [] for alpha in self.alpha_values}\n",
    "        per_sample = []\n",
    "        \n",
    "        for i in range(len(predictions)):\n",
    "            img_size = image_sizes[i] if image_sizes else None\n",
    "            bbox = bboxes[i] if bboxes else None\n",
    "            \n",
    "            pck = self.compute_pck(predictions[i], ground_truths[i], img_size, bbox)\n",
    "            per_sample.append(pck)\n",
    "            \n",
    "            for key, value in pck.items():\n",
    "                all_pck[key].append(value)\n",
    "        \n",
    "        mean_pck = {key: np.mean(values) for key, values in all_pck.items()}\n",
    "        \n",
    "        return {\n",
    "            'mean': mean_pck,\n",
    "            'per_sample': per_sample,\n",
    "            'num_samples': len(predictions)\n",
    "        }\n",
    "\n",
    "evaluator = PCKEvaluator(alpha_values=[0.05, 0.10, 0.15], use_bbox=True)\n",
    "print(\"✓ PCK evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb435eeb",
   "metadata": {},
   "source": [
    "## Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup\n",
    "def setup_datasets(data_root):\n",
    "    \"\"\"Setup benchmark datasets.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DATASET SETUP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n⚠️  Please download datasets manually:\")\n",
    "    print(\"\\n1. PF-Pascal: https://www.di.ens.fr/willow/research/proposalflow/\")\n",
    "    print(f\"   → Extract to: {data_root}/pf-pascal/\")\n",
    "    print(\"\\n2. SPair-71k: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
    "    print(f\"   → Extract to: {data_root}/spair-71k/\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "setup_datasets(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee97ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPair-71k dataset loader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    \"\"\"SPair-71k dataset loader.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', category=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.category = category\n",
    "        self.pairs = []\n",
    "        self._load_annotations()\n",
    "    \n",
    "    def _load_annotations(self):\n",
    "        anno_dir = self.root_dir / 'PairAnnotation' / self.split\n",
    "        \n",
    "        if not anno_dir.exists():\n",
    "            print(f\"⚠️  Annotations not found: {anno_dir}\")\n",
    "            return\n",
    "        \n",
    "        for anno_file in sorted(anno_dir.glob('*.json')):\n",
    "            with open(anno_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if self.category and data.get('category') != self.category:\n",
    "                continue\n",
    "            \n",
    "            # Image paths are: JPEGImages/<category>/<image_name>\n",
    "            cat = data.get('category', 'unknown')\n",
    "            pair = {\n",
    "                'src_img': str(self.root_dir / 'JPEGImages' / cat / data['src_imname']),\n",
    "                'tgt_img': str(self.root_dir / 'JPEGImages' / cat / data['trg_imname']),\n",
    "                'src_kps': np.array(data['src_kps']).T,\n",
    "                'tgt_kps': np.array(data['trg_kps']).T,\n",
    "                'src_bbox': np.array(data.get('src_bndbox', [])),\n",
    "                'tgt_bbox': np.array(data.get('trg_bndbox', [])),\n",
    "                'category': cat\n",
    "            }\n",
    "            self.pairs.append(pair)\n",
    "        \n",
    "        print(f\"✓ Loaded {len(self.pairs)} pairs from SPair-71k {self.split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        src_img = Image.open(pair['src_img']).convert('RGB')\n",
    "        tgt_img = Image.open(pair['tgt_img']).convert('RGB')\n",
    "        \n",
    "        return {\n",
    "            'src_image': src_img,\n",
    "            'tgt_image': tgt_img,\n",
    "            'src_keypoints': pair['src_kps'],\n",
    "            'tgt_keypoints': pair['tgt_kps'],\n",
    "            'src_bbox': pair['src_bbox'],\n",
    "            'tgt_bbox': pair['tgt_bbox'],\n",
    "            'category': pair['category']\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset loaders defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2623fbd",
   "metadata": {},
   "source": [
    "## Visualization and Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_correspondences(src_img, tgt_img, src_kps, pred_kps, gt_kps=None, \n",
    "                              max_points=15, save_path=None):\n",
    "    \"\"\"Visualize correspondence matches.\"\"\"\n",
    "    if isinstance(src_img, Image.Image):\n",
    "        src_img = np.array(src_img)\n",
    "    if isinstance(tgt_img, Image.Image):\n",
    "        tgt_img = np.array(tgt_img)\n",
    "    \n",
    "    # Ensure all arrays are properly shaped 2D arrays (N, 2)\n",
    "    src_kps = np.atleast_2d(src_kps)\n",
    "    if src_kps.shape[0] == 2 and src_kps.shape[1] != 2:  # If shape is (2, M) where M > 2\n",
    "        src_kps = src_kps.T\n",
    "    \n",
    "    pred_kps = np.atleast_2d(pred_kps)\n",
    "    if pred_kps.shape[0] == 2 and pred_kps.shape[1] != 2:\n",
    "        pred_kps = pred_kps.T\n",
    "    \n",
    "    if gt_kps is not None:\n",
    "        gt_kps = np.atleast_2d(gt_kps)\n",
    "        if gt_kps.shape[0] == 2 and gt_kps.shape[1] != 2:\n",
    "            gt_kps = gt_kps.T\n",
    "    \n",
    "    # Ensure all have same number of points by truncating to minimum\n",
    "    min_points = min(len(src_kps), len(pred_kps))\n",
    "    if gt_kps is not None:\n",
    "        min_points = min(min_points, len(gt_kps))\n",
    "    \n",
    "    src_kps = src_kps[:min_points]\n",
    "    pred_kps = pred_kps[:min_points]\n",
    "    if gt_kps is not None:\n",
    "        gt_kps = gt_kps[:min_points]\n",
    "    \n",
    "    # Subsample if needed\n",
    "    if len(src_kps) > max_points:\n",
    "        indices = np.random.choice(len(src_kps), max_points, replace=False)\n",
    "        src_kps = src_kps[indices]\n",
    "        pred_kps = pred_kps[indices]\n",
    "        if gt_kps is not None:\n",
    "            gt_kps = gt_kps[indices]\n",
    "    \n",
    "    ncols = 3 if gt_kps is not None else 2\n",
    "    fig, axes = plt.subplots(1, ncols, figsize=(6*ncols, 6))\n",
    "    if ncols == 2:\n",
    "        axes = [axes[0], axes[1]]\n",
    "    \n",
    "    # Left: source image with source keypoints\n",
    "    axes[0].imshow(src_img)\n",
    "    if len(src_kps) > 0:\n",
    "        axes[0].scatter(src_kps[:, 0], src_kps[:, 1], c='red', s=100, \n",
    "                        edgecolors='white', linewidths=2, marker='o')\n",
    "    axes[0].set_title('Source Image', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Middle: target image with predicted keypoints\n",
    "    axes[1].imshow(tgt_img)\n",
    "    valid_pred = ~np.isnan(pred_kps).any(axis=1)\n",
    "    if valid_pred.sum() > 0:\n",
    "        axes[1].scatter(pred_kps[valid_pred, 0], pred_kps[valid_pred, 1], c='blue', s=100, \n",
    "                        marker='x', linewidths=3)\n",
    "    axes[1].set_title('Target (Predictions)', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Right: target image with GT vs Pred comparison\n",
    "    if gt_kps is not None and ncols == 3:\n",
    "        axes[2].imshow(tgt_img)\n",
    "        valid_gt = ~np.isnan(gt_kps).any(axis=1)\n",
    "        \n",
    "        # Plot GT keypoints\n",
    "        if valid_gt.sum() > 0:\n",
    "            axes[2].scatter(gt_kps[valid_gt, 0], gt_kps[valid_gt, 1], c='green', s=100, \n",
    "                           edgecolors='white', linewidths=2, marker='o', label='GT')\n",
    "        \n",
    "        # Plot predicted keypoints\n",
    "        if valid_pred.sum() > 0:\n",
    "            axes[2].scatter(pred_kps[valid_pred, 0], pred_kps[valid_pred, 1], c='blue', s=50, \n",
    "                           marker='x', linewidths=2, alpha=0.7, label='Pred')\n",
    "        \n",
    "        # Draw lines and compute error only for indices valid in BOTH\n",
    "        valid_both = valid_pred & valid_gt\n",
    "        if valid_both.sum() > 0:\n",
    "            for i in np.where(valid_both)[0]:\n",
    "                axes[2].plot([gt_kps[i, 0], pred_kps[i, 0]], \n",
    "                           [gt_kps[i, 1], pred_kps[i, 1]], \n",
    "                           'r--', alpha=0.3, linewidth=1)\n",
    "            \n",
    "            try:\n",
    "                errors = np.linalg.norm(pred_kps[valid_both] - gt_kps[valid_both], axis=1)\n",
    "                mean_error = errors.mean()\n",
    "            except ValueError:\n",
    "                mean_error = 0\n",
    "        else:\n",
    "            mean_error = 0\n",
    "        \n",
    "        axes[2].set_title(f'GT vs Pred (Mean Error: {mean_error:.1f}px)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[2].legend(loc='upper right')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"✓ Visualization utilities ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46883774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_dataset(dataset, feature_extractor, matcher, evaluator, \n",
    "                       max_samples=None, save_visualizations=True):\n",
    "    \"\"\"Complete evaluation pipeline.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"EVALUATING SAM ON {dataset.__class__.__name__}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    num_samples = min(max_samples, len(dataset)) if max_samples else len(dataset)\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    print(f\"Evaluating: {num_samples} samples\\n\")\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    image_sizes = []\n",
    "    bboxes = []\n",
    "    confidences = []\n",
    "    \n",
    "    for i in tqdm(range(num_samples), desc=\"Processing\"):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        src_img = sample['src_image']\n",
    "        tgt_img = sample['tgt_image']\n",
    "        src_kps = sample['src_keypoints']\n",
    "        tgt_kps = sample['tgt_keypoints']\n",
    "        \n",
    "        if len(src_kps) == 0 or len(tgt_kps) == 0:\n",
    "            continue\n",
    "        \n",
    "        pred_kps, conf = matcher.match_keypoints(\n",
    "            src_img, tgt_img, src_kps, feature_extractor\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred_kps)\n",
    "        ground_truths.append(tgt_kps)\n",
    "        confidences.append(conf)\n",
    "        image_sizes.append(tgt_img.size)\n",
    "        \n",
    "        if 'tgt_bbox' in sample and len(sample['tgt_bbox']) > 0:\n",
    "            bboxes.append(sample['tgt_bbox'])\n",
    "        else:\n",
    "            bboxes.append(None)\n",
    "        \n",
    "        if save_visualizations and i < 5:\n",
    "            vis_path = os.path.join(OUTPUT_DIR, f'sample_{i}.png')\n",
    "            visualize_correspondences(src_img, tgt_img, src_kps, pred_kps, \n",
    "                                    tgt_kps, save_path=vis_path)\n",
    "            plt.close()\n",
    "    \n",
    "    results = evaluator.evaluate_batch(predictions, ground_truths, image_sizes, bboxes)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Samples evaluated: {results['num_samples']}\")\n",
    "    print(\"\\nPCK Scores:\")\n",
    "    for metric, value in sorted(results['mean'].items()):\n",
    "        print(f\"  {metric}: {value*100:.2f}%\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results_file = os.path.join(OUTPUT_DIR, 'evaluation_results.json')\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'backbone': 'SAM ViT-B',\n",
    "            'dataset': dataset.__class__.__name__,\n",
    "            'num_samples': results['num_samples'],\n",
    "            'mean_pck': results['mean'],\n",
    "            'per_sample_pck': results['per_sample']\n",
    "        }, f, indent=2)\n",
    "    print(f\"\\n✓ Results saved to {results_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Evaluation pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84acbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_FINETUNING:\n",
    "    print(\"=\"*80)\n",
    "    print(\"LIGHT FINETUNING ENABLED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Setup paths\n",
    "    SPAIR_PATH = os.path.join(DATA_ROOT, 'SPair-71k')\n",
    "    \n",
    "    if not os.path.exists(SPAIR_PATH):\n",
    "        print(f\"\\n✗ SPair-71k not found at: {SPAIR_PATH}\")\n",
    "        print(\"Please download SPair-71k dataset first.\")\n",
    "        print(\"You can continue with pre-trained model.\")\n",
    "    else:\n",
    "        print(f\"\\n✓ SPair-71k found at: {SPAIR_PATH}\")\n",
    "        \n",
    "        # Create dataloaders\n",
    "        print(f\"\\nCreating SPair-71k dataloaders...\")\n",
    "        print(f\"  Image size: 1024 (SAM default)\")\n",
    "        print(f\"  Batch size: {FT_BATCH_SIZE}\")\n",
    "        \n",
    "        train_loader, val_loader = create_spair_dataloaders(\n",
    "            root_dir=SPAIR_PATH,\n",
    "            batch_size=FT_BATCH_SIZE,\n",
    "            num_workers=2,\n",
    "            train_subset=None,  # Use full dataset\n",
    "            val_subset=None\n",
    "        )\n",
    "        \n",
    "        print(f\"  Train batches: {len(train_loader)}\")\n",
    "        print(f\"  Val batches: {len(val_loader)}\")\n",
    "        \n",
    "        # Unfreeze last k blocks of SAM image encoder\n",
    "        print(f\"\\nUnfreezing last {FT_K_LAYERS} transformer blocks...\")\n",
    "        sam_model.train()\n",
    "        \n",
    "        # Freeze all parameters first\n",
    "        for param in sam_model.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze last k blocks\n",
    "        total_blocks = len(sam_model.image_encoder.blocks)\n",
    "        for i in range(total_blocks - FT_K_LAYERS, total_blocks):\n",
    "            for param in sam_model.image_encoder.blocks[i].parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Count trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in sam_model.image_encoder.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in sam_model.image_encoder.parameters())\n",
    "        print(f\"  Trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, sam_model.image_encoder.parameters()),\n",
    "            lr=FT_LEARNING_RATE,\n",
    "            weight_decay=FT_WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        print(f\"\\nStarting finetuning for {FT_NUM_EPOCHS} epochs...\")\n",
    "        best_val_loss = float('inf')\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, 'sam_finetuned.pth')\n",
    "        \n",
    "        for epoch in range(FT_NUM_EPOCHS):\n",
    "            # Training\n",
    "            sam_model.train()\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                src_imgs = batch['src_img'].to(device)\n",
    "                tgt_imgs = batch['tgt_img'].to(device)\n",
    "                src_kps = batch['src_kps'].to(device)  # [B, N, 2]\n",
    "                tgt_kps = batch['tgt_kps'].to(device)  # [B, N, 2]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Extract features\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    src_feats = sam_model.image_encoder(src_imgs)  # [B, 256, 64, 64]\n",
    "                    tgt_feats = sam_model.image_encoder(tgt_imgs)  # [B, 256, 64, 64]\n",
    "                \n",
    "                B, C, H, W = src_feats.shape\n",
    "                \n",
    "                # Map keypoints to feature grid (1024 -> 64)\n",
    "                src_kps_scaled = src_kps / 16.0  # SAM: patch_size=16\n",
    "                tgt_kps_scaled = tgt_kps / 16.0\n",
    "                \n",
    "                # Clamp to valid range\n",
    "                src_kps_scaled = torch.clamp(src_kps_scaled, 0, H-1)\n",
    "                tgt_kps_scaled = torch.clamp(tgt_kps_scaled, 0, W-1)\n",
    "                \n",
    "                # Convert to integer indices\n",
    "                src_y = src_kps_scaled[:, :, 1].long()  # [B, N]\n",
    "                src_x = src_kps_scaled[:, :, 0].long()  # [B, N]\n",
    "                tgt_y = tgt_kps_scaled[:, :, 1].long()\n",
    "                tgt_x = tgt_kps_scaled[:, :, 0].long()\n",
    "                \n",
    "                # Extract source keypoint features\n",
    "                src_kp_feats = []\n",
    "                for b in range(B):\n",
    "                    feats = src_feats[b, :, src_y[b], src_x[b]].T  # [N, C]\n",
    "                    src_kp_feats.append(feats)\n",
    "                src_kp_feats = torch.stack(src_kp_feats)  # [B, N, C]\n",
    "                \n",
    "                # Compute similarity with all target locations\n",
    "                tgt_feats_flat = tgt_feats.view(B, C, -1)  # [B, C, H*W]\n",
    "                similarity = torch.bmm(src_kp_feats, tgt_feats_flat)  # [B, N, H*W]\n",
    "                \n",
    "                # Find predicted locations\n",
    "                pred_indices = similarity.argmax(dim=2)  # [B, N]\n",
    "                pred_y = pred_indices // W\n",
    "                pred_x = pred_indices % W\n",
    "                pred_kps = torch.stack([pred_x, pred_y], dim=2).float()  # [B, N, 2]\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = compute_keypoint_loss(pred_kps, tgt_kps_scaled)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Epoch {epoch+1}/{FT_NUM_EPOCHS} | Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            sam_model.eval()\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    src_imgs = batch['src_img'].to(device)\n",
    "                    tgt_imgs = batch['tgt_img'].to(device)\n",
    "                    src_kps = batch['src_kps'].to(device)\n",
    "                    tgt_kps = batch['tgt_kps'].to(device)\n",
    "                    \n",
    "                    src_feats = sam_model.image_encoder(src_imgs)\n",
    "                    tgt_feats = sam_model.image_encoder(tgt_imgs)\n",
    "                    \n",
    "                    B, C, H, W = src_feats.shape\n",
    "                    \n",
    "                    src_kps_scaled = torch.clamp(src_kps / 16.0, 0, H-1)\n",
    "                    tgt_kps_scaled = torch.clamp(tgt_kps / 16.0, 0, W-1)\n",
    "                    \n",
    "                    src_y = src_kps_scaled[:, :, 1].long()\n",
    "                    src_x = src_kps_scaled[:, :, 0].long()\n",
    "                    \n",
    "                    src_kp_feats = []\n",
    "                    for b in range(B):\n",
    "                        feats = src_feats[b, :, src_y[b], src_x[b]].T\n",
    "                        src_kp_feats.append(feats)\n",
    "                    src_kp_feats = torch.stack(src_kp_feats)\n",
    "                    \n",
    "                    tgt_feats_flat = tgt_feats.view(B, C, -1)\n",
    "                    similarity = torch.bmm(src_kp_feats, tgt_feats_flat)\n",
    "                    \n",
    "                    pred_indices = similarity.argmax(dim=2)\n",
    "                    pred_y = pred_indices // W\n",
    "                    pred_x = pred_indices % W\n",
    "                    pred_kps = torch.stack([pred_x, pred_y], dim=2).float()\n",
    "                    \n",
    "                    loss = compute_keypoint_loss(pred_kps, tgt_kps_scaled)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{FT_NUM_EPOCHS} Summary:\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': sam_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': avg_val_loss,\n",
    "                }, checkpoint_path)\n",
    "                print(f\"  ✓ Saved best model (val_loss: {best_val_loss:.4f})\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"FINETUNING COMPLETE\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "        print(f\"Model saved to: {checkpoint_path}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Set back to eval mode\n",
    "        sam_model.eval()\n",
    "else:\n",
    "    print(\"\\nFinetuning disabled (ENABLE_FINETUNING=False)\")\n",
    "    print(\"Using pre-trained SAM model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6aa50b",
   "metadata": {},
   "source": [
    "## Section 5: Light Finetuning (Optional)\n",
    "\n",
    "If `ENABLE_FINETUNING=True`, finetune SAM's image encoder last blocks on SPair-71k with keypoint supervision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d036c",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Uncomment to run evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate\n",
    "spair_test = SPairDataset(\n",
    "    root_dir=os.path.join(DATA_ROOT, 'SPair-71k'),  \n",
    "    split='test'\n",
    ")\n",
    "\n",
    "results = evaluate_on_dataset(\n",
    "    dataset=spair_test,\n",
    "    feature_extractor=feature_extractor,\n",
    "    matcher=matcher,\n",
    "    evaluator=evaluator,\n",
    "    max_samples=4,\n",
    "    save_visualizations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208262f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### SAM Implementation Complete ✓\n",
    "\n",
    "**Implementation:**\n",
    "1. ✓ Cross-platform environment setup\n",
    "2. ✓ SAM ViT-B model and checkpoint download\n",
    "3. ✓ Dense feature extraction (64×64×256)\n",
    "4. ✓ Correspondence matching\n",
    "5. ✓ PCK evaluation\n",
    "6. ✓ Dataset loaders\n",
    "7. ✓ Visualization tools\n",
    "8. ✓ Complete pipeline\n",
    "\n",
    "**SAM Advantages:**\n",
    "- **Higher spatial resolution**: 64×64 features vs 16×16 (DINO)\n",
    "- **Task-specific training**: Trained for dense prediction tasks\n",
    "- **Strong boundaries**: Excellent at detecting object boundaries\n",
    "- **Large-scale data**: 11M images, 1.1B masks\n",
    "\n",
    "**Trade-offs:**\n",
    "- Lower feature dimension (256 vs 768)\n",
    "- Larger input size (1024 vs 224) → slower\n",
    "- More memory intensive\n",
    "\n",
    "**Expected Performance:**\n",
    "- Potentially better localization accuracy (higher resolution)\n",
    "- Strong for objects with clear boundaries\n",
    "- May excel on geometric transformations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
