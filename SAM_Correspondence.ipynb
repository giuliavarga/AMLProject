{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27a8f54",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb402999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and configure paths\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"✓ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"✓ Running locally\")\n",
    "\n",
    "# Set up paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = '/content/AMLProject'\n",
    "    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "\n",
    "# Create necessary directories\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints', 'sam')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'sam')\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "\n",
    "for directory in [CHECKPOINT_DIR, OUTPUT_DIR, MODEL_DIR, DATA_ROOT]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (robust, platform-aware with clear errors)\n",
    "import subprocess, sys, platform\n",
    "print(\"Installing required packages...\")\n",
    "# Install Segment Anything first (from git)\n",
    "try:\n",
    "    print(\"Installing Segment Anything (segment-anything)...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'git+https://github.com/facebookresearch/segment-anything.git'])\n",
    "    print(\"✓ segment-anything installed\")\n",
    "except Exception as e:\n",
    "    print(\"You can install it manually with:\")\n",
    "    print(\"  pip install git+https://github.com/facebookresearch/segment-anything.git\")\n",
    "\n",
    "# Common Python packages (install separately to isolate failures)\n",
    "common_packages = [\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'opencv-python',\n",
    "    'pillow',\n",
    "    'scipy',\n",
    "    'tqdm',\n",
    "    'pandas',\n",
    "    'scikit-learn'\n",
    "]\n",
    "try:\n",
    "    print(\"Installing common Python packages...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade'] + common_packages)\n",
    "    print(\"✓ Common packages installed\")\n",
    "except Exception as e:\n",
    "    print(\"You can install them manually, e.g.:\")\n",
    "    print(\"  pip install \")\n",
    " \n",
    "\n",
    "print(\"Installing PyTorch (torch, torchvision, torchaudio)...\")\n",
    "try:\n",
    "    if platform.system() == 'Darwin':\n",
    "        # macOS: pip usually installs the correct (CPU/MPS) wheel or user can use conda\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', 'torch', 'torchvision', 'torchaudio'])\n",
    "    else:\n",
    "        # Linux/Windows: prefer official CUDA wheel index (adjust if you need a different CUDA version)\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--index-url', 'https://download.pytorch.org/whl/cu121', 'torch', 'torchvision', 'torchaudio'])\n",
    "    print(\"✓ PyTorch packages installed\")\n",
    "except Exception as e:\n",
    "    print(\"Attempting CPU-only PyTorch installation as a fallback...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--index-url', 'https://download.pytorch.org/whl/cpu', 'torch', 'torchvision', 'torchaudio'])\n",
    "        print(\"✓ CPU-only PyTorch installed\")\n",
    "    except Exception as e2:\n",
    "        print(\"Please follow the official instructions at https://pytorch.org/get-started/locally/ to install a compatible wheel for your system.\")\n",
    "\n",
    "import sys, subprocess\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"scikit-learn\"])\n",
    "    print(\"✓ scikit-learn installed (or already up-to-date)\")\n",
    "except Exception as e:\n",
    "    print(\"✗ pip install failed:\", e)\n",
    "\n",
    "print(\"\\nInstallation step finished. If any package failed, rerun the cell without '--quiet' or install manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Import SAM\n",
    "try:\n",
    "    from segment_anything import sam_model_registry, SamPredictor\n",
    "    from segment_anything.utils.transforms import ResizeLongestSide\n",
    "    print(\"✓ SAM imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Error importing SAM: {e}\")\n",
    "    print(\"  Please ensure segment-anything is installed\")\n",
    "    raise\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a960a6d",
   "metadata": {},
   "source": [
    "## Section 2: Download and Load SAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SAM checkpoint\n",
    "import urllib.request\n",
    "\n",
    "SAM_CHECKPOINT_URL = 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth'\n",
    "SAM_CHECKPOINT_NAME = 'sam_vit_b_01ec64.pth'\n",
    "sam_checkpoint_path = os.path.join(CHECKPOINT_DIR, SAM_CHECKPOINT_NAME)\n",
    "\n",
    "print(\"Checking SAM checkpoint...\")\n",
    "if os.path.exists(sam_checkpoint_path):\n",
    "    print(f\"✓ Checkpoint already exists: {sam_checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Downloading SAM ViT-B checkpoint...\")\n",
    "    print(f\"URL: {SAM_CHECKPOINT_URL}\")\n",
    "    print(f\"This may take a few minutes (~375 MB)...\")\n",
    "    \n",
    "    try:\n",
    "        urllib.request.urlretrieve(SAM_CHECKPOINT_URL, sam_checkpoint_path)\n",
    "        print(f\"✓ Downloaded successfully to: {sam_checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Download failed: {e}\")\n",
    "        print(\"\\nPlease download manually:\")\n",
    "        print(f\"  URL: {SAM_CHECKPOINT_URL}\")\n",
    "        print(f\"  Save to: {sam_checkpoint_path}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24164fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM model\n",
    "print(\"Loading SAM ViT-B model...\")\n",
    "\n",
    "try:\n",
    "    sam_model = sam_model_registry['vit_b'](checkpoint=sam_checkpoint_path)\n",
    "    sam_model = sam_model.to(device)\n",
    "    sam_model.eval()\n",
    "    \n",
    "    # Create predictor (optional, for segmentation tasks)\n",
    "    sam_predictor = SamPredictor(sam_model)\n",
    "    \n",
    "    print(\"✓ SAM model loaded successfully!\")\n",
    "    print(f\"  - Architecture: ViT-B (Base)\")\n",
    "    print(f\"  - Image encoder output: 64×64 feature map\")\n",
    "    print(f\"  - Feature dimension: 256\")\n",
    "    print(f\"  - Input size: 1024×1024 (longest side)\")\n",
    "    print(f\"  - Device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading SAM: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fe3286",
   "metadata": {},
   "source": [
    "## Section 3: Dense Feature Extraction\n",
    "\n",
    "### SAM Feature Extraction Strategy\n",
    "\n",
    "SAM's image encoder produces high-quality dense features:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Resize longest side to 1024 pixels\n",
    "   - Pad to square (1024×1024)\n",
    "   - Normalize with ImageNet statistics\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - Extract from image encoder (ViT backbone)\n",
    "   - Output: 64×64×256 feature map\n",
    "   - Features encode both semantic and spatial information\n",
    "\n",
    "3. **Key Differences from DINO**:\n",
    "   - Larger input resolution (1024 vs 224)\n",
    "   - Lower feature dimensionality (256 vs 768)\n",
    "   - Designed for segmentation (may be better for spatial tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fa8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract dense spatial features from SAM's image encoder.\n",
    "    \n",
    "    SAM is optimized for dense prediction, making it potentially\n",
    "    excellent for correspondence tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.image_encoder = model.image_encoder\n",
    "        self.img_size = self.image_encoder.img_size  # 1024\n",
    "        self.feat_dim = 256  # SAM feature dimension\n",
    "        \n",
    "        # SAM's preprocessing transform\n",
    "        self.transform = ResizeLongestSide(self.img_size)\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        Preprocess image following SAM's requirements: resize + pad to square.\n",
    "        Returns:\n",
    "            input_tensor: torch.Tensor shaped [1, 3, 1024, 1024] on device with correct dtype\n",
    "            original_size: (H, W) of original image\n",
    "        \"\"\"\n",
    "        # Convert to numpy\n",
    "        if isinstance(image, Image.Image):\n",
    "            image_np = np.array(image)\n",
    "            original_size = (image.height, image.width)\n",
    "        else:\n",
    "            image_np = image\n",
    "            original_size = (image_np.shape[0], image_np.shape[1])\n",
    "\n",
    "        # Apply SAM's transform (resize longest side to 1024)\n",
    "        input_image = self.transform.apply_image(image_np)  # HxWxC, uint8\n",
    "        \n",
    "        # Convert to tensor [C, H, W]\n",
    "        input_tensor = torch.as_tensor(input_image, dtype=torch.float32).permute(2, 0, 1).contiguous()\n",
    "        \n",
    "        # Pad to square (1024x1024) - CRITICAL for SAM\n",
    "        h, w = input_tensor.shape[-2:]\n",
    "        padh = self.img_size - h\n",
    "        padw = self.img_size - w\n",
    "        input_tensor = F.pad(input_tensor, (0, padw, 0, padh))  # pad right and bottom\n",
    "        \n",
    "        # Add batch dimension [1, 3, 1024, 1024]\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Move to device and match model dtype\n",
    "        try:\n",
    "            param_dtype = next(self.image_encoder.parameters()).dtype\n",
    "        except StopIteration:\n",
    "            param_dtype = torch.float32\n",
    "        \n",
    "        input_tensor = input_tensor.to(self.device).to(param_dtype)\n",
    "        \n",
    "        return input_tensor, original_size\n",
    "    \n",
    "    def extract_features(self, image, normalize=True):\n",
    "        \"\"\"\n",
    "        Extract dense feature map from image.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or numpy array\n",
    "            normalize: Apply L2 normalization\n",
    "            \n",
    "        Returns:\n",
    "            features: [H, W, D] numpy array (64×64×256)\n",
    "            info: Metadata dictionary\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        img_tensor, original_size = self.preprocess_image(image)\n",
    "        \n",
    "        # Extract features from image encoder\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.image_encoder(img_tensor)  # [1, 256, 64, 64]\n",
    "        \n",
    "        # Rearrange to [H, W, D]\n",
    "        features = image_embedding[0].permute(1, 2, 0)  # [64, 64, 256]\n",
    "        \n",
    "        # L2 normalize\n",
    "        if normalize:\n",
    "            features = F.normalize(features, p=2, dim=-1)\n",
    "        \n",
    "        features = features.cpu().numpy()\n",
    "        \n",
    "        h, w = features.shape[0], features.shape[1]\n",
    "        \n",
    "        # Get original image size\n",
    "        if isinstance(image, Image.Image):\n",
    "            orig_w, orig_h = image.size\n",
    "        else:\n",
    "            orig_h, orig_w = image.shape[:2]\n",
    "        \n",
    "        info = {\n",
    "            'original_size': (orig_w, orig_h),\n",
    "            'feature_size': (w, h),\n",
    "            'processed_size': original_size,\n",
    "            'scale_x': w / orig_w,\n",
    "            'scale_y': h / orig_h\n",
    "        }\n",
    "        \n",
    "        return features, info\n",
    "    \n",
    "    def map_coords_to_features(self, coords, info):\n",
    "        \"\"\"Map image coordinates to feature space.\"\"\"\n",
    "        coords = np.array(coords).astype(float)\n",
    "        feat_coords = coords.copy()\n",
    "        feat_coords[:, 0] *= info['scale_x']\n",
    "        feat_coords[:, 1] *= info['scale_y']\n",
    "        return feat_coords\n",
    "    \n",
    "    def extract_keypoint_features(self, image, keypoints):\n",
    "        \"\"\"\n",
    "        Extract features at specific keypoint locations.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            keypoints: [N, 2] array of (x, y) coordinates\n",
    "            \n",
    "        Returns:\n",
    "            kp_features: [N, D] feature vectors\n",
    "        \"\"\"\n",
    "        features, info = self.extract_features(image, normalize=True)\n",
    "        h, w, d = features.shape\n",
    "        \n",
    "        # Map to feature space\n",
    "        feat_kps = self.map_coords_to_features(keypoints, info)\n",
    "        \n",
    "        # Clip and round\n",
    "        feat_kps[:, 0] = np.clip(feat_kps[:, 0], 0, w - 1)\n",
    "        feat_kps[:, 1] = np.clip(feat_kps[:, 1], 0, h - 1)\n",
    "        feat_kps = np.round(feat_kps).astype(int)\n",
    "        \n",
    "        # Extract features\n",
    "        kp_features = features[feat_kps[:, 1], feat_kps[:, 0], :]\n",
    "        \n",
    "        return kp_features\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = SAMFeatureExtractor(sam_model, device=device)\n",
    "print(\"✓ SAM feature extractor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62208668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature extraction\n",
    "print(\"Testing SAM feature extraction...\")\n",
    "test_image = Image.new('RGB', (480, 640), color=(128, 128, 128))\n",
    "\n",
    "features, info = feature_extractor.extract_features(test_image)\n",
    "print(f\"\\n✓ Feature extraction successful!\")\n",
    "print(f\"  Input image size: {info['original_size']}\")\n",
    "print(f\"  Feature map size: {info['feature_size']} = {features.shape[0]}×{features.shape[1]}\")\n",
    "print(f\"  Feature dimension: {features.shape[2]}\")\n",
    "print(f\"  Features normalized: {np.allclose(np.linalg.norm(features[0, 0, :]), 1.0)}\")\n",
    "print(f\"\\n  Note: SAM uses 64×64 feature grid (higher resolution than DINO's 16×16)\")\n",
    "\n",
    "# Test keypoint features\n",
    "test_kps = np.array([[100, 150], [200, 300], [400, 500]])\n",
    "kp_features = feature_extractor.extract_keypoint_features(test_image, test_kps)\n",
    "print(f\"\\n✓ Keypoint feature extraction successful!\")\n",
    "print(f\"  Number of keypoints: {len(test_kps)}\")\n",
    "print(f\"  Feature shape: {kp_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881fcd5",
   "metadata": {},
   "source": [
    "## Section 3: Correspondence Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrespondenceMatcher:\n",
    "    \"\"\"\n",
    "    Match keypoints between images using dense feature similarity.\n",
    "    \n",
    "    SAM's higher spatial resolution (64×64 vs 16×16) may provide\n",
    "    more accurate localization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mutual_nn=False, ratio_threshold=None):\n",
    "        self.mutual_nn = mutual_nn\n",
    "        self.ratio_threshold = ratio_threshold\n",
    "    \n",
    "    def match(self, src_features, tgt_features_map, return_scores=True):\n",
    "        \"\"\"Match source features to target feature map.\"\"\"\n",
    "        h, w, d = tgt_features_map.shape\n",
    "        tgt_flat = tgt_features_map.reshape(-1, d)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = src_features @ tgt_flat.T\n",
    "        \n",
    "        # Find best matches\n",
    "        best_indices = np.argmax(similarity, axis=1)\n",
    "        best_scores = np.max(similarity, axis=1)\n",
    "        \n",
    "        # Ratio test\n",
    "        if self.ratio_threshold is not None:\n",
    "            sorted_sim = np.sort(similarity, axis=1)[:, ::-1]\n",
    "            ratios = sorted_sim[:, 0] / (sorted_sim[:, 1] + 1e-8)\n",
    "            valid_mask = ratios > self.ratio_threshold\n",
    "            best_indices[~valid_mask] = -1\n",
    "        \n",
    "        # Mutual nearest neighbor\n",
    "        if self.mutual_nn:\n",
    "            reverse_sim = tgt_flat @ src_features.T\n",
    "            reverse_best = np.argmax(reverse_sim, axis=1)\n",
    "            \n",
    "            for i, tgt_idx in enumerate(best_indices):\n",
    "                if tgt_idx >= 0 and reverse_best[tgt_idx] != i:\n",
    "                    best_indices[i] = -1\n",
    "        \n",
    "        # Convert to coordinates\n",
    "        matched_y = best_indices // w\n",
    "        matched_x = best_indices % w\n",
    "        matched_coords = np.stack([matched_x, matched_y], axis=1).astype(float)\n",
    "        \n",
    "        invalid = best_indices < 0\n",
    "        matched_coords[invalid] = np.nan\n",
    "        \n",
    "        if return_scores:\n",
    "            return matched_coords, best_scores\n",
    "        return matched_coords\n",
    "    \n",
    "    def match_keypoints(self, src_image, tgt_image, src_keypoints, feature_extractor):\n",
    "        \"\"\"End-to-end keypoint matching.\"\"\"\n",
    "        # Extract features\n",
    "        src_features = feature_extractor.extract_keypoint_features(src_image, src_keypoints)\n",
    "        tgt_features_map, tgt_info = feature_extractor.extract_features(tgt_image, normalize=True)\n",
    "        \n",
    "        # Match\n",
    "        matched_coords_feat, confidence = self.match(src_features, tgt_features_map, return_scores=True)\n",
    "        \n",
    "        # Map back to original coordinates\n",
    "        tgt_w, tgt_h = tgt_info['original_size']\n",
    "        feat_w, feat_h = tgt_info['feature_size']\n",
    "        \n",
    "        tgt_keypoints = matched_coords_feat.copy()\n",
    "        tgt_keypoints[:, 0] = matched_coords_feat[:, 0] * (tgt_w / feat_w)\n",
    "        tgt_keypoints[:, 1] = matched_coords_feat[:, 1] * (tgt_h / feat_h)\n",
    "        \n",
    "        return tgt_keypoints, confidence\n",
    "\n",
    "matcher = CorrespondenceMatcher(mutual_nn=False, ratio_threshold=None)\n",
    "print(\"✓ Correspondence matcher initialized\")\n",
    "print(f\"  - Method: Nearest Neighbor\")\n",
    "print(f\"  - Mutual NN: {matcher.mutual_nn}\")\n",
    "print(f\"  - Ratio test: {matcher.ratio_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7d102",
   "metadata": {},
   "source": [
    "## Section 3: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCKEvaluator:\n",
    "    \"\"\"PCK (Percentage of Correct Keypoints) evaluator.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_values=[0.05, 0.10, 0.15], use_bbox=True):\n",
    "        self.alpha_values = alpha_values\n",
    "        self.use_bbox = use_bbox\n",
    "    \n",
    "    def compute_pck(self, predicted_kps, gt_kps, image_size=None, bbox=None):\n",
    "        \"\"\"Compute PCK for single image pair.\"\"\"\n",
    "        valid_mask = ~np.isnan(predicted_kps).any(axis=1) & ~np.isnan(gt_kps).any(axis=1)\n",
    "        if valid_mask.sum() == 0:\n",
    "            return {f'PCK@{alpha:.2f}': 0.0 for alpha in self.alpha_values}\n",
    "        \n",
    "        pred = predicted_kps[valid_mask]\n",
    "        gt = gt_kps[valid_mask]\n",
    "        \n",
    "        distances = np.linalg.norm(pred - gt, axis=1)\n",
    "        \n",
    "        if self.use_bbox and bbox is not None and len(bbox) >= 4:\n",
    "            norm_factor = np.sqrt(bbox[2]**2 + bbox[3]**2)\n",
    "        elif image_size is not None:\n",
    "            norm_factor = np.sqrt(image_size[0]**2 + image_size[1]**2)\n",
    "        else:\n",
    "            norm_factor = 1.0\n",
    "        \n",
    "        pck_dict = {}\n",
    "        for alpha in self.alpha_values:\n",
    "            threshold = alpha * norm_factor\n",
    "            correct = (distances <= threshold).sum()\n",
    "            pck = correct / len(distances) if len(distances) > 0 else 0.0\n",
    "            pck_dict[f'PCK@{alpha:.2f}'] = pck\n",
    "        \n",
    "        return pck_dict\n",
    "    \n",
    "    def evaluate_batch(self, predictions, ground_truths, image_sizes=None, bboxes=None):\n",
    "        \"\"\"Evaluate multiple image pairs.\"\"\"\n",
    "        all_pck = {f'PCK@{alpha:.2f}': [] for alpha in self.alpha_values}\n",
    "        per_sample = []\n",
    "        \n",
    "        for i in range(len(predictions)):\n",
    "            img_size = image_sizes[i] if image_sizes else None\n",
    "            bbox = bboxes[i] if bboxes else None\n",
    "            \n",
    "            pck = self.compute_pck(predictions[i], ground_truths[i], img_size, bbox)\n",
    "            per_sample.append(pck)\n",
    "            \n",
    "            for key, value in pck.items():\n",
    "                all_pck[key].append(value)\n",
    "        \n",
    "        mean_pck = {key: np.mean(values) for key, values in all_pck.items()}\n",
    "        \n",
    "        return {\n",
    "            'mean': mean_pck,\n",
    "            'per_sample': per_sample,\n",
    "            'num_samples': len(predictions)\n",
    "        }\n",
    "\n",
    "evaluator = PCKEvaluator(alpha_values=[0.05, 0.10, 0.15], use_bbox=True)\n",
    "print(\"✓ PCK evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb435eeb",
   "metadata": {},
   "source": [
    "## Dataset Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0341ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup\n",
    "def setup_datasets(data_root):\n",
    "    \"\"\"Setup benchmark datasets.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DATASET SETUP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n⚠️  Please download datasets manually:\")\n",
    "    print(\"\\n1. PF-Pascal: https://www.di.ens.fr/willow/research/proposalflow/\")\n",
    "    print(f\"   → Extract to: {data_root}/pf-pascal/\")\n",
    "    print(\"\\n2. SPair-71k: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
    "    print(f\"   → Extract to: {data_root}/spair-71k/\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "setup_datasets(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee97ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPair-71k dataset loader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    \"\"\"SPair-71k dataset loader.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', category=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.category = category\n",
    "        self.pairs = []\n",
    "        self._load_annotations()\n",
    "    \n",
    "    def _load_annotations(self):\n",
    "        anno_dir = self.root_dir / 'PairAnnotation' / self.split\n",
    "        \n",
    "        if not anno_dir.exists():\n",
    "            print(f\"⚠️  Annotations not found: {anno_dir}\")\n",
    "            return\n",
    "        \n",
    "        for anno_file in sorted(anno_dir.glob('*.json')):\n",
    "            with open(anno_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if self.category and data.get('category') != self.category:\n",
    "                continue\n",
    "            \n",
    "            # Image paths are: JPEGImages/<category>/<image_name>\n",
    "            cat = data.get('category', 'unknown')\n",
    "            pair = {\n",
    "                'src_img': str(self.root_dir / 'JPEGImages' / cat / data['src_imname']),\n",
    "                'tgt_img': str(self.root_dir / 'JPEGImages' / cat / data['trg_imname']),\n",
    "                'src_kps': np.array(data['src_kps']).T,\n",
    "                'tgt_kps': np.array(data['trg_kps']).T,\n",
    "                'src_bbox': np.array(data.get('src_bndbox', [])),\n",
    "                'tgt_bbox': np.array(data.get('trg_bndbox', [])),\n",
    "                'category': cat\n",
    "            }\n",
    "            self.pairs.append(pair)\n",
    "        \n",
    "        print(f\"✓ Loaded {len(self.pairs)} pairs from SPair-71k {self.split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        src_img = Image.open(pair['src_img']).convert('RGB')\n",
    "        tgt_img = Image.open(pair['tgt_img']).convert('RGB')\n",
    "        \n",
    "        return {\n",
    "            'src_image': src_img,\n",
    "            'tgt_image': tgt_img,\n",
    "            'src_keypoints': pair['src_kps'],\n",
    "            'tgt_keypoints': pair['tgt_kps'],\n",
    "            'src_bbox': pair['src_bbox'],\n",
    "            'tgt_bbox': pair['tgt_bbox'],\n",
    "            'category': pair['category']\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset loaders defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2623fbd",
   "metadata": {},
   "source": [
    "## Visualization and Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_correspondences(src_img, tgt_img, src_kps, pred_kps, gt_kps=None, \n",
    "                              max_points=15, save_path=None):\n",
    "    \"\"\"Visualize correspondence matches.\"\"\"\n",
    "    if isinstance(src_img, Image.Image):\n",
    "        src_img = np.array(src_img)\n",
    "    if isinstance(tgt_img, Image.Image):\n",
    "        tgt_img = np.array(tgt_img)\n",
    "    \n",
    "    # Ensure all arrays are properly shaped 2D arrays (N, 2)\n",
    "    src_kps = np.atleast_2d(src_kps)\n",
    "    if src_kps.shape[0] == 2 and src_kps.shape[1] != 2:  # If shape is (2, M) where M > 2\n",
    "        src_kps = src_kps.T\n",
    "    \n",
    "    pred_kps = np.atleast_2d(pred_kps)\n",
    "    if pred_kps.shape[0] == 2 and pred_kps.shape[1] != 2:\n",
    "        pred_kps = pred_kps.T\n",
    "    \n",
    "    if gt_kps is not None:\n",
    "        gt_kps = np.atleast_2d(gt_kps)\n",
    "        if gt_kps.shape[0] == 2 and gt_kps.shape[1] != 2:\n",
    "            gt_kps = gt_kps.T\n",
    "    \n",
    "    # Ensure all have same number of points by truncating to minimum\n",
    "    min_points = min(len(src_kps), len(pred_kps))\n",
    "    if gt_kps is not None:\n",
    "        min_points = min(min_points, len(gt_kps))\n",
    "    \n",
    "    src_kps = src_kps[:min_points]\n",
    "    pred_kps = pred_kps[:min_points]\n",
    "    if gt_kps is not None:\n",
    "        gt_kps = gt_kps[:min_points]\n",
    "    \n",
    "    # Subsample if needed\n",
    "    if len(src_kps) > max_points:\n",
    "        indices = np.random.choice(len(src_kps), max_points, replace=False)\n",
    "        src_kps = src_kps[indices]\n",
    "        pred_kps = pred_kps[indices]\n",
    "        if gt_kps is not None:\n",
    "            gt_kps = gt_kps[indices]\n",
    "    \n",
    "    ncols = 3 if gt_kps is not None else 2\n",
    "    fig, axes = plt.subplots(1, ncols, figsize=(6*ncols, 6))\n",
    "    if ncols == 2:\n",
    "        axes = [axes[0], axes[1]]\n",
    "    \n",
    "    # Left: source image with source keypoints\n",
    "    axes[0].imshow(src_img)\n",
    "    if len(src_kps) > 0:\n",
    "        axes[0].scatter(src_kps[:, 0], src_kps[:, 1], c='red', s=100, \n",
    "                        edgecolors='white', linewidths=2, marker='o')\n",
    "    axes[0].set_title('Source Image', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Middle: target image with predicted keypoints\n",
    "    axes[1].imshow(tgt_img)\n",
    "    valid_pred = ~np.isnan(pred_kps).any(axis=1)\n",
    "    if valid_pred.sum() > 0:\n",
    "        axes[1].scatter(pred_kps[valid_pred, 0], pred_kps[valid_pred, 1], c='blue', s=100, \n",
    "                        marker='x', linewidths=3)\n",
    "    axes[1].set_title('Target (Predictions)', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Right: target image with GT vs Pred comparison\n",
    "    if gt_kps is not None and ncols == 3:\n",
    "        axes[2].imshow(tgt_img)\n",
    "        valid_gt = ~np.isnan(gt_kps).any(axis=1)\n",
    "        \n",
    "        # Plot GT keypoints\n",
    "        if valid_gt.sum() > 0:\n",
    "            axes[2].scatter(gt_kps[valid_gt, 0], gt_kps[valid_gt, 1], c='green', s=100, \n",
    "                           edgecolors='white', linewidths=2, marker='o', label='GT')\n",
    "        \n",
    "        # Plot predicted keypoints\n",
    "        if valid_pred.sum() > 0:\n",
    "            axes[2].scatter(pred_kps[valid_pred, 0], pred_kps[valid_pred, 1], c='blue', s=50, \n",
    "                           marker='x', linewidths=2, alpha=0.7, label='Pred')\n",
    "        \n",
    "        # Draw lines and compute error only for indices valid in BOTH\n",
    "        valid_both = valid_pred & valid_gt\n",
    "        if valid_both.sum() > 0:\n",
    "            for i in np.where(valid_both)[0]:\n",
    "                axes[2].plot([gt_kps[i, 0], pred_kps[i, 0]], \n",
    "                           [gt_kps[i, 1], pred_kps[i, 1]], \n",
    "                           'r--', alpha=0.3, linewidth=1)\n",
    "            \n",
    "            try:\n",
    "                errors = np.linalg.norm(pred_kps[valid_both] - gt_kps[valid_both], axis=1)\n",
    "                mean_error = errors.mean()\n",
    "            except ValueError:\n",
    "                mean_error = 0\n",
    "        else:\n",
    "            mean_error = 0\n",
    "        \n",
    "        axes[2].set_title(f'GT vs Pred (Mean Error: {mean_error:.1f}px)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[2].legend(loc='upper right')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"✓ Visualization utilities ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46883774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_dataset(dataset, feature_extractor, matcher, evaluator, \n",
    "                       max_samples=None, save_visualizations=True):\n",
    "    \"\"\"Complete evaluation pipeline.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"EVALUATING SAM ON {dataset.__class__.__name__}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    num_samples = min(max_samples, len(dataset)) if max_samples else len(dataset)\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    print(f\"Evaluating: {num_samples} samples\\n\")\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    image_sizes = []\n",
    "    bboxes = []\n",
    "    confidences = []\n",
    "    \n",
    "    for i in tqdm(range(num_samples), desc=\"Processing\"):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        src_img = sample['src_image']\n",
    "        tgt_img = sample['tgt_image']\n",
    "        src_kps = sample['src_keypoints']\n",
    "        tgt_kps = sample['tgt_keypoints']\n",
    "        \n",
    "        if len(src_kps) == 0 or len(tgt_kps) == 0:\n",
    "            continue\n",
    "        \n",
    "        pred_kps, conf = matcher.match_keypoints(\n",
    "            src_img, tgt_img, src_kps, feature_extractor\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred_kps)\n",
    "        ground_truths.append(tgt_kps)\n",
    "        confidences.append(conf)\n",
    "        image_sizes.append(tgt_img.size)\n",
    "        \n",
    "        if 'tgt_bbox' in sample and len(sample['tgt_bbox']) > 0:\n",
    "            bboxes.append(sample['tgt_bbox'])\n",
    "        else:\n",
    "            bboxes.append(None)\n",
    "        \n",
    "        if save_visualizations and i < 5:\n",
    "            vis_path = os.path.join(OUTPUT_DIR, f'sample_{i}.png')\n",
    "            visualize_correspondences(src_img, tgt_img, src_kps, pred_kps, \n",
    "                                    tgt_kps, save_path=vis_path)\n",
    "            plt.close()\n",
    "    \n",
    "    results = evaluator.evaluate_batch(predictions, ground_truths, image_sizes, bboxes)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Samples evaluated: {results['num_samples']}\")\n",
    "    print(\"\\nPCK Scores:\")\n",
    "    for metric, value in sorted(results['mean'].items()):\n",
    "        print(f\"  {metric}: {value*100:.2f}%\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results_file = os.path.join(OUTPUT_DIR, 'evaluation_results.json')\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump({\n",
    "            'backbone': 'SAM ViT-B',\n",
    "            'dataset': dataset.__class__.__name__,\n",
    "            'num_samples': results['num_samples'],\n",
    "            'mean_pck': results['mean'],\n",
    "            'per_sample_pck': results['per_sample']\n",
    "        }, f, indent=2)\n",
    "    print(f\"\\n✓ Results saved to {results_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Evaluation pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d036c",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Uncomment to run evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate\n",
    "spair_test = SPairDataset(\n",
    "    root_dir=os.path.join(DATA_ROOT, 'SPair-71k'),  \n",
    "    split='test'\n",
    ")\n",
    "\n",
    "results = evaluate_on_dataset(\n",
    "    dataset=spair_test,\n",
    "    feature_extractor=feature_extractor,\n",
    "    matcher=matcher,\n",
    "    evaluator=evaluator,\n",
    "    max_samples=4,\n",
    "    save_visualizations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208262f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### SAM Implementation Complete ✓\n",
    "\n",
    "**Implementation:**\n",
    "1. ✓ Cross-platform environment setup\n",
    "2. ✓ SAM ViT-B model and checkpoint download\n",
    "3. ✓ Dense feature extraction (64×64×256)\n",
    "4. ✓ Correspondence matching\n",
    "5. ✓ PCK evaluation\n",
    "6. ✓ Dataset loaders\n",
    "7. ✓ Visualization tools\n",
    "8. ✓ Complete pipeline\n",
    "\n",
    "**SAM Advantages:**\n",
    "- **Higher spatial resolution**: 64×64 features vs 16×16 (DINO)\n",
    "- **Task-specific training**: Trained for dense prediction tasks\n",
    "- **Strong boundaries**: Excellent at detecting object boundaries\n",
    "- **Large-scale data**: 11M images, 1.1B masks\n",
    "\n",
    "**Trade-offs:**\n",
    "- Lower feature dimension (256 vs 768)\n",
    "- Larger input size (1024 vs 224) → slower\n",
    "- More memory intensive\n",
    "\n",
    "**Expected Performance:**\n",
    "- Potentially better localization accuracy (higher resolution)\n",
    "- Strong for objects with clear boundaries\n",
    "- May excel on geometric transformations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
