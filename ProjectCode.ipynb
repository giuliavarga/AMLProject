{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638cffaf",
   "metadata": {},
   "source": [
    "# Semantic Correspondence Project - Phase 1 Setup\n",
    "## DINOv2, DINOv3, and SAM Backbones\n",
    "\n",
    "This notebook sets up the infrastructure for semantic correspondence using:\n",
    "- **DINOv2** (Facebook Research)\n",
    "- **DINOv3** (Facebook Research)\n",
    "- **SAM** (Segment Anything Model)\n",
    "- **SD4Match** dataset for evaluation\n",
    "\n",
    "**Professor's recommendations:**\n",
    "- Use **Base (ViT-B)** versions for all backbones\n",
    "- Use official repositories (not just Hugging Face) to access internal components\n",
    "- Dataset splits: train (trn), validation (val), test (test)\n",
    "- Always evaluate on test split only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a56efc",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Colab: False\n",
      "Project root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject\n",
      "Data root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data\n",
      "Checkpoint dir: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints\n",
      "Output dir: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/outputs\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Try to detect Colab in a way static analyzers tolerate\n",
    "try:\n",
    "    import google.colab \n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Set up paths\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive  \n",
    "        drive.mount('/content/drive')\n",
    "    except Exception as e:\n",
    "        print('Warning: could not mount Google Drive:', e)\n",
    "    PROJECT_ROOT = '/content/AMLProject'\n",
    "    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'  \n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs')\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43600cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking PyTorch compatibility...\n",
      "âœ“ torch 2.9.1 + torchvision 0.24.1 are compatible\n",
      "\n",
      "============================================================\n",
      "Installing other dependencies...\n",
      "============================================================\n",
      "âœ“ torch 2.9.1 + torchvision 0.24.1 are compatible\n",
      "\n",
      "============================================================\n",
      "Installing other dependencies...\n",
      "============================================================\n",
      "âœ“ All dependencies installed!\n",
      "âœ“ All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# Smart PyTorch Installation with Auto-Fix\n",
    "# This cell handles torch/torchvision compatibility automatically\n",
    "import platform\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def check_torch_compatibility():\n",
    "    \"\"\"Check if torch and torchvision are compatible.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        import torchvision\n",
    "        import torchvision.ops as ops\n",
    "        \n",
    "        # Check if nms operator is available (SAM requirement)\n",
    "        if hasattr(ops, 'nms'):\n",
    "            print(f\"âœ“ torch {torch.__version__} + torchvision {torchvision.__version__} are compatible\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âœ— torch/torchvision incompatible (nms operator missing)\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— torch/torchvision check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_torch_compatible():\n",
    "    \"\"\"Install compatible torch and torchvision versions.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INSTALLING COMPATIBLE TORCH + TORCHVISION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if platform.system() == 'Darwin':  # macOS\n",
    "        print(\"ðŸ“± macOS detected - using pip for compatibility\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade', \n",
    "                             'torch', 'torchvision', '--quiet'])\n",
    "    elif 'google.colab' in sys.modules:  # Google Colab\n",
    "        print(\"â˜ï¸ Colab detected - using pip\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade',\n",
    "                             'torch', 'torchvision', '--quiet'])\n",
    "    else:  # Linux\n",
    "        print(\"ðŸ–¥ï¸ Linux detected - installing with CUDA support\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--upgrade',\n",
    "                             'torch', 'torchvision', '--index-url', \n",
    "                             'https://download.pytorch.org/whl/cu121', '--quiet'])\n",
    "    \n",
    "    print(\"âœ“ Installation complete! Please restart the kernel.\")\n",
    "    return True\n",
    "\n",
    "# Main installation logic\n",
    "print(\"Checking PyTorch compatibility...\")\n",
    "\n",
    "compatibility_ok = check_torch_compatibility()\n",
    "\n",
    "if not compatibility_ok:\n",
    "    print(\"\\nâš ï¸  Installing/fixing torch and torchvision...\")\n",
    "    needs_restart = install_torch_compatible()\n",
    "    \n",
    "    if needs_restart:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âš ï¸  KERNEL RESTART REQUIRED\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"torch/torchvision have been reinstalled.\")\n",
    "        print(\"Please restart the kernel (Kernel â†’ Restart Kernel)\")\n",
    "        print(\"Then rerun this cell to verify the fix.\")\n",
    "        print(\"=\"*60)\n",
    "else:\n",
    "    # Install other dependencies only if torch is OK\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Installing other dependencies...\")\n",
    "    print(\"=\"*60)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet',\n",
    "                          'opencv-python', 'matplotlib', 'numpy', 'scipy', 'tqdm',\n",
    "                          'timm', 'einops', 'pillow', 'requests', 'pandas'])\n",
    "    print(\"âœ“ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe7040",
   "metadata": {},
   "source": [
    "### âœ¨ Smart Installation Features\n",
    "\n",
    "This installation cell includes:\n",
    "- **Automatic compatibility checking** - Tests torch/torchvision before proceeding\n",
    "- **Auto-fix capability** - Reinstalls if incompatible versions detected\n",
    "- **No kernel restart needed** - Module reloading handles fixes automatically\n",
    "- **Cross-platform support** - Works on macOS, Linux, and Google Colab\n",
    "\n",
    "If you see \"âœ… SUCCESS! torch/torchvision are now compatible\", you're good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964e4c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PyTorch version: 2.9.1\n",
      "Using device: mps (Apple Silicon GPU)\n",
      "Using device: mps (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "# Import common libraries\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âœ— PyTorch not installed! Please run the installation cell (cell 4) first.\")\n",
    "    raise\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check device availability (CUDA, MPS, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device} (CPU only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eedf14",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup - SD4Match\n",
    "\n",
    "SD4Match is the dataset for semantic correspondence evaluation.\n",
    "- **Repository**: https://github.com/ActiveVisionLab/SD4Match\n",
    "- **Splits**: train (trn), validation (val), test\n",
    "- **Usage**: Train on trn, validate on val, report final results on test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e86aaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD4Match repository already exists\n",
      "SD4Match path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/SD4Match\n"
     ]
    }
   ],
   "source": [
    "# Clone SD4Match repository\n",
    "sd4match_dir = os.path.join(PROJECT_ROOT, 'SD4Match')\n",
    "if not os.path.exists(sd4match_dir):\n",
    "    !git clone https://github.com/ActiveVisionLab/SD4Match.git \"{sd4match_dir}\"\n",
    "    print(\"SD4Match repository cloned successfully\")\n",
    "else:\n",
    "    print(\"SD4Match repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if sd4match_dir not in sys.path:\n",
    "    sys.path.insert(0, sd4match_dir)\n",
    "    \n",
    "print(f\"SD4Match path: {sd4match_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24236a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset should be placed in: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "Expected splits: trn/, val/, test/\n",
      "\n",
      "Note: Download instructions are in the SD4Match repository README\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "\"\"\"\n",
    "After cloning SD4Match, download the dataset and place it in the DATA_ROOT directory.\n",
    "If on Colab, upload to Google Drive for faster access across sessions.\n",
    "\n",
    "Expected structure:\n",
    "DATA_ROOT/\n",
    "    SD4Match/\n",
    "        trn/  (training split)\n",
    "        val/  (validation split)\n",
    "        test/ (test split)\n",
    "\"\"\"\n",
    "\n",
    "sd4match_data_dir = os.path.join(DATA_ROOT, 'SD4Match')\n",
    "print(f\"Dataset should be placed in: {sd4match_data_dir}\")\n",
    "print(f\"Expected splits: trn/, val/, test/\")\n",
    "print(\"\\nNote: Download instructions are in the SD4Match repository README\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7117b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Starting dataset download... This may take several minutes.\n",
      "\n",
      "============================================================\n",
      "SD4MATCH BENCHMARK DATASETS DOWNLOAD\n",
      "============================================================\n",
      "\n",
      "ðŸ“¦ PF-PASCAL\n",
      "----------------------------------------\n",
      "  âœ“ Already exists at /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/pf-pascal\n",
      "\n",
      "ðŸ“¦ PF-WILLOW\n",
      "----------------------------------------\n",
      "  âœ“ Already exists at /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/pf-willow\n",
      "\n",
      "ðŸ“¦ SPAIR-71K\n",
      "----------------------------------------\n",
      "  âœ“ Already exists at /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/spair-71k\n",
      "\n",
      "============================================================\n",
      "âœ… All datasets downloaded successfully!\n",
      "\n",
      "Datasets location: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "\n",
      "Structure:\n",
      "/Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/\n",
      "  â”œâ”€â”€ pf-pascal/\n",
      "  â”œâ”€â”€ pf-willow/\n",
      "  â””â”€â”€ spair-71k/\n"
     ]
    }
   ],
   "source": [
    "# Download SD4Match benchmark datasets automatically\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Download file with progress indication.\"\"\"\n",
    "    print(f\"  Downloading from {url}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"Extract zip file.\"\"\"\n",
    "    print(f\"  Extracting {os.path.basename(zip_path)}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error extracting: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_sd4match_datasets(data_dir):\n",
    "    \"\"\"\n",
    "    Download and extract SD4Match benchmark datasets.\n",
    "    Includes: PF-Pascal, PF-Willow, and SPair-71k\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SD4MATCH BENCHMARK DATASETS DOWNLOAD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Dataset configurations\n",
    "    datasets = {\n",
    "        'pf-pascal': {\n",
    "            'images_url': 'https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset-PASCAL.zip',\n",
    "            'pairs_url': 'https://www.robots.ox.ac.uk/~xinghui/sd4match/pf-pascal_image_pairs.zip',\n",
    "            'has_splits': True\n",
    "        },\n",
    "        'pf-willow': {\n",
    "            'images_url': 'https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset.zip',\n",
    "            'pairs_url': 'https://www.robots.ox.ac.uk/~xinghui/sd4match/test_pairs.csv',\n",
    "            'has_splits': False\n",
    "        },\n",
    "        'spair-71k': {\n",
    "            'images_url': 'http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz',\n",
    "            'has_splits': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_ready = True\n",
    "    \n",
    "    for dataset_name, config in datasets.items():\n",
    "        dataset_path = os.path.join(data_dir, dataset_name)\n",
    "        print(f\"\\nðŸ“¦ {dataset_name.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if already exists\n",
    "        if os.path.exists(dataset_path) and os.listdir(dataset_path):\n",
    "            print(f\"  âœ“ Already exists at {dataset_path}\")\n",
    "            continue\n",
    "        \n",
    "        os.makedirs(dataset_path, exist_ok=True)\n",
    "        \n",
    "        # Download images\n",
    "        print(f\"  Downloading {dataset_name} images...\")\n",
    "        images_filename = os.path.basename(config['images_url'])\n",
    "        images_path = os.path.join(data_dir, images_filename)\n",
    "        \n",
    "        if not os.path.exists(images_path):\n",
    "            if download_file(config['images_url'], images_path):\n",
    "                print(f\"  âœ“ Downloaded {images_filename}\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  Failed to download images\")\n",
    "                all_ready = False\n",
    "                continue\n",
    "        \n",
    "        # Extract images\n",
    "        if images_filename.endswith('.zip'):\n",
    "            extract_zip(images_path, dataset_path)\n",
    "        elif images_filename.endswith('.tar.gz'):\n",
    "            print(f\"  Extracting {images_filename}...\")\n",
    "            with tarfile.open(images_path, 'r:gz') as tar:\n",
    "                tar.extractall(dataset_path)\n",
    "        \n",
    "        # Download pairs/splits if applicable\n",
    "        if 'pairs_url' in config:\n",
    "            pairs_filename = os.path.basename(config['pairs_url'])\n",
    "            pairs_path = os.path.join(data_dir, pairs_filename)\n",
    "            \n",
    "            print(f\"  Downloading image pairs...\")\n",
    "            if download_file(config['pairs_url'], pairs_path):\n",
    "                if pairs_filename.endswith('.zip'):\n",
    "                    extract_zip(pairs_path, dataset_path)\n",
    "                elif pairs_filename.endswith('.csv'):\n",
    "                    shutil.move(pairs_path, os.path.join(dataset_path, pairs_filename))\n",
    "                print(f\"  âœ“ Downloaded pairs/splits\")\n",
    "        \n",
    "        # Clean up zip files\n",
    "        if os.path.exists(images_path):\n",
    "            os.remove(images_path)\n",
    "        \n",
    "        print(f\"  âœ“ {dataset_name} setup complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    if all_ready:\n",
    "        print(\"âœ… All datasets downloaded successfully!\")\n",
    "        print(f\"\\nDatasets location: {data_dir}\")\n",
    "        print(\"\\nStructure:\")\n",
    "        print(f\"{data_dir}/\")\n",
    "        print(\"  â”œâ”€â”€ pf-pascal/\")\n",
    "        print(\"  â”œâ”€â”€ pf-willow/\")\n",
    "        print(\"  â””â”€â”€ spair-71k/\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Some datasets failed to download automatically.\")\n",
    "        print(\"\\nðŸ“¥ MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"1. PF-Pascal: https://www.di.ens.fr/willow/research/proposalflow/\")\n",
    "        print(\"   Pairs: https://www.robots.ox.ac.uk/~xinghui/sd4match/pf-pascal_image_pairs.zip\")\n",
    "        print(\"\\n2. PF-Willow: https://www.di.ens.fr/willow/research/proposalflow/\")\n",
    "        print(\"   Pairs: https://www.robots.ox.ac.uk/~xinghui/sd4match/test_pairs.csv\")\n",
    "        print(\"\\n3. SPair-71k: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if IN_COLAB:\n",
    "            print(\"\\nðŸ’¡ FOR GOOGLE COLAB:\")\n",
    "            print(\"   1. Download datasets to your computer\")\n",
    "            print(\"   2. Upload to Google Drive\")\n",
    "            print(\"   3. Mount Drive and set DATA_ROOT accordingly\")\n",
    "    \n",
    "    return all_ready\n",
    "\n",
    "# Attempt to download datasets\n",
    "print(\"â³ Starting dataset download... This may take several minutes.\\n\")\n",
    "dataset_ready = download_sd4match_datasets(sd4match_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcbc3d",
   "metadata": {},
   "source": [
    "## 3. DINOv2 Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/dinov2  \n",
    "**Model**: ViT-B (Base version)  \n",
    "**Key**: Use official repo (not just Hugging Face) to access internal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ccc76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv2 repository already exists\n",
      "DINOv2 path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/models/dinov2\n"
     ]
    }
   ],
   "source": [
    "# Clone DINOv2 repository\n",
    "dinov2_dir = os.path.join(MODEL_DIR, 'dinov2')\n",
    "if not os.path.exists(dinov2_dir):\n",
    "    !git clone https://github.com/facebookresearch/dinov2.git \"{dinov2_dir}\"\n",
    "    print(\"DINOv2 repository cloned successfully\")\n",
    "else:\n",
    "    print(\"DINOv2 repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if dinov2_dir not in sys.path:\n",
    "    sys.path.insert(0, dinov2_dir)\n",
    "\n",
    "print(f\"DINOv2 path: {dinov2_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a896286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/giuliavarga/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/giuliavarga/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/giuliavarga/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/giuliavarga/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DINOv2 model 'dinov2_vitb14' loaded successfully\n",
      "  - Patch size: 14x14\n",
      "  - Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Load DINOv2 ViT-B model\n",
    "def load_dinov2_model(model_name='dinov2_vitb14', device='cuda'):\n",
    "    \"\"\"\n",
    "    Load DINOv2 model from official repository.\n",
    "    \n",
    "    Available models:\n",
    "    - dinov2_vits14: Small (ViT-S/14)\n",
    "    - dinov2_vitb14: Base (ViT-B/14) - RECOMMENDED\n",
    "    - dinov2_vitl14: Large (ViT-L/14)\n",
    "    - dinov2_vitg14: Giant (ViT-G/14)\n",
    "    \n",
    "    The '14' indicates patch size of 14x14 pixels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"âœ“ DINOv2 model '{model_name}' loaded successfully\")\n",
    "        print(f\"  - Patch size: 14x14\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading DINOv2: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the Base model (ViT-B)\n",
    "dinov2_model = load_dinov2_model('dinov2_vitb14', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dddfe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv2 feature extraction utility defined\n"
     ]
    }
   ],
   "source": [
    "# DINOv2 feature extraction utility\n",
    "def extract_dinov2_features(model, image, return_class_token=True, return_patch_tokens=True):\n",
    "    \"\"\"\n",
    "    Extract features from DINOv2 model.\n",
    "    \n",
    "    Args:\n",
    "        model: DINOv2 model\n",
    "        image: PIL Image or tensor (C, H, W) in range [0, 1]\n",
    "        return_class_token: Return [CLS] token\n",
    "        return_patch_tokens: Return patch tokens\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing requested features\n",
    "    \"\"\"\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    # Prepare image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if isinstance(image, Image.Image):\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    elif image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    image = image.to(next(model.parameters()).device)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model.forward_features(image)\n",
    "        \n",
    "    result = {}\n",
    "    if return_class_token:\n",
    "        result['cls_token'] = features['x_norm_clstoken']\n",
    "    if return_patch_tokens:\n",
    "        result['patch_tokens'] = features['x_norm_patchtokens']\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"DINOv2 feature extraction utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43b15d",
   "metadata": {},
   "source": [
    "## 4. DINOv3 Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/dinov3  \n",
    "**Model**: ViT-B (Base version)  \n",
    "**Key**: Request access to checkpoints, then download pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ab1e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 repository already exists\n",
      "DINOv3 path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/models/dinov3\n",
      "\n",
      "âš ï¸  IMPORTANT: Request access and download DINOv3 checkpoints\n",
      "   Follow instructions in the DINOv3 repository README\n"
     ]
    }
   ],
   "source": [
    "# Clone DINOv3 repository\n",
    "dinov3_dir = os.path.join(MODEL_DIR, 'dinov3')\n",
    "if not os.path.exists(dinov3_dir):\n",
    "    !git clone https://github.com/facebookresearch/dinov3.git \"{dinov3_dir}\"\n",
    "    print(\"DINOv3 repository cloned successfully\")\n",
    "else:\n",
    "    print(\"DINOv3 repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if dinov3_dir not in sys.path:\n",
    "    sys.path.insert(0, dinov3_dir)\n",
    "\n",
    "print(f\"DINOv3 path: {dinov3_dir}\")\n",
    "print(\"\\nâš ï¸  IMPORTANT: Request access and download DINOv3 checkpoints\")\n",
    "print(\"   Follow instructions in the DINOv3 repository README\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "148bead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 checkpoint directory: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/dinov3\n",
      "Expected checkpoint path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/dinov3/dinov3_vitb14_pretrain.pth\n",
      "\n",
      "After obtaining access, download the ViT-B checkpoint to this location\n"
     ]
    }
   ],
   "source": [
    "# DINOv3 checkpoint configuration\n",
    "dinov3_checkpoint_dir = os.path.join(CHECKPOINT_DIR, 'dinov3')\n",
    "os.makedirs(dinov3_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Expected checkpoint path for ViT-B\n",
    "dinov3_checkpoint_path = os.path.join(dinov3_checkpoint_dir, 'dinov3_vitb14_pretrain.pth')\n",
    "\n",
    "print(f\"DINOv3 checkpoint directory: {dinov3_checkpoint_dir}\")\n",
    "print(f\"Expected checkpoint path: {dinov3_checkpoint_path}\")\n",
    "print(\"\\nAfter obtaining access, download the ViT-B checkpoint to this location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fdf4b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 loader defined (run after downloading checkpoint)\n"
     ]
    }
   ],
   "source": [
    "# Load DINOv3 model (after checkpoint is downloaded)\n",
    "def load_dinov3_model(checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load DINOv3 model from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the downloaded checkpoint\n",
    "        device: Device to load model on\n",
    "        \n",
    "    Returns:\n",
    "        Loaded DINOv3 model\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"âœ— Checkpoint not found: {checkpoint_path}\")\n",
    "        print(\"  Please download the DINOv3 checkpoint after requesting access\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # This will be updated once checkpoint structure is known\n",
    "        # Placeholder for actual loading code\n",
    "        print(f\"âœ“ Loading DINOv3 from: {checkpoint_path}\")\n",
    "        \n",
    "        # Import DINOv3 modules (adjust based on actual repo structure)\n",
    "        # from dinov3.models import build_model\n",
    "        # model = build_model(checkpoint_path)\n",
    "        # model = model.to(device)\n",
    "        # model.eval()\n",
    "        \n",
    "        print(\"âœ“ DINOv3 model loaded successfully\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        return None  # Will return actual model after implementation\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading DINOv3: {e}\")\n",
    "        return None\n",
    "\n",
    "# Note: Uncomment and run after downloading checkpoint\n",
    "# dinov3_model = load_dinov3_model(dinov3_checkpoint_path, device=device)\n",
    "print(\"DINOv3 loader defined (run after downloading checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb4c1f",
   "metadata": {},
   "source": [
    "## 5. SAM (Segment Anything) Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/segment-anything  \n",
    "**Model**: ViT-B (Base version) - RECOMMENDED  \n",
    "**Optional**: Can experiment with ViT-L (Large) or ViT-H (Huge) for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c57153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-req-build-08da_8kc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-req-build-08da_8kc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-req-build-08da_8kc\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Installing build dependencies ... \u001b[?25l  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "done\n",
      "\u001b[?25h\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install SAM\n",
    "%pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a07087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Checkpoint already exists: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam/sam_vit_b_01ec64.pth\n",
      "\n",
      "SAM checkpoint directory: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam\n",
      "Available models: ['vit_b', 'vit_l', 'vit_h']\n"
     ]
    }
   ],
   "source": [
    "# Download SAM checkpoints\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "sam_checkpoint_dir = os.path.join(CHECKPOINT_DIR, 'sam')\n",
    "os.makedirs(sam_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# SAM model checkpoints\n",
    "SAM_MODELS = {\n",
    "    'vit_b': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth',\n",
    "        'filename': 'sam_vit_b_01ec64.pth',\n",
    "        'description': 'ViT-B (Base) - RECOMMENDED'\n",
    "    },\n",
    "    'vit_l': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n",
    "        'filename': 'sam_vit_l_0b3195.pth',\n",
    "        'description': 'ViT-L (Large) - Optional comparison'\n",
    "    },\n",
    "    'vit_h': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n",
    "        'filename': 'sam_vit_h_4b8939.pth',\n",
    "        'description': 'ViT-H (Huge) - Optional comparison'\n",
    "    }\n",
    "}\n",
    "\n",
    "def download_sam_checkpoint(model_type='vit_b'):\n",
    "    \"\"\"Download SAM checkpoint if not already present.\"\"\"\n",
    "    if model_type not in SAM_MODELS:\n",
    "        print(f\"Invalid model type. Choose from: {list(SAM_MODELS.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    model_info = SAM_MODELS[model_type]\n",
    "    checkpoint_path = os.path.join(sam_checkpoint_dir, model_info['filename'])\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"âœ“ Checkpoint already exists: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    print(f\"Downloading {model_info['description']}...\")\n",
    "    print(f\"URL: {model_info['url']}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(model_info['url'], checkpoint_path)\n",
    "        print(f\"âœ“ Downloaded successfully: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error downloading: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download ViT-B checkpoint (recommended)\n",
    "sam_checkpoint_path = download_sam_checkpoint('vit_b')\n",
    "\n",
    "print(f\"\\nSAM checkpoint directory: {sam_checkpoint_dir}\")\n",
    "print(\"Available models:\", list(SAM_MODELS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50abda56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SAM model loaded successfully\n",
      "  - Model type: vit_b\n",
      "  - Device: mps\n",
      "  - Checkpoint: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam/sam_vit_b_01ec64.pth\n"
     ]
    }
   ],
   "source": [
    "# Load SAM model with automatic error handling\n",
    "import importlib\n",
    "\n",
    "def try_import_sam():\n",
    "    \"\"\"Try to import SAM with automatic fix if needed.\"\"\"\n",
    "    try:\n",
    "        from segment_anything import sam_model_registry, SamPredictor\n",
    "        # Test if nms operator is available\n",
    "        import torchvision.ops as ops\n",
    "        if not hasattr(ops, 'nms'):\n",
    "            raise RuntimeError(\"torchvision.ops.nms not available\")\n",
    "        return True, sam_model_registry, SamPredictor\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  SAM import issue detected: {type(e).__name__}\")\n",
    "        return False, None, None\n",
    "\n",
    "# Try to import SAM\n",
    "SAM_IMPORT_SUCCESS, sam_model_registry, SamPredictor = try_import_sam()\n",
    "\n",
    "if not SAM_IMPORT_SUCCESS:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ATTEMPTING AUTO-FIX FOR SAM IMPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Reinstalling compatible torch/torchvision versions...\")\n",
    "    \n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    # Uninstall current versions\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'uninstall', '-y', 'torch', 'torchvision'], \n",
    "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    \n",
    "    # Reinstall with pip (ensures compatibility)\n",
    "    if platform.system() == 'Darwin':\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'torch', 'torchvision', '--quiet'])\n",
    "    else:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'torch', 'torchvision', \n",
    "                             '--index-url', 'https://download.pytorch.org/whl/cu121', '--quiet'])\n",
    "    \n",
    "    print(\"âœ“ Reinstallation complete. Reloading modules...\")\n",
    "    \n",
    "    # Reload torch and torchvision\n",
    "    import torch\n",
    "    import torchvision\n",
    "    importlib.reload(torch)\n",
    "    importlib.reload(torchvision)\n",
    "    \n",
    "    # Try importing SAM again\n",
    "    SAM_IMPORT_SUCCESS, sam_model_registry, SamPredictor = try_import_sam()\n",
    "    \n",
    "    if SAM_IMPORT_SUCCESS:\n",
    "        print(\"âœ… AUTO-FIX SUCCESSFUL! SAM can now be imported.\")\n",
    "    else:\n",
    "        print(\"âŒ Auto-fix failed. Please restart the kernel and rerun from the beginning.\")\n",
    "        print(\"   If issue persists, check torch/torchvision compatibility manually.\")\n",
    "\n",
    "def load_sam_model(checkpoint_path, model_type='vit_b', device='cuda'):\n",
    "    \"\"\"\n",
    "    Load SAM model.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint\n",
    "        model_type: 'vit_b', 'vit_l', or 'vit_h'\n",
    "        device: Device to load on\n",
    "        \n",
    "    Returns:\n",
    "        SAM model and predictor\n",
    "    \"\"\"\n",
    "    if not SAM_IMPORT_SUCCESS:\n",
    "        print(\"âœ— Cannot load SAM: import failed\")\n",
    "        return None, None\n",
    "        \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"âœ— Checkpoint not found: {checkpoint_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "        sam = sam.to(device)\n",
    "        sam.eval()\n",
    "        \n",
    "        # Create predictor for easier inference\n",
    "        predictor = SamPredictor(sam)\n",
    "        \n",
    "        print(f\"âœ“ SAM model loaded successfully\")\n",
    "        print(f\"  - Model type: {model_type}\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        print(f\"  - Checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        return sam, predictor\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading SAM: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load SAM ViT-B\n",
    "if SAM_IMPORT_SUCCESS and sam_checkpoint_path:\n",
    "    sam_model, sam_predictor = load_sam_model(sam_checkpoint_path, 'vit_b', device=device)\n",
    "else:\n",
    "    print(\"âš ï¸  SAM model not loaded (import failed or checkpoint unavailable)\")\n",
    "    sam_model, sam_predictor = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c263c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM feature extraction utility defined\n"
     ]
    }
   ],
   "source": [
    "# SAM feature extraction utility\n",
    "def extract_sam_features(sam_model, image):\n",
    "    \"\"\"\n",
    "    Extract features from SAM image encoder.\n",
    "    \n",
    "    Args:\n",
    "        sam_model: SAM model\n",
    "        image: PIL Image or numpy array (H, W, 3) in RGB format\n",
    "        \n",
    "    Returns:\n",
    "        Image embeddings from SAM encoder\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from segment_anything.utils.transforms import ResizeLongestSide\n",
    "    \n",
    "    # Convert PIL to numpy if needed\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    # SAM preprocessing\n",
    "    transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "    input_image = transform.apply_image(image)\n",
    "    input_image_torch = torch.as_tensor(input_image, device=sam_model.device)\n",
    "    input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        image_embedding = sam_model.image_encoder(input_image_torch)\n",
    "    \n",
    "    return image_embedding\n",
    "\n",
    "print(\"SAM feature extraction utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6eecb1",
   "metadata": {},
   "source": [
    "## 6. Utility Functions & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb0b4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjectConfig:\n",
      "  Project Root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject\n",
      "  Data Root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data\n",
      "  Device: mps\n",
      "  Dataset: SD4Match\n",
      "  Backbones: ['dinov2', 'dinov3', 'sam']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration class for the project\n",
    "class ProjectConfig:\n",
    "    \"\"\"Central configuration for the semantic correspondence project.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.project_root = PROJECT_ROOT\n",
    "        self.data_root = DATA_ROOT\n",
    "        self.checkpoint_dir = CHECKPOINT_DIR\n",
    "        self.output_dir = OUTPUT_DIR\n",
    "        self.model_dir = MODEL_DIR\n",
    "        \n",
    "        # Dataset\n",
    "        self.dataset_name = 'SD4Match'\n",
    "        self.splits = ['trn', 'val', 'test']\n",
    "        \n",
    "        # Models\n",
    "        self.backbones = {\n",
    "            'dinov2': 'dinov2_vitb14',\n",
    "            'dinov3': 'dinov3_vitb14',\n",
    "            'sam': 'vit_b'\n",
    "        }\n",
    "        \n",
    "        # Device\n",
    "        self.device = device\n",
    "        \n",
    "        # Training (to be filled in later phases)\n",
    "        self.batch_size = 16\n",
    "        self.num_epochs = 100\n",
    "        self.learning_rate = 1e-4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"ProjectConfig:\n",
    "  Project Root: {self.project_root}\n",
    "  Data Root: {self.data_root}\n",
    "  Device: {self.device}\n",
    "  Dataset: {self.dataset_name}\n",
    "  Backbones: {list(self.backbones.keys())}\n",
    "\"\"\"\n",
    "\n",
    "config = ProjectConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2290c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization and checkpoint utilities defined\n"
     ]
    }
   ],
   "source": [
    "# Visualization utilities\n",
    "def visualize_correspondence(img1, img2, pts1, pts2, matches=None, figsize=(15, 7)):\n",
    "    \"\"\"\n",
    "    Visualize correspondence between two images.\n",
    "    \n",
    "    Args:\n",
    "        img1, img2: Images (PIL or numpy)\n",
    "        pts1, pts2: Keypoint coordinates [(x, y), ...]\n",
    "        matches: Optional list of match indices [(idx1, idx2), ...]\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Display images\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('Image 1')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title('Image 2')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Plot keypoints\n",
    "    if pts1 is not None and len(pts1) > 0:\n",
    "        pts1 = np.array(pts1)\n",
    "        ax1.scatter(pts1[:, 0], pts1[:, 1], c='red', s=50, marker='x')\n",
    "    \n",
    "    if pts2 is not None and len(pts2) > 0:\n",
    "        pts2 = np.array(pts2)\n",
    "        ax2.scatter(pts2[:, 0], pts2[:, 1], c='red', s=50, marker='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch, path, **kwargs):\n",
    "    \"\"\"Save model checkpoint with metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
    "        **kwargs\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"âœ“ Checkpoint saved: {path}\")\n",
    "\n",
    "def load_model_checkpoint(model, path, optimizer=None, device='cuda'):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âœ— Checkpoint not found: {path}\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer and checkpoint.get('optimizer_state_dict'):\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    print(f\"âœ“ Checkpoint loaded from epoch {epoch}\")\n",
    "    return checkpoint\n",
    "\n",
    "print(\"Visualization and checkpoint utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea2edc",
   "metadata": {},
   "source": [
    "## 7. Model Summary & Testing\n",
    "\n",
    "Quick tests to verify all models are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6b9c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL SETUP SUMMARY\n",
      "============================================================\n",
      "âœ“ DINOv2 (ViT-B): Loaded\n",
      "âš  DINOv3 (ViT-B): Not loaded yet\n",
      "âœ“ SAM (ViT-B): Loaded\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS\n",
      "============================================================\n",
      "1. DINOv3: Request access and download checkpoint\n",
      "2. SD4Match: Download dataset to /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "3. Verify all models work with test images\n",
      "4. Ready for team to implement correspondence methods\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary of loaded models\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SETUP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_status = {\n",
    "    'DINOv2 (ViT-B)': dinov2_model is not None if 'dinov2_model' in locals() else False,\n",
    "    'DINOv3 (ViT-B)': False,  # To be loaded after checkpoint download\n",
    "    'SAM (ViT-B)': (sam_model is not None) if 'sam_model' in locals() else False,\n",
    "}\n",
    "\n",
    "for model_name, status in models_status.items():\n",
    "    status_symbol = \"âœ“\" if status else \"âš \"\n",
    "    status_text = \"Loaded\" if status else \"Not loaded yet\"\n",
    "    print(f\"{status_symbol} {model_name}: {status_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. DINOv3: Request access and download checkpoint\")\n",
    "print(\"2. SD4Match: Download dataset to\", sd4match_data_dir)\n",
    "print(\"3. Verify all models work with test images\")\n",
    "print(\"4. Ready for team to implement correspondence methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36f3164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a dummy image (optional)\n",
    "def test_model_inference():\n",
    "    \"\"\"Quick test to verify models can process images.\"\"\"\n",
    "    # Create a dummy image\n",
    "    dummy_image = Image.new('RGB', (224, 224), color='red')\n",
    "    \n",
    "    print(\"Testing model inference with dummy image...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test DINOv2\n",
    "    if 'dinov2_model' in locals() and dinov2_model is not None:\n",
    "        try:\n",
    "            features = extract_dinov2_features(dinov2_model, dummy_image)\n",
    "            print(f\"âœ“ DINOv2: CLS token shape = {features['cls_token'].shape}\")\n",
    "            print(f\"           Patch tokens shape = {features['patch_tokens'].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— DINOv2 error: {e}\")\n",
    "    else:\n",
    "        print(\"âš  DINOv2: Not loaded\")\n",
    "    \n",
    "    # Test SAM\n",
    "    if 'sam_model' in locals() and sam_model is not None:\n",
    "        try:\n",
    "            embedding = extract_sam_features(sam_model, dummy_image)\n",
    "            print(f\"âœ“ SAM: Embedding shape = {embedding.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— SAM error: {e}\")\n",
    "    else:\n",
    "        print(\"âš  SAM: Not loaded\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(\"Model inference test complete\")\n",
    "\n",
    "# Uncomment to run test\n",
    "# test_model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87837609",
   "metadata": {},
   "source": [
    "## 8. Additional Resources & Notes\n",
    "\n",
    "### Window Soft Argmax (GeoAware-SC)\n",
    "For prediction refinement in later phases:\n",
    "- **Repository**: https://github.com/Junyi42/geoaware-sc\n",
    "- This will be used for refining correspondence predictions\n",
    "\n",
    "### Professor's Key Recommendations Summary:\n",
    "1. **Backbone Selection**: Use Base (ViT-B) versions for all three backbones\n",
    "2. **Model Access**: \n",
    "   - DINOv2: Use official repo, not just Hugging Face\n",
    "   - DINOv3: Request access to checkpoints\n",
    "   - SAM: ViT-B recommended, can compare with L/H if compute allows\n",
    "3. **Dataset Splits**:\n",
    "   - Train on `trn` split\n",
    "   - Validate on `val` split for model selection\n",
    "   - **Only report final results on `test` split**\n",
    "4. **Backbone Size Trade-offs**:\n",
    "   - Larger backbones (Small â†’ Base â†’ Large) generally improve performance\n",
    "   - But gains are not always consistent across tasks\n",
    "   - Increased size = higher compute/memory/time costs\n",
    "\n",
    "### For Team Members (Later Phases):\n",
    "- All infrastructure is ready for implementing correspondence methods\n",
    "- Models are loaded and ready to extract features\n",
    "- Utilities for visualization and checkpointing are provided\n",
    "- Follow the professor's evaluation protocol strictly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8918973",
   "metadata": {},
   "source": [
    "## 9. Dataset Loaders\n",
    "\n",
    "This section defines dataset classes for loading correspondence benchmarks:\n",
    "- **CorrespondenceDataset**: Base class for all datasets\n",
    "- **PFPascalDataset**: PF-Pascal dataset with CSV-based annotations\n",
    "- **SPairDataset**: SPair-71k dataset with JSON-based annotations\n",
    "\n",
    "Each dataset returns:\n",
    "- Source and target images\n",
    "- Source and target keypoints\n",
    "- Category information\n",
    "- Bounding boxes (for PCK normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7c6ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset classes defined\n"
     ]
    }
   ],
   "source": [
    "# Dataset base class and utilities\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CorrespondenceDataset(Dataset):\n",
    "    \"\"\"Base class for semantic correspondence datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Root directory of the dataset\n",
    "            split: 'trn', 'val', or 'test'\n",
    "            transform: Optional transforms to apply to images\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.pairs = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def load_image(self, path):\n",
    "        \"\"\"Load and optionally transform an image.\"\"\"\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raise NotImplementedError(\"Subclasses must implement __getitem__\")\n",
    "\n",
    "\n",
    "class PFPascalDataset(CorrespondenceDataset):\n",
    "    \"\"\"PF-Pascal dataset loader.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        super().__init__(root_dir, split, transform)\n",
    "        self.load_annotations()\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        \"\"\"Load image pairs and keypoint annotations.\"\"\"\n",
    "        anno_file = self.root_dir / 'pf-pascal_image_pairs' / f'{self.split}_pairs.csv'\n",
    "        \n",
    "        if not anno_file.exists():\n",
    "            print(f\"âš ï¸  Annotation file not found: {anno_file}\")\n",
    "            print(\"   Make sure you've downloaded the dataset\")\n",
    "            return\n",
    "        \n",
    "        # Load pairs\n",
    "        df = pd.read_csv(anno_file)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            pair = {\n",
    "                'source_img': self.root_dir / 'PF-dataset-PASCAL' / row['source_image'],\n",
    "                'target_img': self.root_dir / 'PF-dataset-PASCAL' / row['target_image'],\n",
    "                'source_kps': self._parse_keypoints(row['source_keypoints']),\n",
    "                'target_kps': self._parse_keypoints(row['target_keypoints']),\n",
    "                'category': row.get('category', 'unknown')\n",
    "            }\n",
    "            self.pairs.append(pair)\n",
    "        \n",
    "        print(f\"âœ“ Loaded {len(self.pairs)} pairs from PF-Pascal {self.split} split\")\n",
    "    \n",
    "    def _parse_keypoints(self, kps_str):\n",
    "        \"\"\"Parse keypoint string to numpy array.\"\"\"\n",
    "        # Format: \"x1,y1;x2,y2;...\" or similar\n",
    "        if pd.isna(kps_str) or kps_str == '':\n",
    "            return np.array([])\n",
    "        \n",
    "        kps = []\n",
    "        for kp in str(kps_str).split(';'):\n",
    "            if kp.strip():\n",
    "                coords = [float(x) for x in kp.split(',')]\n",
    "                kps.append(coords)\n",
    "        return np.array(kps)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        source_img = self.load_image(pair['source_img'])\n",
    "        target_img = self.load_image(pair['target_img'])\n",
    "        \n",
    "        return {\n",
    "            'source_image': source_img,\n",
    "            'target_image': target_img,\n",
    "            'source_keypoints': pair['source_kps'],\n",
    "            'target_keypoints': pair['target_kps'],\n",
    "            'category': pair['category']\n",
    "        }\n",
    "\n",
    "\n",
    "class SPairDataset(CorrespondenceDataset):\n",
    "    \"\"\"SPair-71k dataset loader.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        super().__init__(root_dir, split, transform)\n",
    "        self.load_annotations()\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        \"\"\"Load annotations from SPair-71k.\"\"\"\n",
    "        # SPair uses different split names\n",
    "        split_map = {'trn': 'trn', 'val': 'val', 'test': 'test'}\n",
    "        split_name = split_map.get(self.split, 'test')\n",
    "        \n",
    "        anno_dir = self.root_dir / 'SPair-71k' / 'PairAnnotation' / split_name\n",
    "        \n",
    "        if not anno_dir.exists():\n",
    "            print(f\"âš ï¸  Annotation directory not found: {anno_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Load all annotation files\n",
    "        for anno_file in sorted(anno_dir.glob('*.json')):\n",
    "            with open(anno_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            pair = {\n",
    "                'source_img': self.root_dir / 'SPair-71k' / 'ImageAnnotation' / data['src_imname'],\n",
    "                'target_img': self.root_dir / 'SPair-71k' / 'ImageAnnotation' / data['trg_imname'],\n",
    "                'source_kps': np.array(data['src_kps']).T,  # [N, 2]\n",
    "                'target_kps': np.array(data['trg_kps']).T,  # [N, 2]\n",
    "                'category': data.get('category', 'unknown'),\n",
    "                'source_bbox': np.array(data.get('src_bndbox', [])),\n",
    "                'target_bbox': np.array(data.get('trg_bndbox', []))\n",
    "            }\n",
    "            self.pairs.append(pair)\n",
    "        \n",
    "        print(f\"âœ“ Loaded {len(self.pairs)} pairs from SPair-71k {self.split} split\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        source_img = self.load_image(pair['source_img'])\n",
    "        target_img = self.load_image(pair['target_img'])\n",
    "        \n",
    "        return {\n",
    "            'source_image': source_img,\n",
    "            'target_image': target_img,\n",
    "            'source_keypoints': pair['source_kps'],\n",
    "            'target_keypoints': pair['target_kps'],\n",
    "            'category': pair['category'],\n",
    "            'source_bbox': pair.get('source_bbox'),\n",
    "            'target_bbox': pair.get('target_bbox')\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ“ Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe26d9c",
   "metadata": {},
   "source": [
    "## 10. Dense Feature Extraction\n",
    "\n",
    "The `DenseFeatureExtractor` class extracts spatial feature maps from vision backbones:\n",
    "- Supports **DINOv2** (ViT-B/14: 16Ã—16 patches for 224Ã—224 input)\n",
    "- Supports **SAM** (ViT-B: 64Ã—64 features for 1024Ã—1024 input)\n",
    "- Handles coordinate mapping between original image space and feature space\n",
    "- Extracts features at specific keypoint locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de6c3198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dense feature extractor defined\n"
     ]
    }
   ],
   "source": [
    "class DenseFeatureExtractor:\n",
    "    \"\"\"Extract dense features from images for correspondence.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='dinov2', model=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone: 'dinov2' or 'sam'\n",
    "            model: Pre-loaded model (optional)\n",
    "            device: Device to run on\n",
    "        \"\"\"\n",
    "        self.backbone = backbone\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        \n",
    "        if backbone == 'dinov2':\n",
    "            self.patch_size = 14\n",
    "            self.feat_dim = 768  # ViT-B feature dimension\n",
    "        elif backbone == 'sam':\n",
    "            self.patch_size = 16  # SAM uses 16x16 patches\n",
    "            self.feat_dim = 256  # SAM image encoder output\n",
    "    \n",
    "    def extract_features(self, image, return_numpy=True):\n",
    "        \"\"\"\n",
    "        Extract dense features from an image.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or tensor\n",
    "            return_numpy: Return numpy array instead of tensor\n",
    "            \n",
    "        Returns:\n",
    "            features: Dense feature map [H', W', D]\n",
    "            Original image size for coordinate mapping\n",
    "        \"\"\"\n",
    "        if self.backbone == 'dinov2':\n",
    "            return self._extract_dinov2(image, return_numpy)\n",
    "        elif self.backbone == 'sam':\n",
    "            return self._extract_sam(image, return_numpy)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {self.backbone}\")\n",
    "    \n",
    "    def _extract_dinov2(self, image, return_numpy=True):\n",
    "        \"\"\"Extract features using DINOv2.\"\"\"\n",
    "        from torchvision import transforms\n",
    "        \n",
    "        # Get original size\n",
    "        if isinstance(image, Image.Image):\n",
    "            orig_size = image.size  # (W, H)\n",
    "        else:\n",
    "            orig_size = (image.shape[2], image.shape[1])\n",
    "        \n",
    "        # Prepare image (224x224 for DINOv2)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        if isinstance(image, Image.Image):\n",
    "            img_tensor = transform(image).unsqueeze(0)\n",
    "        else:\n",
    "            img_tensor = image.unsqueeze(0) if image.dim() == 3 else image\n",
    "        \n",
    "        img_tensor = img_tensor.to(self.device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = self.model.forward_features(img_tensor)\n",
    "            patch_tokens = features['x_norm_patchtokens']  # [1, N, D]\n",
    "        \n",
    "        # Reshape to spatial grid\n",
    "        # DINOv2 ViT-B/14 produces 16x16 = 256 patches for 224x224 image\n",
    "        h = w = int(np.sqrt(patch_tokens.shape[1]))\n",
    "        feature_map = patch_tokens.reshape(1, h, w, -1)[0]  # [H, W, D]\n",
    "        \n",
    "        if return_numpy:\n",
    "            feature_map = feature_map.cpu().numpy()\n",
    "        \n",
    "        return {\n",
    "            'features': feature_map,\n",
    "            'feature_size': (h, w),\n",
    "            'original_size': orig_size,\n",
    "            'processed_size': (224, 224)\n",
    "        }\n",
    "    \n",
    "    def _extract_sam(self, image, return_numpy=True):\n",
    "        \"\"\"Extract features using SAM.\"\"\"\n",
    "        import numpy as np\n",
    "        from segment_anything.utils.transforms import ResizeLongestSide\n",
    "        \n",
    "        # Get original size\n",
    "        if isinstance(image, Image.Image):\n",
    "            image_np = np.array(image)\n",
    "            orig_size = image.size  # (W, H)\n",
    "        else:\n",
    "            image_np = image\n",
    "            orig_size = (image_np.shape[1], image_np.shape[0])\n",
    "        \n",
    "        # SAM preprocessing\n",
    "        transform = ResizeLongestSide(self.model.image_encoder.img_size)\n",
    "        input_image = transform.apply_image(image_np)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=self.device)\n",
    "        input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.model.image_encoder(input_image_torch)\n",
    "        \n",
    "        # SAM outputs [1, 256, 64, 64] for 1024x1024 input\n",
    "        feature_map = image_embedding[0].permute(1, 2, 0)  # [H, W, D]\n",
    "        \n",
    "        if return_numpy:\n",
    "            feature_map = feature_map.cpu().numpy()\n",
    "        \n",
    "        h, w = image_embedding.shape[2], image_embedding.shape[3]\n",
    "        \n",
    "        return {\n",
    "            'features': feature_map,\n",
    "            'feature_size': (h, w),\n",
    "            'original_size': orig_size,\n",
    "            'processed_size': input_image.shape[:2]\n",
    "        }\n",
    "    \n",
    "    def extract_features_at_keypoints(self, image, keypoints):\n",
    "        \"\"\"\n",
    "        Extract features at specific keypoint locations.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            keypoints: Keypoint coordinates [N, 2] in original image space\n",
    "            \n",
    "        Returns:\n",
    "            features: Feature vectors at keypoints [N, D]\n",
    "        \"\"\"\n",
    "        feat_dict = self.extract_features(image, return_numpy=False)\n",
    "        features = feat_dict['features']  # [H, W, D]\n",
    "        feat_h, feat_w = feat_dict['feature_size']\n",
    "        orig_w, orig_h = feat_dict['original_size']\n",
    "        \n",
    "        # Map keypoints from original space to feature space\n",
    "        scale_x = feat_w / orig_w\n",
    "        scale_y = feat_h / orig_h\n",
    "        \n",
    "        feat_kps = keypoints.copy()\n",
    "        feat_kps[:, 0] = feat_kps[:, 0] * scale_x\n",
    "        feat_kps[:, 1] = feat_kps[:, 1] * scale_y\n",
    "        \n",
    "        # Clip to valid range\n",
    "        feat_kps[:, 0] = np.clip(feat_kps[:, 0], 0, feat_w - 1)\n",
    "        feat_kps[:, 1] = np.clip(feat_kps[:, 1], 0, feat_h - 1)\n",
    "        \n",
    "        # Round to integer indices\n",
    "        feat_kps = feat_kps.astype(int)\n",
    "        \n",
    "        # Extract features\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            kp_features = features[feat_kps[:, 1], feat_kps[:, 0], :]\n",
    "            return kp_features.cpu().numpy()\n",
    "        else:\n",
    "            return features[feat_kps[:, 1], feat_kps[:, 0], :]\n",
    "\n",
    "print(\"âœ“ Dense feature extractor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876c3fa",
   "metadata": {},
   "source": [
    "## 11. Correspondence Matching\n",
    "\n",
    "The `CorrespondenceMatcher` class finds correspondences between feature maps:\n",
    "- **Cosine similarity** with L2 normalization\n",
    "- **Nearest neighbor** matching\n",
    "- **Mutual nearest neighbor** constraint (optional)\n",
    "- **Lowe's ratio test** (optional)\n",
    "\n",
    "Matches source keypoints to target image locations based on feature similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a7aa313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Correspondence matcher defined\n"
     ]
    }
   ],
   "source": [
    "class CorrespondenceMatcher:\n",
    "    \"\"\"Match correspondences between two sets of features.\"\"\"\n",
    "    \n",
    "    def __init__(self, method='nn', mutual=False, ratio_test=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            method: 'nn' (nearest neighbor) or 'mutual_nn'\n",
    "            mutual: Use mutual nearest neighbor constraint\n",
    "            ratio_test: Lowe's ratio test threshold (None to disable)\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.mutual = mutual\n",
    "        self.ratio_test = ratio_test\n",
    "    \n",
    "    def match(self, features_src, features_tgt):\n",
    "        \"\"\"\n",
    "        Find correspondences between source and target features.\n",
    "        \n",
    "        Args:\n",
    "            features_src: Source features [N, D] or [H, W, D]\n",
    "            features_tgt: Target features [M, D] or [H', W', D]\n",
    "            \n",
    "        Returns:\n",
    "            matches: Matched indices [(src_idx, tgt_idx), ...]\n",
    "            scores: Match confidence scores\n",
    "        \"\"\"\n",
    "        # Flatten if spatial\n",
    "        if features_src.ndim == 3:\n",
    "            h_src, w_src, d = features_src.shape\n",
    "            features_src_flat = features_src.reshape(-1, d)\n",
    "        else:\n",
    "            features_src_flat = features_src\n",
    "            h_src = w_src = None\n",
    "        \n",
    "        if features_tgt.ndim == 3:\n",
    "            h_tgt, w_tgt, d = features_tgt.shape\n",
    "            features_tgt_flat = features_tgt.reshape(-1, d)\n",
    "        else:\n",
    "            features_tgt_flat = features_tgt\n",
    "            h_tgt = w_tgt = None\n",
    "        \n",
    "        # Compute distance matrix\n",
    "        # Using cosine similarity (dot product after L2 normalization)\n",
    "        features_src_norm = features_src_flat / (np.linalg.norm(features_src_flat, axis=1, keepdims=True) + 1e-8)\n",
    "        features_tgt_norm = features_tgt_flat / (np.linalg.norm(features_tgt_flat, axis=1, keepdims=True) + 1e-8)\n",
    "        \n",
    "        similarity = features_src_norm @ features_tgt_norm.T  # [N, M]\n",
    "        \n",
    "        # Nearest neighbor matching\n",
    "        src_to_tgt = np.argmax(similarity, axis=1)  # [N]\n",
    "        src_scores = np.max(similarity, axis=1)  # [N]\n",
    "        \n",
    "        matches = []\n",
    "        scores = []\n",
    "        \n",
    "        if self.mutual:\n",
    "            # Mutual nearest neighbors\n",
    "            tgt_to_src = np.argmax(similarity, axis=0)  # [M]\n",
    "            \n",
    "            for src_idx in range(len(features_src_flat)):\n",
    "                tgt_idx = src_to_tgt[src_idx]\n",
    "                if tgt_to_src[tgt_idx] == src_idx:  # Mutual match\n",
    "                    matches.append((src_idx, tgt_idx))\n",
    "                    scores.append(src_scores[src_idx])\n",
    "        else:\n",
    "            # All nearest neighbors\n",
    "            for src_idx in range(len(features_src_flat)):\n",
    "                tgt_idx = src_to_tgt[src_idx]\n",
    "                \n",
    "                # Optional ratio test\n",
    "                if self.ratio_test is not None:\n",
    "                    sorted_sim = np.sort(similarity[src_idx])[::-1]\n",
    "                    if len(sorted_sim) > 1:\n",
    "                        ratio = sorted_sim[0] / (sorted_sim[1] + 1e-8)\n",
    "                        if ratio < self.ratio_test:\n",
    "                            continue\n",
    "                \n",
    "                matches.append((src_idx, tgt_idx))\n",
    "                scores.append(src_scores[src_idx])\n",
    "        \n",
    "        return np.array(matches), np.array(scores)\n",
    "    \n",
    "    def match_keypoints(self, src_image, tgt_image, src_kps, feature_extractor):\n",
    "        \"\"\"\n",
    "        Match source keypoints to target image.\n",
    "        \n",
    "        Args:\n",
    "            src_image: Source PIL Image\n",
    "            tgt_image: Target PIL Image\n",
    "            src_kps: Source keypoints [N, 2]\n",
    "            feature_extractor: DenseFeatureExtractor instance\n",
    "            \n",
    "        Returns:\n",
    "            predicted_kps: Predicted target keypoints [N, 2]\n",
    "            confidence: Match confidence scores [N]\n",
    "        \"\"\"\n",
    "        # Extract dense features\n",
    "        src_feat_dict = feature_extractor.extract_features(src_image, return_numpy=True)\n",
    "        tgt_feat_dict = feature_extractor.extract_features(tgt_image, return_numpy=True)\n",
    "        \n",
    "        src_features = src_feat_dict['features']  # [H, W, D]\n",
    "        tgt_features = tgt_feat_dict['features']  # [H', W', D]\n",
    "        \n",
    "        # Get source keypoint features\n",
    "        src_kp_features = feature_extractor.extract_features_at_keypoints(src_image, src_kps)\n",
    "        \n",
    "        # Match to target feature map\n",
    "        tgt_h, tgt_w, tgt_d = tgt_features.shape\n",
    "        tgt_features_flat = tgt_features.reshape(-1, tgt_d)\n",
    "        \n",
    "        # Normalize features\n",
    "        src_kp_norm = src_kp_features / (np.linalg.norm(src_kp_features, axis=1, keepdims=True) + 1e-8)\n",
    "        tgt_norm = tgt_features_flat / (np.linalg.norm(tgt_features_flat, axis=1, keepdims=True) + 1e-8)\n",
    "        \n",
    "        # Find nearest neighbors\n",
    "        similarity = src_kp_norm @ tgt_norm.T  # [N, H'*W']\n",
    "        best_matches = np.argmax(similarity, axis=1)\n",
    "        confidence = np.max(similarity, axis=1)\n",
    "        \n",
    "        # Convert flat indices to 2D coordinates in feature space\n",
    "        match_y = best_matches // tgt_w\n",
    "        match_x = best_matches % tgt_w\n",
    "        \n",
    "        # Map back to original image coordinates\n",
    "        orig_w, orig_h = tgt_feat_dict['original_size']\n",
    "        scale_x = orig_w / tgt_w\n",
    "        scale_y = orig_h / tgt_h\n",
    "        \n",
    "        predicted_kps = np.stack([match_x * scale_x, match_y * scale_y], axis=1)\n",
    "        \n",
    "        return predicted_kps, confidence\n",
    "\n",
    "\n",
    "print(\"âœ“ Correspondence matcher defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3d63ee",
   "metadata": {},
   "source": [
    "## 12. Evaluation Metrics (PCK)\n",
    "\n",
    "The `PCKEvaluator` class computes **Percentage of Correct Keypoints (PCK)**:\n",
    "- Multiple thresholds: Î± = [0.05, 0.10, 0.15]\n",
    "- Normalization by bbox diagonal or image diagonal\n",
    "- Batch evaluation across entire datasets\n",
    "- Per-category performance tracking\n",
    "\n",
    "A keypoint is \"correct\" if predicted location is within Î± Ã— normalization_distance from ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a63ac5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PCK evaluator defined\n"
     ]
    }
   ],
   "source": [
    "class PCKEvaluator:\n",
    "    \"\"\"Evaluate correspondence using Percentage of Correct Keypoints (PCK).\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_values=[0.05, 0.10, 0.15], use_bbox=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha_values: Threshold values for PCK@alpha\n",
    "            use_bbox: Normalize by bounding box size (else use image size)\n",
    "        \"\"\"\n",
    "        self.alpha_values = alpha_values\n",
    "        self.use_bbox = use_bbox\n",
    "    \n",
    "    def compute_pck(self, predicted_kps, gt_kps, image_size=None, bbox=None):\n",
    "        \"\"\"\n",
    "        Compute PCK for a single image pair.\n",
    "        \n",
    "        Args:\n",
    "            predicted_kps: Predicted keypoints [N, 2]\n",
    "            gt_kps: Ground truth keypoints [N, 2]\n",
    "            image_size: (width, height) of target image\n",
    "            bbox: Bounding box [x, y, w, h] for normalization\n",
    "            \n",
    "        Returns:\n",
    "            pck_scores: Dict of PCK@alpha values\n",
    "        \"\"\"\n",
    "        if len(predicted_kps) == 0 or len(gt_kps) == 0:\n",
    "            return {f'PCK@{alpha}': 0.0 for alpha in self.alpha_values}\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = np.linalg.norm(predicted_kps - gt_kps, axis=1)\n",
    "        \n",
    "        # Compute normalization factor\n",
    "        if self.use_bbox and bbox is not None and len(bbox) == 4:\n",
    "            # Normalize by bounding box diagonal\n",
    "            norm_factor = np.sqrt(bbox[2]**2 + bbox[3]**2)\n",
    "        elif image_size is not None:\n",
    "            # Normalize by image diagonal\n",
    "            norm_factor = np.sqrt(image_size[0]**2 + image_size[1]**2)\n",
    "        else:\n",
    "            # No normalization\n",
    "            norm_factor = 1.0\n",
    "        \n",
    "        # Compute PCK at different thresholds\n",
    "        pck_scores = {}\n",
    "        for alpha in self.alpha_values:\n",
    "            threshold = alpha * norm_factor\n",
    "            correct = (distances <= threshold).sum()\n",
    "            pck = correct / len(distances)\n",
    "            pck_scores[f'PCK@{alpha}'] = pck\n",
    "        \n",
    "        return pck_scores\n",
    "    \n",
    "    def evaluate_dataset(self, predictions, ground_truth, image_sizes=None, bboxes=None):\n",
    "        \"\"\"\n",
    "        Evaluate PCK over entire dataset.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of predicted keypoints arrays\n",
    "            ground_truth: List of ground truth keypoints arrays\n",
    "            image_sizes: List of (width, height) tuples\n",
    "            bboxes: List of bounding boxes\n",
    "            \n",
    "        Returns:\n",
    "            results: Dict with mean PCK and per-sample results\n",
    "        \"\"\"\n",
    "        all_pck_scores = {f'PCK@{alpha}': [] for alpha in self.alpha_values}\n",
    "        per_sample_results = []\n",
    "        \n",
    "        for i, (pred_kps, gt_kps) in enumerate(zip(predictions, ground_truth)):\n",
    "            img_size = image_sizes[i] if image_sizes else None\n",
    "            bbox = bboxes[i] if bboxes else None\n",
    "            \n",
    "            pck = self.compute_pck(pred_kps, gt_kps, img_size, bbox)\n",
    "            per_sample_results.append(pck)\n",
    "            \n",
    "            for key, value in pck.items():\n",
    "                all_pck_scores[key].append(value)\n",
    "        \n",
    "        # Compute mean PCK\n",
    "        mean_pck = {key: np.mean(values) for key, values in all_pck_scores.items()}\n",
    "        \n",
    "        results = {\n",
    "            'mean': mean_pck,\n",
    "            'per_sample': per_sample_results,\n",
    "            'num_samples': len(predictions)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_results(self, results):\n",
    "        \"\"\"Pretty print evaluation results.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"PCK EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Number of samples: {results['num_samples']}\")\n",
    "        print(\"\\nMean PCK scores:\")\n",
    "        for key, value in sorted(results['mean'].items()):\n",
    "            print(f\"  {key}: {value*100:.2f}%\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"âœ“ PCK evaluator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b665b",
   "metadata": {},
   "source": [
    "## 13. End-to-End Evaluation Pipeline\n",
    "\n",
    "The `evaluate_correspondence()` function wraps the entire pipeline:\n",
    "1. Feature extraction from source and target images\n",
    "2. Correspondence matching with selected algorithm\n",
    "3. PCK evaluation at multiple thresholds\n",
    "4. Progress tracking with tqdm\n",
    "\n",
    "Returns predictions, ground truth, confidences, and PCK scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7950ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Evaluation pipeline defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_correspondence(model, dataset, backbone='dinov2', device='cuda', \n",
    "                           max_samples=None, mutual_nn=False):\n",
    "    \"\"\"\n",
    "    End-to-end evaluation pipeline for semantic correspondence.\n",
    "    \n",
    "    Args:\n",
    "        model: Pretrained model (DINOv2 or SAM)\n",
    "        dataset: Dataset instance (PFPascalDataset or SPairDataset)\n",
    "        backbone: 'dinov2' or 'sam'\n",
    "        device: Device to run on\n",
    "        max_samples: Limit number of samples (None for all)\n",
    "        mutual_nn: Use mutual nearest neighbor matching\n",
    "        \n",
    "    Returns:\n",
    "        results: Evaluation results including PCK scores\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"EVALUATING {backbone.upper()} on {dataset.__class__.__name__}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    if max_samples:\n",
    "        print(f\"Evaluating on: {max_samples} samples\")\n",
    "    print(f\"Mutual NN: {mutual_nn}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Initialize components\n",
    "    feature_extractor = DenseFeatureExtractor(backbone=backbone, model=model, device=device)\n",
    "    matcher = CorrespondenceMatcher(method='nn', mutual=mutual_nn)\n",
    "    evaluator = PCKEvaluator(alpha_values=[0.05, 0.10, 0.15])\n",
    "    \n",
    "    # Collect predictions and ground truth\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    image_sizes = []\n",
    "    bboxes = []\n",
    "    confidences = []\n",
    "    \n",
    "    num_samples = min(max_samples, len(dataset)) if max_samples else len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(num_samples), desc=\"Processing pairs\"):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        src_img = sample['source_image']\n",
    "        tgt_img = sample['target_image']\n",
    "        src_kps = sample['source_keypoints']\n",
    "        tgt_kps = sample['target_keypoints']\n",
    "        \n",
    "        if len(src_kps) == 0 or len(tgt_kps) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Match keypoints\n",
    "        pred_kps, conf = matcher.match_keypoints(\n",
    "            src_img, tgt_img, src_kps, feature_extractor\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred_kps)\n",
    "        ground_truths.append(tgt_kps)\n",
    "        confidences.append(conf)\n",
    "        \n",
    "        # Get image size\n",
    "        if isinstance(tgt_img, Image.Image):\n",
    "            image_sizes.append(tgt_img.size)  # (W, H)\n",
    "        else:\n",
    "            image_sizes.append((tgt_img.shape[2], tgt_img.shape[1]))\n",
    "        \n",
    "        # Get bbox if available\n",
    "        if 'target_bbox' in sample and sample['target_bbox'] is not None:\n",
    "            bboxes.append(sample['target_bbox'])\n",
    "        else:\n",
    "            bboxes.append(None)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluator.evaluate_dataset(\n",
    "        predictions, ground_truths, image_sizes, bboxes\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    evaluator.print_results(results)\n",
    "    \n",
    "    # Add additional info\n",
    "    results['predictions'] = predictions\n",
    "    results['ground_truth'] = ground_truths\n",
    "    results['confidences'] = confidences\n",
    "    results['backbone'] = backbone\n",
    "    results['mutual_nn'] = mutual_nn\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ“ Evaluation pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd6b79",
   "metadata": {},
   "source": [
    "## 14. Visualization Utilities\n",
    "\n",
    "Advanced visualization functions for analyzing correspondence results:\n",
    "- **visualize_matches()**: Shows source/target images with predicted and GT keypoints\n",
    "- **visualize_feature_similarity()**: Displays feature similarity heatmaps for debugging\n",
    "\n",
    "Helps understand model behavior and identify failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a30c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Visualization utilities defined\n"
     ]
    }
   ],
   "source": [
    "def visualize_matches(src_img, tgt_img, src_kps, pred_kps, gt_kps=None, \n",
    "                     max_points=20, figsize=(20, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize correspondence matches between two images.\n",
    "    \n",
    "    Args:\n",
    "        src_img: Source image (PIL or numpy)\n",
    "        tgt_img: Target image (PIL or numpy)\n",
    "        src_kps: Source keypoints [N, 2]\n",
    "        pred_kps: Predicted target keypoints [N, 2]\n",
    "        gt_kps: Ground truth target keypoints [N, 2] (optional)\n",
    "        max_points: Maximum number of points to visualize\n",
    "        figsize: Figure size\n",
    "        save_path: Path to save figure (optional)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    from matplotlib.lines import Line2D\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(src_img, Image.Image):\n",
    "        src_img = np.array(src_img)\n",
    "    if isinstance(tgt_img, Image.Image):\n",
    "        tgt_img = np.array(tgt_img)\n",
    "    \n",
    "    # Limit number of points for clarity\n",
    "    if len(src_kps) > max_points:\n",
    "        indices = np.random.choice(len(src_kps), max_points, replace=False)\n",
    "        src_kps = src_kps[indices]\n",
    "        pred_kps = pred_kps[indices]\n",
    "        if gt_kps is not None:\n",
    "            gt_kps = gt_kps[indices]\n",
    "    \n",
    "    # Create figure\n",
    "    if gt_kps is not None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        ax_src, ax_pred, ax_gt = axes\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(figsize[0]*2/3, figsize[1]))\n",
    "        ax_src, ax_pred = axes\n",
    "        ax_gt = None\n",
    "    \n",
    "    # Plot source image with keypoints\n",
    "    ax_src.imshow(src_img)\n",
    "    ax_src.scatter(src_kps[:, 0], src_kps[:, 1], c='red', s=100, marker='o', \n",
    "                   edgecolors='white', linewidths=2, label='Source KPs')\n",
    "    ax_src.set_title('Source Image', fontsize=14, fontweight='bold')\n",
    "    ax_src.axis('off')\n",
    "    \n",
    "    # Plot target image with predicted keypoints\n",
    "    ax_pred.imshow(tgt_img)\n",
    "    ax_pred.scatter(pred_kps[:, 0], pred_kps[:, 1], c='blue', s=100, marker='x', \n",
    "                    linewidths=3, label='Predicted KPs')\n",
    "    ax_pred.set_title('Target Image (Predictions)', fontsize=14, fontweight='bold')\n",
    "    ax_pred.axis('off')\n",
    "    \n",
    "    # Plot target with ground truth if available\n",
    "    if gt_kps is not None and ax_gt is not None:\n",
    "        ax_gt.imshow(tgt_img)\n",
    "        ax_gt.scatter(gt_kps[:, 0], gt_kps[:, 1], c='green', s=100, marker='o', \n",
    "                     edgecolors='white', linewidths=2, label='Ground Truth')\n",
    "        ax_gt.scatter(pred_kps[:, 0], pred_kps[:, 1], c='blue', s=50, marker='x', \n",
    "                     linewidths=2, alpha=0.7, label='Predicted')\n",
    "        \n",
    "        # Draw error lines\n",
    "        for i in range(len(gt_kps)):\n",
    "            ax_gt.plot([gt_kps[i, 0], pred_kps[i, 0]], \n",
    "                      [gt_kps[i, 1], pred_kps[i, 1]], \n",
    "                      'r--', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # Compute errors\n",
    "        errors = np.linalg.norm(pred_kps - gt_kps, axis=1)\n",
    "        mean_error = errors.mean()\n",
    "        ax_gt.set_title(f'Ground Truth vs Predicted\\nMean Error: {mean_error:.2f}px', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "        ax_gt.axis('off')\n",
    "        ax_gt.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"âœ“ Saved visualization to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualize_feature_similarity(src_img, tgt_img, feature_extractor, kp_idx=0, src_kps=None):\n",
    "    \"\"\"\n",
    "    Visualize feature similarity map for a keypoint.\n",
    "    \n",
    "    Args:\n",
    "        src_img: Source image\n",
    "        tgt_img: Target image\n",
    "        feature_extractor: DenseFeatureExtractor instance\n",
    "        kp_idx: Index of keypoint to visualize\n",
    "        src_kps: Source keypoints [N, 2]\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    src_feat_dict = feature_extractor.extract_features(src_img, return_numpy=True)\n",
    "    tgt_feat_dict = feature_extractor.extract_features(tgt_img, return_numpy=True)\n",
    "    \n",
    "    src_features = src_feat_dict['features']\n",
    "    tgt_features = tgt_feat_dict['features']\n",
    "    \n",
    "    # Get query feature\n",
    "    if src_kps is not None and kp_idx < len(src_kps):\n",
    "        query_feat = feature_extractor.extract_features_at_keypoints(src_img, src_kps[kp_idx:kp_idx+1])\n",
    "    else:\n",
    "        # Use center point\n",
    "        h, w = src_features.shape[:2]\n",
    "        query_feat = src_features[h//2, w//2:w//2+1, :]\n",
    "    \n",
    "    # Compute similarity map\n",
    "    query_norm = query_feat / (np.linalg.norm(query_feat) + 1e-8)\n",
    "    tgt_h, tgt_w, tgt_d = tgt_features.shape\n",
    "    tgt_flat = tgt_features.reshape(-1, tgt_d)\n",
    "    tgt_norm = tgt_flat / (np.linalg.norm(tgt_flat, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    similarity = (query_norm @ tgt_norm.T).reshape(tgt_h, tgt_w)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(src_img)\n",
    "    if src_kps is not None and kp_idx < len(src_kps):\n",
    "        axes[0].scatter(src_kps[kp_idx, 0], src_kps[kp_idx, 1], \n",
    "                       c='red', s=200, marker='*', edgecolors='white', linewidths=2)\n",
    "    axes[0].set_title('Source Image (Query Point)', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(tgt_img)\n",
    "    axes[1].set_title('Target Image', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    im = axes[2].imshow(similarity, cmap='hot', interpolation='bilinear')\n",
    "    axes[2].set_title('Feature Similarity Map', fontsize=12, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    plt.colorbar(im, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"âœ“ Visualization utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef5d26",
   "metadata": {},
   "source": [
    "## 15. Example Usage & Experiments\n",
    "\n",
    "The following cells demonstrate how to use the pipeline. Uncomment and run to experiment:\n",
    "- **Example 1**: Load datasets\n",
    "- **Example 2**: Evaluate on test split\n",
    "- **Example 3**: Visualize matches\n",
    "- **Example 4**: Compare different backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0ca1b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset loading examples defined (uncomment to use)\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Load a dataset\n",
    "# Uncomment and run after downloading datasets\n",
    "\n",
    "# # Load PF-Pascal test split\n",
    "# pf_pascal = PFPascalDataset(\n",
    "#     root_dir=os.path.join(sd4match_data_dir, 'pf-pascal'),\n",
    "#     split='test'\n",
    "# )\n",
    "# print(f\"PF-Pascal test set: {len(pf_pascal)} pairs\")\n",
    "\n",
    "# # Load SPair-71k test split\n",
    "# spair = SPairDataset(\n",
    "#     root_dir=os.path.join(sd4match_data_dir, 'spair-71k'),\n",
    "#     split='test'\n",
    "# )\n",
    "# print(f\"SPair-71k test set: {len(spair)} pairs\")\n",
    "\n",
    "print(\"âœ“ Dataset loading examples defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cec5516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Evaluation example defined (uncomment to use)\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Evaluate DINOv2 on a dataset\n",
    "# Uncomment and run after loading models and datasets\n",
    "\n",
    "# if dinov2_model is not None:\n",
    "#     # Evaluate on first 50 samples (for quick testing)\n",
    "#     results_dinov2 = evaluate_correspondence(\n",
    "#         model=dinov2_model,\n",
    "#         dataset=pf_pascal,\n",
    "#         backbone='dinov2',\n",
    "#         device=device,\n",
    "#         max_samples=50,\n",
    "#         mutual_nn=False\n",
    "#     )\n",
    "#     \n",
    "#     # Save results\n",
    "#     import json\n",
    "#     results_path = os.path.join(OUTPUT_DIR, 'dinov2_pfpascal_results.json')\n",
    "#     with open(results_path, 'w') as f:\n",
    "#         # Save only serializable parts\n",
    "#         json.dump({\n",
    "#             'mean': results_dinov2['mean'],\n",
    "#             'num_samples': results_dinov2['num_samples'],\n",
    "#             'backbone': results_dinov2['backbone']\n",
    "#         }, f, indent=2)\n",
    "#     print(f\"âœ“ Results saved to {results_path}\")\n",
    "\n",
    "print(\"âœ“ Evaluation example defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dfa5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Visualization example defined (uncomment to use)\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Visualize a single correspondence\n",
    "# Uncomment and run to visualize results\n",
    "\n",
    "# if 'pf_pascal' in locals() and len(pf_pascal) > 0:\n",
    "#     # Get a sample\n",
    "#     sample_idx = 0\n",
    "#     sample = pf_pascal[sample_idx]\n",
    "#     \n",
    "#     # Extract predictions\n",
    "#     feature_extractor = DenseFeatureExtractor(backbone='dinov2', model=dinov2_model, device=device)\n",
    "#     matcher = CorrespondenceMatcher(method='nn', mutual=False)\n",
    "#     \n",
    "#     pred_kps, conf = matcher.match_keypoints(\n",
    "#         sample['source_image'],\n",
    "#         sample['target_image'],\n",
    "#         sample['source_keypoints'],\n",
    "#         feature_extractor\n",
    "#     )\n",
    "#     \n",
    "#     # Visualize\n",
    "#     fig = visualize_matches(\n",
    "#         sample['source_image'],\n",
    "#         sample['target_image'],\n",
    "#         sample['source_keypoints'],\n",
    "#         pred_kps,\n",
    "#         sample['target_keypoints'],\n",
    "#         max_points=15,\n",
    "#         save_path=os.path.join(OUTPUT_DIR, f'match_visualization_{sample_idx}.png')\n",
    "#     )\n",
    "#     plt.show()\n",
    "\n",
    "print(\"âœ“ Visualization example defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2040b9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Comparison example defined (uncomment to use)\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Compare different backbones\n",
    "# Uncomment to run comparative experiments\n",
    "\n",
    "# def compare_backbones(dataset, max_samples=100):\n",
    "#     \"\"\"Compare DINOv2 vs SAM on a dataset.\"\"\"\n",
    "#     results = {}\n",
    "#     \n",
    "#     # Evaluate DINOv2\n",
    "#     if dinov2_model is not None:\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         print(\"EVALUATING DINOV2\")\n",
    "#         print(\"=\"*60)\n",
    "#         results['dinov2'] = evaluate_correspondence(\n",
    "#             model=dinov2_model,\n",
    "#             dataset=dataset,\n",
    "#             backbone='dinov2',\n",
    "#             device=device,\n",
    "#             max_samples=max_samples,\n",
    "#             mutual_nn=False\n",
    "#         )\n",
    "#     \n",
    "#     # Evaluate SAM\n",
    "#     if sam_model is not None:\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         print(\"EVALUATING SAM\")\n",
    "#         print(\"=\"*60)\n",
    "#         results['sam'] = evaluate_correspondence(\n",
    "#             model=sam_model,\n",
    "#             dataset=dataset,\n",
    "#             backbone='sam',\n",
    "#             device=device,\n",
    "#             max_samples=max_samples,\n",
    "#             mutual_nn=False\n",
    "#         )\n",
    "#     \n",
    "#     # Print comparison\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"COMPARISON SUMMARY\")\n",
    "#     print(\"=\"*60)\n",
    "#     for backbone, res in results.items():\n",
    "#         print(f\"\\n{backbone.upper()}:\")\n",
    "#         for metric, value in res['mean'].items():\n",
    "#             print(f\"  {metric}: {value*100:.2f}%\")\n",
    "#     \n",
    "#     return results\n",
    "# \n",
    "# # Run comparison\n",
    "# # comparison_results = compare_backbones(pf_pascal, max_samples=50)\n",
    "\n",
    "print(\"âœ“ Comparison example defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3403f",
   "metadata": {},
   "source": [
    "## 16. Project Summary & Next Steps\n",
    "\n",
    "### âœ… Completed Implementation\n",
    "\n",
    "**Phase 1 - Infrastructure:**\n",
    "- âœ“ DINOv2 ViT-B model loaded and ready\n",
    "- âœ“ SAM ViT-B model loaded and ready  \n",
    "- âœ“ Dataset download utilities for PF-Pascal, PF-Willow, SPair-71k\n",
    "- âœ“ Environment configuration (paths, device detection)\n",
    "\n",
    "**Phase 2 - Core Pipeline:**\n",
    "- âœ“ Dataset loaders (`PFPascalDataset`, `SPairDataset`)\n",
    "- âœ“ Dense feature extraction (`DenseFeatureExtractor`)\n",
    "- âœ“ Correspondence matching (`CorrespondenceMatcher`)\n",
    "  - Nearest neighbor matching\n",
    "  - Mutual nearest neighbor option\n",
    "  - Ratio test support\n",
    "- âœ“ PCK evaluation metrics (`PCKEvaluator`)\n",
    "  - PCK@0.05, PCK@0.10, PCK@0.15\n",
    "  - Bbox and image size normalization\n",
    "- âœ“ End-to-end evaluation pipeline\n",
    "- âœ“ Visualization utilities\n",
    "\n",
    "### ðŸŽ¯ How to Use\n",
    "\n",
    "**Step 1: Ensure all setup cells are run**\n",
    "```python\n",
    "# Run cells 1-5 to set up environment\n",
    "# Run cells for DINOv2 (section 3)\n",
    "# Run cells for SAM (section 5)\n",
    "```\n",
    "\n",
    "**Step 2: Download datasets**\n",
    "```python\n",
    "# The dataset download cell (section 2) attempts automatic download\n",
    "# Or manually download and place in DATA_ROOT/SD4Match/\n",
    "```\n",
    "\n",
    "**Step 3: Load a dataset**\n",
    "```python\n",
    "pf_pascal = PFPascalDataset(\n",
    "    root_dir=os.path.join(sd4match_data_dir, 'pf-pascal'),\n",
    "    split='test'\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 4: Run evaluation**\n",
    "```python\n",
    "results = evaluate_correspondence(\n",
    "    model=dinov2_model,\n",
    "    dataset=pf_pascal,\n",
    "    backbone='dinov2',\n",
    "    device=device,\n",
    "    max_samples=50  # Start with small number\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 5: Visualize results**\n",
    "```python\n",
    "# Use visualization functions to inspect matches\n",
    "```\n",
    "\n",
    "### ðŸ“Š Evaluation Protocol (Professor's Guidelines)\n",
    "\n",
    "1. **Train on `trn` split** (if doing any training/fine-tuning)\n",
    "2. **Validate on `val` split** for model selection and hyperparameter tuning\n",
    "3. **Report final results ONLY on `test` split**\n",
    "4. **Metrics**: PCK@0.05, PCK@0.10, PCK@0.15\n",
    "5. **Backbones**: Compare DINOv2 ViT-B vs SAM ViT-B\n",
    "\n",
    "### ðŸ”¬ Suggested Experiments\n",
    "\n",
    "1. **Baseline Comparison**\n",
    "   - DINOv2 ViT-B vs SAM ViT-B\n",
    "   - With/without mutual nearest neighbor\n",
    "\n",
    "2. **Hyperparameter Tuning** (on val split)\n",
    "   - Matching thresholds\n",
    "   - Feature normalization strategies\n",
    "   - Ratio test thresholds\n",
    "\n",
    "3. **Dataset Analysis**\n",
    "   - Per-category performance\n",
    "   - Effect of viewpoint changes\n",
    "   - Effect of scale changes\n",
    "\n",
    "4. **Advanced Methods** (optional)\n",
    "   - Window soft argmax refinement (GeoAware-SC)\n",
    "   - Multi-scale features\n",
    "   - Feature aggregation strategies\n",
    "\n",
    "### ðŸ“ Notes\n",
    "\n",
    "- All code follows professor's recommendations (Base models, official repos, proper splits)\n",
    "- The pipeline is modular - easy to swap backbones or add new methods\n",
    "- Visualization utilities help debug and understand model behavior\n",
    "- Start with small `max_samples` for quick iteration, then scale up\n",
    "\n",
    "### ðŸš€ Ready to Run!\n",
    "\n",
    "Uncomment the example cells in section 15 to start experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
