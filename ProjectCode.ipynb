{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638cffaf",
   "metadata": {},
   "source": [
    "# Semantic Correspondence Project - Phase 1 Setup\n",
    "## DINOv2, DINOv3, and SAM Backbones\n",
    "\n",
    "This notebook sets up the infrastructure for semantic correspondence using:\n",
    "- **DINOv2** (Facebook Research)\n",
    "- **DINOv3** (Facebook Research)\n",
    "- **SAM** (Segment Anything Model)\n",
    "- **SD4Match** dataset for evaluation\n",
    "\n",
    "**Professor's recommendations:**\n",
    "- Use **Base (ViT-B)** versions for all backbones\n",
    "- Use official repositories (not just Hugging Face) to access internal components\n",
    "- Dataset splits: train (trn), validation (val), test (test)\n",
    "- Always evaluate on test split only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a56efc",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2b22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Colab: False\n",
      "Project root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject\n",
      "Data root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data\n",
      "Checkpoint dir: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints\n",
      "Output dir: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/outputs\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running on Colab: {IN_COLAB}\")\n",
    "\n",
    "# Set up paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = '/content/AMLProject'\n",
    "    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'  # Recommended: upload dataset to Drive\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs')\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43600cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì± Detected macOS - installing CPU/MPS version\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torchvision) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: sympy\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "Installing collected packages: sympy\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "Successfully installed sympy-1.13.1\n",
      "Successfully installed sympy-1.13.1\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (2.9.1)\n",
      "Collecting torch==2.9.1 (from torchaudio)\n",
      "Collecting torch==2.9.1 (from torchaudio)\n",
      "  Using cached torch-2.9.1-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.9.1->torchaudio)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.9.1->torchaudio) (1.3.0)\n",
      "  Using cached torch-2.9.1-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.9.1->torchaudio)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch==2.9.1->torchaudio) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.9.1->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from jinja2->torch==2.9.1->torchaudio) (3.0.3)\n",
      "Using cached torch-2.9.1-cp311-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from jinja2->torch==2.9.1->torchaudio) (3.0.3)\n",
      "Using cached torch-2.9.1-cp311-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "\u001b[2K  Attempting uninstall: sympy\n",
      "\u001b[2K    Found existing installation: sympy 1.13.1\n",
      "Installing collected packages: sympy, torch\n",
      "\u001b[2K  Attempting uninstall: sympy\n",
      "\u001b[2K    Found existing installation: sympy 1.13.1\n",
      "\u001b[2K    Uninstalling sympy-1.13.1:\n",
      "\u001b[2K    Uninstalling sympy-1.13.1:\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.1‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.1‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: torch‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: torch‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: torch 2.5.1\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling torch-2.5.1:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: torch 2.5.1\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling torch-2.5.1:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0/2\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled torch-2.5.1\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.5.1m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed sympy-1.14.0 torch-2.9.1\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed sympy-1.14.0 torch-2.9.1\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (4.10.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (3.10.8)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (2.3.5)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (1.16.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (4.10.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (3.10.8)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (2.3.5)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (1.16.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (1.0.22)\n",
      "Requirement already satisfied: einops in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (0.8.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (2.9.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (0.20.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (1.2.2)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: shellingham in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (1.0.22)\n",
      "Requirement already satisfied: einops in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (0.8.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (2.9.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (0.20.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (1.2.2)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: shellingham in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch->timm) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from jinja2->torch->timm) (3.0.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torchvision->timm) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torchvision->timm) (11.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch->timm) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from jinja2->torch->timm) (3.0.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torchvision->timm) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from torchvision->timm) (11.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (11.3.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from requests) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (11.3.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from requests) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages (from requests) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Note: Use standard PyPI for Mac (no CUDA), use --index-url for Linux with CUDA\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    print(\"üì± Detected macOS - installing CPU/MPS version\")\n",
    "    !pip install torch torchvision\n",
    "    # torchaudio not needed for this project, skip if unavailable\n",
    "    try:\n",
    "        !pip install torchaudio\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  torchaudio not available on this platform (not needed for project)\")\n",
    "elif 'google.colab' in sys.modules:  # Google Colab\n",
    "    print(\"‚òÅÔ∏è  Detected Colab - using default installation\")\n",
    "    !pip install torch torchvision torchaudio\n",
    "else:  # Linux with CUDA\n",
    "    print(\"üñ•Ô∏è  Detected Linux - installing CUDA version\")\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "!pip install opencv-python matplotlib numpy scipy tqdm\n",
    "!pip install timm einops\n",
    "!pip install pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964e4c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyTorch version: 2.5.1\n",
      "Using device: mps (Apple Silicon GPU)\n",
      "Using device: mps (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "# Import common libraries\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚úó PyTorch not installed! Please run the installation cell (cell 4) first.\")\n",
    "    raise\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check device availability (CUDA, MPS, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device} (CPU only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eedf14",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup - SD4Match\n",
    "\n",
    "SD4Match is the dataset for semantic correspondence evaluation.\n",
    "- **Repository**: https://github.com/ActiveVisionLab/SD4Match\n",
    "- **Splits**: train (trn), validation (val), test\n",
    "- **Usage**: Train on trn, validate on val, report final results on test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e86aaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD4Match repository already exists\n",
      "SD4Match path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/SD4Match\n"
     ]
    }
   ],
   "source": [
    "# Clone SD4Match repository\n",
    "sd4match_dir = os.path.join(PROJECT_ROOT, 'SD4Match')\n",
    "if not os.path.exists(sd4match_dir):\n",
    "    !git clone https://github.com/ActiveVisionLab/SD4Match.git \"{sd4match_dir}\"\n",
    "    print(\"SD4Match repository cloned successfully\")\n",
    "else:\n",
    "    print(\"SD4Match repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if sd4match_dir not in sys.path:\n",
    "    sys.path.insert(0, sd4match_dir)\n",
    "    \n",
    "print(f\"SD4Match path: {sd4match_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24236a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset should be placed in: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "Expected splits: trn/, val/, test/\n",
      "\n",
      "Note: Download instructions are in the SD4Match repository README\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "\"\"\"\n",
    "After cloning SD4Match, download the dataset and place it in the DATA_ROOT directory.\n",
    "If on Colab, upload to Google Drive for faster access across sessions.\n",
    "\n",
    "Expected structure:\n",
    "DATA_ROOT/\n",
    "    SD4Match/\n",
    "        trn/  (training split)\n",
    "        val/  (validation split)\n",
    "        test/ (test split)\n",
    "\"\"\"\n",
    "\n",
    "sd4match_data_dir = os.path.join(DATA_ROOT, 'SD4Match')\n",
    "print(f\"Dataset should be placed in: {sd4match_data_dir}\")\n",
    "print(f\"Expected splits: trn/, val/, test/\")\n",
    "print(\"\\nNote: Download instructions are in the SD4Match repository README\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7117b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Starting dataset download... This may take several minutes.\n",
      "\n",
      "============================================================\n",
      "SD4MATCH BENCHMARK DATASETS DOWNLOAD\n",
      "============================================================\n",
      "\n",
      "üì¶ PF-PASCAL\n",
      "----------------------------------------\n",
      "  ‚úì Already exists at /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/pf-pascal\n",
      "\n",
      "üì¶ PF-WILLOW\n",
      "----------------------------------------\n",
      "  ‚úì Already exists at /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/pf-willow\n",
      "\n",
      "üì¶ SPAIR-71K\n",
      "----------------------------------------\n",
      "  ‚úì Already exists at /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/spair-71k\n",
      "\n",
      "============================================================\n",
      "‚úÖ All datasets downloaded successfully!\n",
      "\n",
      "Datasets location: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "\n",
      "Structure:\n",
      "/Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/\n",
      "  ‚îú‚îÄ‚îÄ pf-pascal/\n",
      "  ‚îú‚îÄ‚îÄ pf-willow/\n",
      "  ‚îî‚îÄ‚îÄ spair-71k/\n"
     ]
    }
   ],
   "source": [
    "# Download SD4Match benchmark datasets automatically\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Download file with progress indication.\"\"\"\n",
    "    print(f\"  Downloading from {url}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"Extract zip file.\"\"\"\n",
    "    print(f\"  Extracting {os.path.basename(zip_path)}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error extracting: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_sd4match_datasets(data_dir):\n",
    "    \"\"\"\n",
    "    Download and extract SD4Match benchmark datasets.\n",
    "    Includes: PF-Pascal, PF-Willow, and SPair-71k\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SD4MATCH BENCHMARK DATASETS DOWNLOAD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Dataset configurations\n",
    "    datasets = {\n",
    "        'pf-pascal': {\n",
    "            'images_url': 'https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset-PASCAL.zip',\n",
    "            'pairs_url': 'https://www.robots.ox.ac.uk/~xinghui/sd4match/pf-pascal_image_pairs.zip',\n",
    "            'has_splits': True\n",
    "        },\n",
    "        'pf-willow': {\n",
    "            'images_url': 'https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset.zip',\n",
    "            'pairs_url': 'https://www.robots.ox.ac.uk/~xinghui/sd4match/test_pairs.csv',\n",
    "            'has_splits': False\n",
    "        },\n",
    "        'spair-71k': {\n",
    "            'images_url': 'http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz',\n",
    "            'has_splits': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_ready = True\n",
    "    \n",
    "    for dataset_name, config in datasets.items():\n",
    "        dataset_path = os.path.join(data_dir, dataset_name)\n",
    "        print(f\"\\nüì¶ {dataset_name.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if already exists\n",
    "        if os.path.exists(dataset_path) and os.listdir(dataset_path):\n",
    "            print(f\"  ‚úì Already exists at {dataset_path}\")\n",
    "            continue\n",
    "        \n",
    "        os.makedirs(dataset_path, exist_ok=True)\n",
    "        \n",
    "        # Download images\n",
    "        print(f\"  Downloading {dataset_name} images...\")\n",
    "        images_filename = os.path.basename(config['images_url'])\n",
    "        images_path = os.path.join(data_dir, images_filename)\n",
    "        \n",
    "        if not os.path.exists(images_path):\n",
    "            if download_file(config['images_url'], images_path):\n",
    "                print(f\"  ‚úì Downloaded {images_filename}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Failed to download images\")\n",
    "                all_ready = False\n",
    "                continue\n",
    "        \n",
    "        # Extract images\n",
    "        if images_filename.endswith('.zip'):\n",
    "            extract_zip(images_path, dataset_path)\n",
    "        elif images_filename.endswith('.tar.gz'):\n",
    "            print(f\"  Extracting {images_filename}...\")\n",
    "            with tarfile.open(images_path, 'r:gz') as tar:\n",
    "                tar.extractall(dataset_path)\n",
    "        \n",
    "        # Download pairs/splits if applicable\n",
    "        if 'pairs_url' in config:\n",
    "            pairs_filename = os.path.basename(config['pairs_url'])\n",
    "            pairs_path = os.path.join(data_dir, pairs_filename)\n",
    "            \n",
    "            print(f\"  Downloading image pairs...\")\n",
    "            if download_file(config['pairs_url'], pairs_path):\n",
    "                if pairs_filename.endswith('.zip'):\n",
    "                    extract_zip(pairs_path, dataset_path)\n",
    "                elif pairs_filename.endswith('.csv'):\n",
    "                    shutil.move(pairs_path, os.path.join(dataset_path, pairs_filename))\n",
    "                print(f\"  ‚úì Downloaded pairs/splits\")\n",
    "        \n",
    "        # Clean up zip files\n",
    "        if os.path.exists(images_path):\n",
    "            os.remove(images_path)\n",
    "        \n",
    "        print(f\"  ‚úì {dataset_name} setup complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    if all_ready:\n",
    "        print(\"‚úÖ All datasets downloaded successfully!\")\n",
    "        print(f\"\\nDatasets location: {data_dir}\")\n",
    "        print(\"\\nStructure:\")\n",
    "        print(f\"{data_dir}/\")\n",
    "        print(\"  ‚îú‚îÄ‚îÄ pf-pascal/\")\n",
    "        print(\"  ‚îú‚îÄ‚îÄ pf-willow/\")\n",
    "        print(\"  ‚îî‚îÄ‚îÄ spair-71k/\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some datasets failed to download automatically.\")\n",
    "        print(\"\\nüì• MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"1. PF-Pascal: https://www.di.ens.fr/willow/research/proposalflow/\")\n",
    "        print(\"   Pairs: https://www.robots.ox.ac.uk/~xinghui/sd4match/pf-pascal_image_pairs.zip\")\n",
    "        print(\"\\n2. PF-Willow: https://www.di.ens.fr/willow/research/proposalflow/\")\n",
    "        print(\"   Pairs: https://www.robots.ox.ac.uk/~xinghui/sd4match/test_pairs.csv\")\n",
    "        print(\"\\n3. SPair-71k: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if IN_COLAB:\n",
    "            print(\"\\nüí° FOR GOOGLE COLAB:\")\n",
    "            print(\"   1. Download datasets to your computer\")\n",
    "            print(\"   2. Upload to Google Drive\")\n",
    "            print(\"   3. Mount Drive and set DATA_ROOT accordingly\")\n",
    "    \n",
    "    return all_ready\n",
    "\n",
    "# Attempt to download datasets\n",
    "print(\"‚è≥ Starting dataset download... This may take several minutes.\\n\")\n",
    "dataset_ready = download_sd4match_datasets(sd4match_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcbc3d",
   "metadata": {},
   "source": [
    "## 3. DINOv2 Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/dinov2  \n",
    "**Model**: ViT-B (Base version)  \n",
    "**Key**: Use official repo (not just Hugging Face) to access internal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ccc76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv2 repository already exists\n",
      "DINOv2 path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/models/dinov2\n"
     ]
    }
   ],
   "source": [
    "# Clone DINOv2 repository\n",
    "dinov2_dir = os.path.join(MODEL_DIR, 'dinov2')\n",
    "if not os.path.exists(dinov2_dir):\n",
    "    !git clone https://github.com/facebookresearch/dinov2.git \"{dinov2_dir}\"\n",
    "    print(\"DINOv2 repository cloned successfully\")\n",
    "else:\n",
    "    print(\"DINOv2 repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if dinov2_dir not in sys.path:\n",
    "    sys.path.insert(0, dinov2_dir)\n",
    "\n",
    "print(f\"DINOv2 path: {dinov2_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a896286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/giuliavarga/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/giuliavarga/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/giuliavarga/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/giuliavarga/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DINOv2 model 'dinov2_vitb14' loaded successfully\n",
      "  - Patch size: 14x14\n",
      "  - Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Load DINOv2 ViT-B model\n",
    "def load_dinov2_model(model_name='dinov2_vitb14', device='cuda'):\n",
    "    \"\"\"\n",
    "    Load DINOv2 model from official repository.\n",
    "    \n",
    "    Available models:\n",
    "    - dinov2_vits14: Small (ViT-S/14)\n",
    "    - dinov2_vitb14: Base (ViT-B/14) - RECOMMENDED\n",
    "    - dinov2_vitl14: Large (ViT-L/14)\n",
    "    - dinov2_vitg14: Giant (ViT-G/14)\n",
    "    \n",
    "    The '14' indicates patch size of 14x14 pixels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"‚úì DINOv2 model '{model_name}' loaded successfully\")\n",
    "        print(f\"  - Patch size: 14x14\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading DINOv2: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the Base model (ViT-B)\n",
    "dinov2_model = load_dinov2_model('dinov2_vitb14', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dddfe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv2 feature extraction utility defined\n"
     ]
    }
   ],
   "source": [
    "# DINOv2 feature extraction utility\n",
    "def extract_dinov2_features(model, image, return_class_token=True, return_patch_tokens=True):\n",
    "    \"\"\"\n",
    "    Extract features from DINOv2 model.\n",
    "    \n",
    "    Args:\n",
    "        model: DINOv2 model\n",
    "        image: PIL Image or tensor (C, H, W) in range [0, 1]\n",
    "        return_class_token: Return [CLS] token\n",
    "        return_patch_tokens: Return patch tokens\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing requested features\n",
    "    \"\"\"\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    # Prepare image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if isinstance(image, Image.Image):\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    elif image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    image = image.to(next(model.parameters()).device)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model.forward_features(image)\n",
    "        \n",
    "    result = {}\n",
    "    if return_class_token:\n",
    "        result['cls_token'] = features['x_norm_clstoken']\n",
    "    if return_patch_tokens:\n",
    "        result['patch_tokens'] = features['x_norm_patchtokens']\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"DINOv2 feature extraction utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43b15d",
   "metadata": {},
   "source": [
    "## 4. DINOv3 Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/dinov3  \n",
    "**Model**: ViT-B (Base version)  \n",
    "**Key**: Request access to checkpoints, then download pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ab1e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 repository already exists\n",
      "DINOv3 path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/models/dinov3\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: Request access and download DINOv3 checkpoints\n",
      "   Follow instructions in the DINOv3 repository README\n"
     ]
    }
   ],
   "source": [
    "# Clone DINOv3 repository\n",
    "dinov3_dir = os.path.join(MODEL_DIR, 'dinov3')\n",
    "if not os.path.exists(dinov3_dir):\n",
    "    !git clone https://github.com/facebookresearch/dinov3.git \"{dinov3_dir}\"\n",
    "    print(\"DINOv3 repository cloned successfully\")\n",
    "else:\n",
    "    print(\"DINOv3 repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if dinov3_dir not in sys.path:\n",
    "    sys.path.insert(0, dinov3_dir)\n",
    "\n",
    "print(f\"DINOv3 path: {dinov3_dir}\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: Request access and download DINOv3 checkpoints\")\n",
    "print(\"   Follow instructions in the DINOv3 repository README\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "148bead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 checkpoint directory: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/dinov3\n",
      "Expected checkpoint path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/dinov3/dinov3_vitb14_pretrain.pth\n",
      "\n",
      "After obtaining access, download the ViT-B checkpoint to this location\n"
     ]
    }
   ],
   "source": [
    "# DINOv3 checkpoint configuration\n",
    "dinov3_checkpoint_dir = os.path.join(CHECKPOINT_DIR, 'dinov3')\n",
    "os.makedirs(dinov3_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Expected checkpoint path for ViT-B\n",
    "dinov3_checkpoint_path = os.path.join(dinov3_checkpoint_dir, 'dinov3_vitb14_pretrain.pth')\n",
    "\n",
    "print(f\"DINOv3 checkpoint directory: {dinov3_checkpoint_dir}\")\n",
    "print(f\"Expected checkpoint path: {dinov3_checkpoint_path}\")\n",
    "print(\"\\nAfter obtaining access, download the ViT-B checkpoint to this location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fdf4b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DINOv3 loader defined (run after downloading checkpoint)\n"
     ]
    }
   ],
   "source": [
    "# Load DINOv3 model (after checkpoint is downloaded)\n",
    "def load_dinov3_model(checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load DINOv3 model from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the downloaded checkpoint\n",
    "        device: Device to load model on\n",
    "        \n",
    "    Returns:\n",
    "        Loaded DINOv3 model\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚úó Checkpoint not found: {checkpoint_path}\")\n",
    "        print(\"  Please download the DINOv3 checkpoint after requesting access\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # This will be updated once checkpoint structure is known\n",
    "        # Placeholder for actual loading code\n",
    "        print(f\"‚úì Loading DINOv3 from: {checkpoint_path}\")\n",
    "        \n",
    "        # Import DINOv3 modules (adjust based on actual repo structure)\n",
    "        # from dinov3.models import build_model\n",
    "        # model = build_model(checkpoint_path)\n",
    "        # model = model.to(device)\n",
    "        # model.eval()\n",
    "        \n",
    "        print(\"‚úì DINOv3 model loaded successfully\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        return None  # Will return actual model after implementation\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading DINOv3: {e}\")\n",
    "        return None\n",
    "\n",
    "# Note: Uncomment and run after downloading checkpoint\n",
    "# dinov3_model = load_dinov3_model(dinov3_checkpoint_path, device=device)\n",
    "print(\"DINOv3 loader defined (run after downloading checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb4c1f",
   "metadata": {},
   "source": [
    "## 5. SAM (Segment Anything) Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/segment-anything  \n",
    "**Model**: ViT-B (Base version) - RECOMMENDED  \n",
    "**Optional**: Can experiment with ViT-L (Large) or ViT-H (Huge) for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c57153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-req-build-gt5zw5tq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-req-build-gt5zw5tq\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Installing build dependencies ... \u001b[?25l  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25done\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install SAM\n",
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561f272",
   "metadata": {},
   "source": [
    "### Troubleshooting: torch/torchvision Version Mismatch\n",
    "\n",
    "If you encounter `RuntimeError: operator torchvision::nms does not exist` when importing SAM, this means your `torch` and `torchvision` versions are mismatched. The compiled C++ operators in torchvision don't match your PyTorch installation.\n",
    "\n",
    "**Steps to fix:**\n",
    "1. Run the diagnostic cell below to check versions\n",
    "2. If mismatch detected, run the fix cell to reinstall compatible versions\n",
    "3. Restart the kernel\n",
    "4. Re-run the diagnostic to verify\n",
    "\n",
    "The fix uses conda to ensure binary compatibility between torch and torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bec827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aml_project/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aml_project/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TORCH/TORCHVISION VERSION CHECK\n",
      "============================================================\n",
      "torch version: 2.5.1\n",
      "torchvision version: 0.20.1\n",
      "CUDA available: False\n",
      "\n",
      "------------------------------------------------------------\n",
      "Checking torchvision.ops.nms availability...\n",
      "‚úì torchvision.ops imported successfully\n",
      "  has nms attribute: True\n",
      "  ‚úì nms operator is available\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check torch/torchvision versions\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TORCH/TORCHVISION VERSION CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Checking torchvision.ops.nms availability...\")\n",
    "try:\n",
    "    import torchvision.ops as ops\n",
    "    print(f\"‚úì torchvision.ops imported successfully\")\n",
    "    print(f\"  has nms attribute: {hasattr(ops, 'nms')}\")\n",
    "    if hasattr(ops, 'nms'):\n",
    "        print(f\"  ‚úì nms operator is available\")\n",
    "    else:\n",
    "        print(f\"  ‚úó nms operator NOT found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {e}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54ad3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIXING TORCH/TORCHVISION MISMATCH\n",
      "============================================================\n",
      "üì± Detected macOS - Reinstalling compatible versions via conda\n",
      "\n",
      "Executing: conda install pytorch torchvision -c pytorch -y\n",
      "------------------------------------------------------------\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "Channels:\n",
      " - pytorch\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): - Channels:\n",
      " - pytorch\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json)\\ done\n",
      "Solving environment: done\n",
      "Solving environment\\ done\n",
      "done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3/envs/aml_project\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  pytorch            pytorch/osx-arm64::pytorch-2.5.1-py3.11_0 \n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3/envs/aml_project\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  pytorch            pytorch/osx-arm64::pytorch-2.5.1-py3.11_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: / \n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Verifying transactiondone\n",
      "Executing transaction: / done\n",
      "Executing transaction: done\n",
      "done\n",
      "\n",
      "============================================================\n",
      "‚úì Reinstallation complete!\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT: Restart the kernel to use the new installation!\n",
      "   In Jupyter/VSCode: Kernel ‚Üí Restart Kernel\n",
      "\n",
      "Then re-run the diagnostic cell above to verify the fix.\n",
      "\n",
      "============================================================\n",
      "‚úì Reinstallation complete!\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT: Restart the kernel to use the new installation!\n",
      "   In Jupyter/VSCode: Kernel ‚Üí Restart Kernel\n",
      "\n",
      "Then re-run the diagnostic cell above to verify the fix.\n"
     ]
    }
   ],
   "source": [
    "# FIX: Reinstall matching torch/torchvision versions\n",
    "# This uses conda to ensure binary compatibility between torch and torchvision\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FIXING TORCH/TORCHVISION MISMATCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    print(\"üì± Detected macOS - Reinstalling compatible versions via conda\")\n",
    "    print(\"\\nExecuting: conda install pytorch torchvision -c pytorch -y\")\n",
    "    print(\"-\"*60)\n",
    "    !conda install pytorch torchvision -c pytorch -y\n",
    "    \n",
    "elif 'google.colab' in sys.modules:  # Google Colab\n",
    "    print(\"‚òÅÔ∏è Detected Colab - Reinstalling via pip\")\n",
    "    !pip uninstall -y torch torchvision\n",
    "    !pip install torch torchvision --no-cache-dir\n",
    "    \n",
    "else:  # Linux (possibly with CUDA)\n",
    "    print(\"üñ•Ô∏è Detected Linux - Reinstalling compatible versions via conda\")\n",
    "    print(\"\\nIf you have CUDA, this will install the CUDA-enabled version.\")\n",
    "    print(\"Executing: conda install pytorch torchvision pytorch-cuda -c pytorch -c nvidia -y\")\n",
    "    print(\"-\"*60)\n",
    "    !conda install pytorch torchvision pytorch-cuda -c pytorch -c nvidia -y\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Reinstallation complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Restart the kernel to use the new installation!\")\n",
    "print(\"   In Jupyter/VSCode: Kernel ‚Üí Restart Kernel\")\n",
    "print(\"\\nThen re-run the diagnostic cell above to verify the fix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef792e",
   "metadata": {},
   "source": [
    "**‚ö†Ô∏è After running the fix above:**\n",
    "- **Restart the kernel** (Kernel ‚Üí Restart Kernel)\n",
    "- Re-run the diagnostic cell to verify the fix worked\n",
    "- Then proceed to download SAM checkpoints below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a07087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Checkpoint already exists: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam/sam_vit_b_01ec64.pth\n",
      "\n",
      "SAM checkpoint directory: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam\n",
      "Available models: ['vit_b', 'vit_l', 'vit_h']\n"
     ]
    }
   ],
   "source": [
    "# Download SAM checkpoints\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "sam_checkpoint_dir = os.path.join(CHECKPOINT_DIR, 'sam')\n",
    "os.makedirs(sam_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# SAM model checkpoints\n",
    "SAM_MODELS = {\n",
    "    'vit_b': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth',\n",
    "        'filename': 'sam_vit_b_01ec64.pth',\n",
    "        'description': 'ViT-B (Base) - RECOMMENDED'\n",
    "    },\n",
    "    'vit_l': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n",
    "        'filename': 'sam_vit_l_0b3195.pth',\n",
    "        'description': 'ViT-L (Large) - Optional comparison'\n",
    "    },\n",
    "    'vit_h': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n",
    "        'filename': 'sam_vit_h_4b8939.pth',\n",
    "        'description': 'ViT-H (Huge) - Optional comparison'\n",
    "    }\n",
    "}\n",
    "\n",
    "def download_sam_checkpoint(model_type='vit_b'):\n",
    "    \"\"\"Download SAM checkpoint if not already present.\"\"\"\n",
    "    if model_type not in SAM_MODELS:\n",
    "        print(f\"Invalid model type. Choose from: {list(SAM_MODELS.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    model_info = SAM_MODELS[model_type]\n",
    "    checkpoint_path = os.path.join(sam_checkpoint_dir, model_info['filename'])\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"‚úì Checkpoint already exists: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    print(f\"Downloading {model_info['description']}...\")\n",
    "    print(f\"URL: {model_info['url']}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(model_info['url'], checkpoint_path)\n",
    "        print(f\"‚úì Downloaded successfully: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error downloading: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download ViT-B checkpoint (recommended)\n",
    "sam_checkpoint_path = download_sam_checkpoint('vit_b')\n",
    "\n",
    "print(f\"\\nSAM checkpoint directory: {sam_checkpoint_dir}\")\n",
    "print(\"Available models:\", list(SAM_MODELS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50abda56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SAM model loaded successfully\n",
      "  - Model type: vit_b\n",
      "  - Device: mps\n",
      "  - Checkpoint: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam/sam_vit_b_01ec64.pth\n"
     ]
    }
   ],
   "source": [
    "# Load SAM model\n",
    "try:\n",
    "    from segment_anything import sam_model_registry, SamPredictor\n",
    "    SAM_IMPORT_SUCCESS = True\n",
    "except RuntimeError as e:\n",
    "    if 'torchvision::nms does not exist' in str(e):\n",
    "        print(\"=\"*60)\n",
    "        print(\"‚ö†Ô∏è  SAM IMPORT ERROR: torch/torchvision mismatch detected\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Error: operator torchvision::nms does not exist\")\n",
    "        print(\"\\nThis means your torch and torchvision versions are incompatible.\")\n",
    "        print(\"\\nüìã TO FIX:\")\n",
    "        print(\"   1. Scroll up to find the diagnostic cell (after 'Install SAM')\")\n",
    "        print(\"   2. Run the fix cell to reinstall matching versions\")\n",
    "        print(\"   3. Restart the kernel (Kernel ‚Üí Restart Kernel)\")\n",
    "        print(\"   4. Re-run this cell\")\n",
    "        print(\"=\"*60)\n",
    "    SAM_IMPORT_SUCCESS = False\n",
    "    sam_model_registry = None\n",
    "    SamPredictor = None\n",
    "\n",
    "def load_sam_model(checkpoint_path, model_type='vit_b', device='cuda'):\n",
    "    \"\"\"\n",
    "    Load SAM model.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint\n",
    "        model_type: 'vit_b', 'vit_l', or 'vit_h'\n",
    "        device: Device to load on\n",
    "        \n",
    "    Returns:\n",
    "        SAM model and predictor\n",
    "    \"\"\"\n",
    "    if not SAM_IMPORT_SUCCESS:\n",
    "        print(\"‚úó Cannot load SAM: import failed (see error above)\")\n",
    "        return None, None\n",
    "        \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚úó Checkpoint not found: {checkpoint_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "        sam = sam.to(device)\n",
    "        sam.eval()\n",
    "        \n",
    "        # Create predictor for easier inference\n",
    "        predictor = SamPredictor(sam)\n",
    "        \n",
    "        print(f\"‚úì SAM model loaded successfully\")\n",
    "        print(f\"  - Model type: {model_type}\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        print(f\"  - Checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        return sam, predictor\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading SAM: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load SAM ViT-B\n",
    "if SAM_IMPORT_SUCCESS and sam_checkpoint_path:\n",
    "    sam_model, sam_predictor = load_sam_model(sam_checkpoint_path, 'vit_b', device=device)\n",
    "elif not SAM_IMPORT_SUCCESS:\n",
    "    print(\"‚ö†Ô∏è  Skipping SAM model loading due to import error\")\n",
    "    sam_model, sam_predictor = None, None\n",
    "else:\n",
    "    print(\"SAM checkpoint not available yet\")\n",
    "    sam_model, sam_predictor = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c263c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM feature extraction utility defined\n"
     ]
    }
   ],
   "source": [
    "# SAM feature extraction utility\n",
    "def extract_sam_features(sam_model, image):\n",
    "    \"\"\"\n",
    "    Extract features from SAM image encoder.\n",
    "    \n",
    "    Args:\n",
    "        sam_model: SAM model\n",
    "        image: PIL Image or numpy array (H, W, 3) in RGB format\n",
    "        \n",
    "    Returns:\n",
    "        Image embeddings from SAM encoder\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from segment_anything.utils.transforms import ResizeLongestSide\n",
    "    \n",
    "    # Convert PIL to numpy if needed\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    # SAM preprocessing\n",
    "    transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "    input_image = transform.apply_image(image)\n",
    "    input_image_torch = torch.as_tensor(input_image, device=sam_model.device)\n",
    "    input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        image_embedding = sam_model.image_encoder(input_image_torch)\n",
    "    \n",
    "    return image_embedding\n",
    "\n",
    "print(\"SAM feature extraction utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6eecb1",
   "metadata": {},
   "source": [
    "## 6. Utility Functions & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb0b4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjectConfig:\n",
      "  Project Root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject\n",
      "  Data Root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data\n",
      "  Device: mps\n",
      "  Dataset: SD4Match\n",
      "  Backbones: ['dinov2', 'dinov3', 'sam']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration class for the project\n",
    "class ProjectConfig:\n",
    "    \"\"\"Central configuration for the semantic correspondence project.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.project_root = PROJECT_ROOT\n",
    "        self.data_root = DATA_ROOT\n",
    "        self.checkpoint_dir = CHECKPOINT_DIR\n",
    "        self.output_dir = OUTPUT_DIR\n",
    "        self.model_dir = MODEL_DIR\n",
    "        \n",
    "        # Dataset\n",
    "        self.dataset_name = 'SD4Match'\n",
    "        self.splits = ['trn', 'val', 'test']\n",
    "        \n",
    "        # Models\n",
    "        self.backbones = {\n",
    "            'dinov2': 'dinov2_vitb14',\n",
    "            'dinov3': 'dinov3_vitb14',\n",
    "            'sam': 'vit_b'\n",
    "        }\n",
    "        \n",
    "        # Device\n",
    "        self.device = device\n",
    "        \n",
    "        # Training (to be filled in later phases)\n",
    "        self.batch_size = 16\n",
    "        self.num_epochs = 100\n",
    "        self.learning_rate = 1e-4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"ProjectConfig:\n",
    "  Project Root: {self.project_root}\n",
    "  Data Root: {self.data_root}\n",
    "  Device: {self.device}\n",
    "  Dataset: {self.dataset_name}\n",
    "  Backbones: {list(self.backbones.keys())}\n",
    "\"\"\"\n",
    "\n",
    "config = ProjectConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2290c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization and checkpoint utilities defined\n"
     ]
    }
   ],
   "source": [
    "# Visualization utilities\n",
    "def visualize_correspondence(img1, img2, pts1, pts2, matches=None, figsize=(15, 7)):\n",
    "    \"\"\"\n",
    "    Visualize correspondence between two images.\n",
    "    \n",
    "    Args:\n",
    "        img1, img2: Images (PIL or numpy)\n",
    "        pts1, pts2: Keypoint coordinates [(x, y), ...]\n",
    "        matches: Optional list of match indices [(idx1, idx2), ...]\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Display images\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('Image 1')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title('Image 2')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Plot keypoints\n",
    "    if pts1 is not None and len(pts1) > 0:\n",
    "        pts1 = np.array(pts1)\n",
    "        ax1.scatter(pts1[:, 0], pts1[:, 1], c='red', s=50, marker='x')\n",
    "    \n",
    "    if pts2 is not None and len(pts2) > 0:\n",
    "        pts2 = np.array(pts2)\n",
    "        ax2.scatter(pts2[:, 0], pts2[:, 1], c='red', s=50, marker='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch, path, **kwargs):\n",
    "    \"\"\"Save model checkpoint with metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
    "        **kwargs\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"‚úì Checkpoint saved: {path}\")\n",
    "\n",
    "def load_model_checkpoint(model, path, optimizer=None, device='cuda'):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚úó Checkpoint not found: {path}\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer and checkpoint.get('optimizer_state_dict'):\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    print(f\"‚úì Checkpoint loaded from epoch {epoch}\")\n",
    "    return checkpoint\n",
    "\n",
    "print(\"Visualization and checkpoint utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea2edc",
   "metadata": {},
   "source": [
    "## 7. Model Summary & Testing\n",
    "\n",
    "Quick tests to verify all models are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6b9c284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL SETUP SUMMARY\n",
      "============================================================\n",
      "‚úì DINOv2 (ViT-B): Loaded\n",
      "‚ö† DINOv3 (ViT-B): Not loaded yet\n",
      "‚úì SAM (ViT-B): Loaded\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS\n",
      "============================================================\n",
      "1. DINOv3: Request access and download checkpoint\n",
      "2. SD4Match: Download dataset to /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "3. Verify all models work with test images\n",
      "4. Ready for team to implement correspondence methods\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary of loaded models\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SETUP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_status = {\n",
    "    'DINOv2 (ViT-B)': dinov2_model is not None if 'dinov2_model' in locals() else False,\n",
    "    'DINOv3 (ViT-B)': False,  # To be loaded after checkpoint download\n",
    "    'SAM (ViT-B)': (sam_model is not None) if 'sam_model' in locals() else False,\n",
    "}\n",
    "\n",
    "for model_name, status in models_status.items():\n",
    "    status_symbol = \"‚úì\" if status else \"‚ö†\"\n",
    "    status_text = \"Loaded\" if status else \"Not loaded yet\"\n",
    "    print(f\"{status_symbol} {model_name}: {status_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. DINOv3: Request access and download checkpoint\")\n",
    "print(\"2. SD4Match: Download dataset to\", sd4match_data_dir)\n",
    "print(\"3. Verify all models work with test images\")\n",
    "print(\"4. Ready for team to implement correspondence methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f3164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a dummy image (optional)\n",
    "def test_model_inference():\n",
    "    \"\"\"Quick test to verify models can process images.\"\"\"\n",
    "    # Create a dummy image\n",
    "    dummy_image = Image.new('RGB', (224, 224), color='red')\n",
    "    \n",
    "    print(\"Testing model inference with dummy image...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test DINOv2\n",
    "    if 'dinov2_model' in locals() and dinov2_model is not None:\n",
    "        try:\n",
    "            features = extract_dinov2_features(dinov2_model, dummy_image)\n",
    "            print(f\"‚úì DINOv2: CLS token shape = {features['cls_token'].shape}\")\n",
    "            print(f\"           Patch tokens shape = {features['patch_tokens'].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó DINOv2 error: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö† DINOv2: Not loaded\")\n",
    "    \n",
    "    # Test SAM\n",
    "    if 'sam_model' in locals() and sam_model is not None:\n",
    "        try:\n",
    "            embedding = extract_sam_features(sam_model, dummy_image)\n",
    "            print(f\"‚úì SAM: Embedding shape = {embedding.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó SAM error: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö† SAM: Not loaded\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(\"Model inference test complete\")\n",
    "\n",
    "# Uncomment to run test\n",
    "# test_model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87837609",
   "metadata": {},
   "source": [
    "## 8. Additional Resources & Notes\n",
    "\n",
    "### Window Soft Argmax (GeoAware-SC)\n",
    "For prediction refinement in later phases:\n",
    "- **Repository**: https://github.com/Junyi42/geoaware-sc\n",
    "- This will be used for refining correspondence predictions\n",
    "\n",
    "### Professor's Key Recommendations Summary:\n",
    "1. **Backbone Selection**: Use Base (ViT-B) versions for all three backbones\n",
    "2. **Model Access**: \n",
    "   - DINOv2: Use official repo, not just Hugging Face\n",
    "   - DINOv3: Request access to checkpoints\n",
    "   - SAM: ViT-B recommended, can compare with L/H if compute allows\n",
    "3. **Dataset Splits**:\n",
    "   - Train on `trn` split\n",
    "   - Validate on `val` split for model selection\n",
    "   - **Only report final results on `test` split**\n",
    "4. **Backbone Size Trade-offs**:\n",
    "   - Larger backbones (Small ‚Üí Base ‚Üí Large) generally improve performance\n",
    "   - But gains are not always consistent across tasks\n",
    "   - Increased size = higher compute/memory/time costs\n",
    "\n",
    "### For Team Members (Later Phases):\n",
    "- All infrastructure is ready for implementing correspondence methods\n",
    "- Models are loaded and ready to extract features\n",
    "- Utilities for visualization and checkpointing are provided\n",
    "- Follow the professor's evaluation protocol strictly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8918973",
   "metadata": {},
   "source": [
    "## 9. Dataset Loaders\n",
    "\n",
    "This section defines dataset classes for loading correspondence benchmarks:\n",
    "- **CorrespondenceDataset**: Base class for all datasets\n",
    "- **PFPascalDataset**: PF-Pascal dataset with CSV-based annotations\n",
    "- **SPairDataset**: SPair-71k dataset with JSON-based annotations\n",
    "\n",
    "Each dataset returns:\n",
    "- Source and target images\n",
    "- Source and target keypoints\n",
    "- Category information\n",
    "- Bounding boxes (for PCK normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6ce26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dataset base class and utilities\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorrespondenceDataset\u001b[39;00m(Dataset):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Dataset base class and utilities\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CorrespondenceDataset(Dataset):\n",
    "    \"\"\"Base class for semantic correspondence datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Root directory of the dataset\n",
    "            split: 'trn', 'val', or 'test'\n",
    "            transform: Optional transforms to apply to images\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.pairs = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def load_image(self, path):\n",
    "        \"\"\"Load and optionally transform an image.\"\"\"\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        raise NotImplementedError(\"Subclasses must implement __getitem__\")\n",
    "\n",
    "\n",
    "class PFPascalDataset(CorrespondenceDataset):\n",
    "    \"\"\"PF-Pascal dataset loader.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        super().__init__(root_dir, split, transform)\n",
    "        self.load_annotations()\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        \"\"\"Load image pairs and keypoint annotations.\"\"\"\n",
    "        anno_file = self.root_dir / 'pf-pascal_image_pairs' / f'{self.split}_pairs.csv'\n",
    "        \n",
    "        if not anno_file.exists():\n",
    "            print(f\"‚ö†Ô∏è  Annotation file not found: {anno_file}\")\n",
    "            print(\"   Make sure you've downloaded the dataset\")\n",
    "            return\n",
    "        \n",
    "        # Load pairs\n",
    "        df = pd.read_csv(anno_file)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            pair = {\n",
    "                'source_img': self.root_dir / 'PF-dataset-PASCAL' / row['source_image'],\n",
    "                'target_img': self.root_dir / 'PF-dataset-PASCAL' / row['target_image'],\n",
    "                'source_kps': self._parse_keypoints(row['source_keypoints']),\n",
    "                'target_kps': self._parse_keypoints(row['target_keypoints']),\n",
    "                'category': row.get('category', 'unknown')\n",
    "            }\n",
    "            self.pairs.append(pair)\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(self.pairs)} pairs from PF-Pascal {self.split} split\")\n",
    "    \n",
    "    def _parse_keypoints(self, kps_str):\n",
    "        \"\"\"Parse keypoint string to numpy array.\"\"\"\n",
    "        # Format: \"x1,y1;x2,y2;...\" or similar\n",
    "        if pd.isna(kps_str) or kps_str == '':\n",
    "            return np.array([])\n",
    "        \n",
    "        kps = []\n",
    "        for kp in str(kps_str).split(';'):\n",
    "            if kp.strip():\n",
    "                coords = [float(x) for x in kp.split(',')]\n",
    "                kps.append(coords)\n",
    "        return np.array(kps)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        source_img = self.load_image(pair['source_img'])\n",
    "        target_img = self.load_image(pair['target_img'])\n",
    "        \n",
    "        return {\n",
    "            'source_image': source_img,\n",
    "            'target_image': target_img,\n",
    "            'source_keypoints': pair['source_kps'],\n",
    "            'target_keypoints': pair['target_kps'],\n",
    "            'category': pair['category']\n",
    "        }\n",
    "\n",
    "\n",
    "class SPairDataset(CorrespondenceDataset):\n",
    "    \"\"\"SPair-71k dataset loader.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='test', transform=None):\n",
    "        super().__init__(root_dir, split, transform)\n",
    "        self.load_annotations()\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        \"\"\"Load annotations from SPair-71k.\"\"\"\n",
    "        # SPair uses different split names\n",
    "        split_map = {'trn': 'trn', 'val': 'val', 'test': 'test'}\n",
    "        split_name = split_map.get(self.split, 'test')\n",
    "        \n",
    "        anno_dir = self.root_dir / 'SPair-71k' / 'PairAnnotation' / split_name\n",
    "        \n",
    "        if not anno_dir.exists():\n",
    "            print(f\"‚ö†Ô∏è  Annotation directory not found: {anno_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Load all annotation files\n",
    "        for anno_file in sorted(anno_dir.glob('*.json')):\n",
    "            with open(anno_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            pair = {\n",
    "                'source_img': self.root_dir / 'SPair-71k' / 'ImageAnnotation' / data['src_imname'],\n",
    "                'target_img': self.root_dir / 'SPair-71k' / 'ImageAnnotation' / data['trg_imname'],\n",
    "                'source_kps': np.array(data['src_kps']).T,  # [N, 2]\n",
    "                'target_kps': np.array(data['trg_kps']).T,  # [N, 2]\n",
    "                'category': data.get('category', 'unknown'),\n",
    "                'source_bbox': np.array(data.get('src_bndbox', [])),\n",
    "                'target_bbox': np.array(data.get('trg_bndbox', []))\n",
    "            }\n",
    "            self.pairs.append(pair)\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(self.pairs)} pairs from SPair-71k {self.split} split\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        source_img = self.load_image(pair['source_img'])\n",
    "        target_img = self.load_image(pair['target_img'])\n",
    "        \n",
    "        return {\n",
    "            'source_image': source_img,\n",
    "            'target_image': target_img,\n",
    "            'source_keypoints': pair['source_kps'],\n",
    "            'target_keypoints': pair['target_kps'],\n",
    "            'category': pair['category'],\n",
    "            'source_bbox': pair.get('source_bbox'),\n",
    "            'target_bbox': pair.get('target_bbox')\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úì Dataset classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe26d9c",
   "metadata": {},
   "source": [
    "## 10. Dense Feature Extraction\n",
    "\n",
    "The `DenseFeatureExtractor` class extracts spatial feature maps from vision backbones:\n",
    "- Supports **DINOv2** (ViT-B/14: 16√ó16 patches for 224√ó224 input)\n",
    "- Supports **SAM** (ViT-B: 64√ó64 features for 1024√ó1024 input)\n",
    "- Handles coordinate mapping between original image space and feature space\n",
    "- Extracts features at specific keypoint locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c3198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseFeatureExtractor:\n",
    "    \"\"\"Extract dense features from images for correspondence.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='dinov2', model=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone: 'dinov2' or 'sam'\n",
    "            model: Pre-loaded model (optional)\n",
    "            device: Device to run on\n",
    "        \"\"\"\n",
    "        self.backbone = backbone\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        \n",
    "        if backbone == 'dinov2':\n",
    "            self.patch_size = 14\n",
    "            self.feat_dim = 768  # ViT-B feature dimension\n",
    "        elif backbone == 'sam':\n",
    "            self.patch_size = 16  # SAM uses 16x16 patches\n",
    "            self.feat_dim = 256  # SAM image encoder output\n",
    "    \n",
    "    def extract_features(self, image, return_numpy=True):\n",
    "        \"\"\"\n",
    "        Extract dense features from an image.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or tensor\n",
    "            return_numpy: Return numpy array instead of tensor\n",
    "            \n",
    "        Returns:\n",
    "            features: Dense feature map [H', W', D]\n",
    "            Original image size for coordinate mapping\n",
    "        \"\"\"\n",
    "        if self.backbone == 'dinov2':\n",
    "            return self._extract_dinov2(image, return_numpy)\n",
    "        elif self.backbone == 'sam':\n",
    "            return self._extract_sam(image, return_numpy)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {self.backbone}\")\n",
    "    \n",
    "    def _extract_dinov2(self, image, return_numpy=True):\n",
    "        \"\"\"Extract features using DINOv2.\"\"\"\n",
    "        from torchvision import transforms\n",
    "        \n",
    "        # Get original size\n",
    "        if isinstance(image, Image.Image):\n",
    "            orig_size = image.size  # (W, H)\n",
    "        else:\n",
    "            orig_size = (image.shape[2], image.shape[1])\n",
    "        \n",
    "        # Prepare image (224x224 for DINOv2)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        if isinstance(image, Image.Image):\n",
    "            img_tensor = transform(image).unsqueeze(0)\n",
    "        else:\n",
    "            img_tensor = image.unsqueeze(0) if image.dim() == 3 else image\n",
    "        \n",
    "        img_tensor = img_tensor.to(self.device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = self.model.forward_features(img_tensor)\n",
    "            patch_tokens = features['x_norm_patchtokens']  # [1, N, D]\n",
    "        \n",
    "        # Reshape to spatial grid\n",
    "        # DINOv2 ViT-B/14 produces 16x16 = 256 patches for 224x224 image\n",
    "        h = w = int(np.sqrt(patch_tokens.shape[1]))\n",
    "        feature_map = patch_tokens.reshape(1, h, w, -1)[0]  # [H, W, D]\n",
    "        \n",
    "        if return_numpy:\n",
    "            feature_map = feature_map.cpu().numpy()\n",
    "        \n",
    "        return {\n",
    "            'features': feature_map,\n",
    "            'feature_size': (h, w),\n",
    "            'original_size': orig_size,\n",
    "            'processed_size': (224, 224)\n",
    "        }\n",
    "    \n",
    "    def _extract_sam(self, image, return_numpy=True):\n",
    "        \"\"\"Extract features using SAM.\"\"\"\n",
    "        import numpy as np\n",
    "        from segment_anything.utils.transforms import ResizeLongestSide\n",
    "        \n",
    "        # Get original size\n",
    "        if isinstance(image, Image.Image):\n",
    "            image_np = np.array(image)\n",
    "            orig_size = image.size  # (W, H)\n",
    "        else:\n",
    "            image_np = image\n",
    "            orig_size = (image_np.shape[1], image_np.shape[0])\n",
    "        \n",
    "        # SAM preprocessing\n",
    "        transform = ResizeLongestSide(self.model.image_encoder.img_size)\n",
    "        input_image = transform.apply_image(image_np)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=self.device)\n",
    "        input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.model.image_encoder(input_image_torch)\n",
    "        \n",
    "        # SAM outputs [1, 256, 64, 64] for 1024x1024 input\n",
    "        feature_map = image_embedding[0].permute(1, 2, 0)  # [H, W, D]\n",
    "        \n",
    "        if return_numpy:\n",
    "            feature_map = feature_map.cpu().numpy()\n",
    "        \n",
    "        h, w = image_embedding.shape[2], image_embedding.shape[3]\n",
    "        \n",
    "        return {\n",
    "            'features': feature_map,\n",
    "            'feature_size': (h, w),\n",
    "            'original_size': orig_size,\n",
    "            'processed_size': input_image.shape[:2]\n",
    "        }\n",
    "    \n",
    "    def extract_features_at_keypoints(self, image, keypoints):\n",
    "        \"\"\"\n",
    "        Extract features at specific keypoint locations.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            keypoints: Keypoint coordinates [N, 2] in original image space\n",
    "            \n",
    "        Returns:\n",
    "            features: Feature vectors at keypoints [N, D]\n",
    "        \"\"\"\n",
    "        feat_dict = self.extract_features(image, return_numpy=False)\n",
    "        features = feat_dict['features']  # [H, W, D]\n",
    "        feat_h, feat_w = feat_dict['feature_size']\n",
    "        orig_w, orig_h = feat_dict['original_size']\n",
    "        \n",
    "        # Map keypoints from original space to feature space\n",
    "        scale_x = feat_w / orig_w\n",
    "        scale_y = feat_h / orig_h\n",
    "        \n",
    "        feat_kps = keypoints.copy()\n",
    "        feat_kps[:, 0] = feat_kps[:, 0] * scale_x\n",
    "        feat_kps[:, 1] = feat_kps[:, 1] * scale_y\n",
    "        \n",
    "        # Clip to valid range\n",
    "        feat_kps[:, 0] = np.clip(feat_kps[:, 0], 0, feat_w - 1)\n",
    "        feat_kps[:, 1] = np.clip(feat_kps[:, 1], 0, feat_h - 1)\n",
    "        \n",
    "        # Round to integer indices\n",
    "        feat_kps = feat_kps.astype(int)\n",
    "        \n",
    "        # Extract features\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            kp_features = features[feat_kps[:, 1], feat_kps[:, 0], :]\n",
    "            return kp_features.cpu().numpy()\n",
    "        else:\n",
    "            return features[feat_kps[:, 1], feat_kps[:, 0], :]\n",
    "\n",
    "print(\"‚úì Dense feature extractor defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876c3fa",
   "metadata": {},
   "source": [
    "## 11. Correspondence Matching\n",
    "\n",
    "The `CorrespondenceMatcher` class finds correspondences between feature maps:\n",
    "- **Cosine similarity** with L2 normalization\n",
    "- **Nearest neighbor** matching\n",
    "- **Mutual nearest neighbor** constraint (optional)\n",
    "- **Lowe's ratio test** (optional)\n",
    "\n",
    "Matches source keypoints to target image locations based on feature similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7aa313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrespondenceMatcher:\n",
    "    \"\"\"Match correspondences between two sets of features.\"\"\"\n",
    "    \n",
    "    def __init__(self, method='nn', mutual=False, ratio_test=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            method: 'nn' (nearest neighbor) or 'mutual_nn'\n",
    "            mutual: Use mutual nearest neighbor constraint\n",
    "            ratio_test: Lowe's ratio test threshold (None to disable)\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.mutual = mutual\n",
    "        self.ratio_test = ratio_test\n",
    "    \n",
    "    def match(self, features_src, features_tgt):\n",
    "        \"\"\"\n",
    "        Find correspondences between source and target features.\n",
    "        \n",
    "        Args:\n",
    "            features_src: Source features [N, D] or [H, W, D]\n",
    "            features_tgt: Target features [M, D] or [H', W', D]\n",
    "            \n",
    "        Returns:\n",
    "            matches: Matched indices [(src_idx, tgt_idx), ...]\n",
    "            scores: Match confidence scores\n",
    "        \"\"\"\n",
    "        # Flatten if spatial\n",
    "        if features_src.ndim == 3:\n",
    "            h_src, w_src, d = features_src.shape\n",
    "            features_src_flat = features_src.reshape(-1, d)\n",
    "        else:\n",
    "            features_src_flat = features_src\n",
    "            h_src = w_src = None\n",
    "        \n",
    "        if features_tgt.ndim == 3:\n",
    "            h_tgt, w_tgt, d = features_tgt.shape\n",
    "            features_tgt_flat = features_tgt.reshape(-1, d)\n",
    "        else:\n",
    "            features_tgt_flat = features_tgt\n",
    "            h_tgt = w_tgt = None\n",
    "        \n",
    "        # Compute distance matrix\n",
    "        # Using cosine similarity (dot product after L2 normalization)\n",
    "        features_src_norm = features_src_flat / (np.linalg.norm(features_src_flat, axis=1, keepdims=True) + 1e-8)\n",
    "        features_tgt_norm = features_tgt_flat / (np.linalg.norm(features_tgt_flat, axis=1, keepdims=True) + 1e-8)\n",
    "        \n",
    "        similarity = features_src_norm @ features_tgt_norm.T  # [N, M]\n",
    "        \n",
    "        # Nearest neighbor matching\n",
    "        src_to_tgt = np.argmax(similarity, axis=1)  # [N]\n",
    "        src_scores = np.max(similarity, axis=1)  # [N]\n",
    "        \n",
    "        matches = []\n",
    "        scores = []\n",
    "        \n",
    "        if self.mutual:\n",
    "            # Mutual nearest neighbors\n",
    "            tgt_to_src = np.argmax(similarity, axis=0)  # [M]\n",
    "            \n",
    "            for src_idx in range(len(features_src_flat)):\n",
    "                tgt_idx = src_to_tgt[src_idx]\n",
    "                if tgt_to_src[tgt_idx] == src_idx:  # Mutual match\n",
    "                    matches.append((src_idx, tgt_idx))\n",
    "                    scores.append(src_scores[src_idx])\n",
    "        else:\n",
    "            # All nearest neighbors\n",
    "            for src_idx in range(len(features_src_flat)):\n",
    "                tgt_idx = src_to_tgt[src_idx]\n",
    "                \n",
    "                # Optional ratio test\n",
    "                if self.ratio_test is not None:\n",
    "                    sorted_sim = np.sort(similarity[src_idx])[::-1]\n",
    "                    if len(sorted_sim) > 1:\n",
    "                        ratio = sorted_sim[0] / (sorted_sim[1] + 1e-8)\n",
    "                        if ratio < self.ratio_test:\n",
    "                            continue\n",
    "                \n",
    "                matches.append((src_idx, tgt_idx))\n",
    "                scores.append(src_scores[src_idx])\n",
    "        \n",
    "        return np.array(matches), np.array(scores)\n",
    "    \n",
    "    def match_keypoints(self, src_image, tgt_image, src_kps, feature_extractor):\n",
    "        \"\"\"\n",
    "        Match source keypoints to target image.\n",
    "        \n",
    "        Args:\n",
    "            src_image: Source PIL Image\n",
    "            tgt_image: Target PIL Image\n",
    "            src_kps: Source keypoints [N, 2]\n",
    "            feature_extractor: DenseFeatureExtractor instance\n",
    "            \n",
    "        Returns:\n",
    "            predicted_kps: Predicted target keypoints [N, 2]\n",
    "            confidence: Match confidence scores [N]\n",
    "        \"\"\"\n",
    "        # Extract dense features\n",
    "        src_feat_dict = feature_extractor.extract_features(src_image, return_numpy=True)\n",
    "        tgt_feat_dict = feature_extractor.extract_features(tgt_image, return_numpy=True)\n",
    "        \n",
    "        src_features = src_feat_dict['features']  # [H, W, D]\n",
    "        tgt_features = tgt_feat_dict['features']  # [H', W', D]\n",
    "        \n",
    "        # Get source keypoint features\n",
    "        src_kp_features = feature_extractor.extract_features_at_keypoints(src_image, src_kps)\n",
    "        \n",
    "        # Match to target feature map\n",
    "        tgt_h, tgt_w, tgt_d = tgt_features.shape\n",
    "        tgt_features_flat = tgt_features.reshape(-1, tgt_d)\n",
    "        \n",
    "        # Normalize features\n",
    "        src_kp_norm = src_kp_features / (np.linalg.norm(src_kp_features, axis=1, keepdims=True) + 1e-8)\n",
    "        tgt_norm = tgt_features_flat / (np.linalg.norm(tgt_features_flat, axis=1, keepdims=True) + 1e-8)\n",
    "        \n",
    "        # Find nearest neighbors\n",
    "        similarity = src_kp_norm @ tgt_norm.T  # [N, H'*W']\n",
    "        best_matches = np.argmax(similarity, axis=1)\n",
    "        confidence = np.max(similarity, axis=1)\n",
    "        \n",
    "        # Convert flat indices to 2D coordinates in feature space\n",
    "        match_y = best_matches // tgt_w\n",
    "        match_x = best_matches % tgt_w\n",
    "        \n",
    "        # Map back to original image coordinates\n",
    "        orig_w, orig_h = tgt_feat_dict['original_size']\n",
    "        scale_x = orig_w / tgt_w\n",
    "        scale_y = orig_h / tgt_h\n",
    "        \n",
    "        predicted_kps = np.stack([match_x * scale_x, match_y * scale_y], axis=1)\n",
    "        \n",
    "        return predicted_kps, confidence\n",
    "\n",
    "\n",
    "print(\"‚úì Correspondence matcher defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3d63ee",
   "metadata": {},
   "source": [
    "## 12. Evaluation Metrics (PCK)\n",
    "\n",
    "The `PCKEvaluator` class computes **Percentage of Correct Keypoints (PCK)**:\n",
    "- Multiple thresholds: Œ± = [0.05, 0.10, 0.15]\n",
    "- Normalization by bbox diagonal or image diagonal\n",
    "- Batch evaluation across entire datasets\n",
    "- Per-category performance tracking\n",
    "\n",
    "A keypoint is \"correct\" if predicted location is within Œ± √ó normalization_distance from ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ac5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCKEvaluator:\n",
    "    \"\"\"Evaluate correspondence using Percentage of Correct Keypoints (PCK).\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_values=[0.05, 0.10, 0.15], use_bbox=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha_values: Threshold values for PCK@alpha\n",
    "            use_bbox: Normalize by bounding box size (else use image size)\n",
    "        \"\"\"\n",
    "        self.alpha_values = alpha_values\n",
    "        self.use_bbox = use_bbox\n",
    "    \n",
    "    def compute_pck(self, predicted_kps, gt_kps, image_size=None, bbox=None):\n",
    "        \"\"\"\n",
    "        Compute PCK for a single image pair.\n",
    "        \n",
    "        Args:\n",
    "            predicted_kps: Predicted keypoints [N, 2]\n",
    "            gt_kps: Ground truth keypoints [N, 2]\n",
    "            image_size: (width, height) of target image\n",
    "            bbox: Bounding box [x, y, w, h] for normalization\n",
    "            \n",
    "        Returns:\n",
    "            pck_scores: Dict of PCK@alpha values\n",
    "        \"\"\"\n",
    "        if len(predicted_kps) == 0 or len(gt_kps) == 0:\n",
    "            return {f'PCK@{alpha}': 0.0 for alpha in self.alpha_values}\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = np.linalg.norm(predicted_kps - gt_kps, axis=1)\n",
    "        \n",
    "        # Compute normalization factor\n",
    "        if self.use_bbox and bbox is not None and len(bbox) == 4:\n",
    "            # Normalize by bounding box diagonal\n",
    "            norm_factor = np.sqrt(bbox[2]**2 + bbox[3]**2)\n",
    "        elif image_size is not None:\n",
    "            # Normalize by image diagonal\n",
    "            norm_factor = np.sqrt(image_size[0]**2 + image_size[1]**2)\n",
    "        else:\n",
    "            # No normalization\n",
    "            norm_factor = 1.0\n",
    "        \n",
    "        # Compute PCK at different thresholds\n",
    "        pck_scores = {}\n",
    "        for alpha in self.alpha_values:\n",
    "            threshold = alpha * norm_factor\n",
    "            correct = (distances <= threshold).sum()\n",
    "            pck = correct / len(distances)\n",
    "            pck_scores[f'PCK@{alpha}'] = pck\n",
    "        \n",
    "        return pck_scores\n",
    "    \n",
    "    def evaluate_dataset(self, predictions, ground_truth, image_sizes=None, bboxes=None):\n",
    "        \"\"\"\n",
    "        Evaluate PCK over entire dataset.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of predicted keypoints arrays\n",
    "            ground_truth: List of ground truth keypoints arrays\n",
    "            image_sizes: List of (width, height) tuples\n",
    "            bboxes: List of bounding boxes\n",
    "            \n",
    "        Returns:\n",
    "            results: Dict with mean PCK and per-sample results\n",
    "        \"\"\"\n",
    "        all_pck_scores = {f'PCK@{alpha}': [] for alpha in self.alpha_values}\n",
    "        per_sample_results = []\n",
    "        \n",
    "        for i, (pred_kps, gt_kps) in enumerate(zip(predictions, ground_truth)):\n",
    "            img_size = image_sizes[i] if image_sizes else None\n",
    "            bbox = bboxes[i] if bboxes else None\n",
    "            \n",
    "            pck = self.compute_pck(pred_kps, gt_kps, img_size, bbox)\n",
    "            per_sample_results.append(pck)\n",
    "            \n",
    "            for key, value in pck.items():\n",
    "                all_pck_scores[key].append(value)\n",
    "        \n",
    "        # Compute mean PCK\n",
    "        mean_pck = {key: np.mean(values) for key, values in all_pck_scores.items()}\n",
    "        \n",
    "        results = {\n",
    "            'mean': mean_pck,\n",
    "            'per_sample': per_sample_results,\n",
    "            'num_samples': len(predictions)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_results(self, results):\n",
    "        \"\"\"Pretty print evaluation results.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"PCK EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Number of samples: {results['num_samples']}\")\n",
    "        print(\"\\nMean PCK scores:\")\n",
    "        for key, value in sorted(results['mean'].items()):\n",
    "            print(f\"  {key}: {value*100:.2f}%\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"‚úì PCK evaluator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b665b",
   "metadata": {},
   "source": [
    "## 13. End-to-End Evaluation Pipeline\n",
    "\n",
    "The `evaluate_correspondence()` function wraps the entire pipeline:\n",
    "1. Feature extraction from source and target images\n",
    "2. Correspondence matching with selected algorithm\n",
    "3. PCK evaluation at multiple thresholds\n",
    "4. Progress tracking with tqdm\n",
    "\n",
    "Returns predictions, ground truth, confidences, and PCK scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_correspondence(model, dataset, backbone='dinov2', device='cuda', \n",
    "                           max_samples=None, mutual_nn=False):\n",
    "    \"\"\"\n",
    "    End-to-end evaluation pipeline for semantic correspondence.\n",
    "    \n",
    "    Args:\n",
    "        model: Pretrained model (DINOv2 or SAM)\n",
    "        dataset: Dataset instance (PFPascalDataset or SPairDataset)\n",
    "        backbone: 'dinov2' or 'sam'\n",
    "        device: Device to run on\n",
    "        max_samples: Limit number of samples (None for all)\n",
    "        mutual_nn: Use mutual nearest neighbor matching\n",
    "        \n",
    "    Returns:\n",
    "        results: Evaluation results including PCK scores\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"EVALUATING {backbone.upper()} on {dataset.__class__.__name__}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total samples: {len(dataset)}\")\n",
    "    if max_samples:\n",
    "        print(f\"Evaluating on: {max_samples} samples\")\n",
    "    print(f\"Mutual NN: {mutual_nn}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Initialize components\n",
    "    feature_extractor = DenseFeatureExtractor(backbone=backbone, model=model, device=device)\n",
    "    matcher = CorrespondenceMatcher(method='nn', mutual=mutual_nn)\n",
    "    evaluator = PCKEvaluator(alpha_values=[0.05, 0.10, 0.15])\n",
    "    \n",
    "    # Collect predictions and ground truth\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    image_sizes = []\n",
    "    bboxes = []\n",
    "    confidences = []\n",
    "    \n",
    "    num_samples = min(max_samples, len(dataset)) if max_samples else len(dataset)\n",
    "    \n",
    "    for i in tqdm(range(num_samples), desc=\"Processing pairs\"):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        src_img = sample['source_image']\n",
    "        tgt_img = sample['target_image']\n",
    "        src_kps = sample['source_keypoints']\n",
    "        tgt_kps = sample['target_keypoints']\n",
    "        \n",
    "        if len(src_kps) == 0 or len(tgt_kps) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Match keypoints\n",
    "        pred_kps, conf = matcher.match_keypoints(\n",
    "            src_img, tgt_img, src_kps, feature_extractor\n",
    "        )\n",
    "        \n",
    "        predictions.append(pred_kps)\n",
    "        ground_truths.append(tgt_kps)\n",
    "        confidences.append(conf)\n",
    "        \n",
    "        # Get image size\n",
    "        if isinstance(tgt_img, Image.Image):\n",
    "            image_sizes.append(tgt_img.size)  # (W, H)\n",
    "        else:\n",
    "            image_sizes.append((tgt_img.shape[2], tgt_img.shape[1]))\n",
    "        \n",
    "        # Get bbox if available\n",
    "        if 'target_bbox' in sample and sample['target_bbox'] is not None:\n",
    "            bboxes.append(sample['target_bbox'])\n",
    "        else:\n",
    "            bboxes.append(None)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluator.evaluate_dataset(\n",
    "        predictions, ground_truths, image_sizes, bboxes\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    evaluator.print_results(results)\n",
    "    \n",
    "    # Add additional info\n",
    "    results['predictions'] = predictions\n",
    "    results['ground_truth'] = ground_truths\n",
    "    results['confidences'] = confidences\n",
    "    results['backbone'] = backbone\n",
    "    results['mutual_nn'] = mutual_nn\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úì Evaluation pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd6b79",
   "metadata": {},
   "source": [
    "## 14. Visualization Utilities\n",
    "\n",
    "Advanced visualization functions for analyzing correspondence results:\n",
    "- **visualize_matches()**: Shows source/target images with predicted and GT keypoints\n",
    "- **visualize_feature_similarity()**: Displays feature similarity heatmaps for debugging\n",
    "\n",
    "Helps understand model behavior and identify failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(src_img, tgt_img, src_kps, pred_kps, gt_kps=None, \n",
    "                     max_points=20, figsize=(20, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize correspondence matches between two images.\n",
    "    \n",
    "    Args:\n",
    "        src_img: Source image (PIL or numpy)\n",
    "        tgt_img: Target image (PIL or numpy)\n",
    "        src_kps: Source keypoints [N, 2]\n",
    "        pred_kps: Predicted target keypoints [N, 2]\n",
    "        gt_kps: Ground truth target keypoints [N, 2] (optional)\n",
    "        max_points: Maximum number of points to visualize\n",
    "        figsize: Figure size\n",
    "        save_path: Path to save figure (optional)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    from matplotlib.lines import Line2D\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(src_img, Image.Image):\n",
    "        src_img = np.array(src_img)\n",
    "    if isinstance(tgt_img, Image.Image):\n",
    "        tgt_img = np.array(tgt_img)\n",
    "    \n",
    "    # Limit number of points for clarity\n",
    "    if len(src_kps) > max_points:\n",
    "        indices = np.random.choice(len(src_kps), max_points, replace=False)\n",
    "        src_kps = src_kps[indices]\n",
    "        pred_kps = pred_kps[indices]\n",
    "        if gt_kps is not None:\n",
    "            gt_kps = gt_kps[indices]\n",
    "    \n",
    "    # Create figure\n",
    "    if gt_kps is not None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        ax_src, ax_pred, ax_gt = axes\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(figsize[0]*2/3, figsize[1]))\n",
    "        ax_src, ax_pred = axes\n",
    "        ax_gt = None\n",
    "    \n",
    "    # Plot source image with keypoints\n",
    "    ax_src.imshow(src_img)\n",
    "    ax_src.scatter(src_kps[:, 0], src_kps[:, 1], c='red', s=100, marker='o', \n",
    "                   edgecolors='white', linewidths=2, label='Source KPs')\n",
    "    ax_src.set_title('Source Image', fontsize=14, fontweight='bold')\n",
    "    ax_src.axis('off')\n",
    "    \n",
    "    # Plot target image with predicted keypoints\n",
    "    ax_pred.imshow(tgt_img)\n",
    "    ax_pred.scatter(pred_kps[:, 0], pred_kps[:, 1], c='blue', s=100, marker='x', \n",
    "                    linewidths=3, label='Predicted KPs')\n",
    "    ax_pred.set_title('Target Image (Predictions)', fontsize=14, fontweight='bold')\n",
    "    ax_pred.axis('off')\n",
    "    \n",
    "    # Plot target with ground truth if available\n",
    "    if gt_kps is not None and ax_gt is not None:\n",
    "        ax_gt.imshow(tgt_img)\n",
    "        ax_gt.scatter(gt_kps[:, 0], gt_kps[:, 1], c='green', s=100, marker='o', \n",
    "                     edgecolors='white', linewidths=2, label='Ground Truth')\n",
    "        ax_gt.scatter(pred_kps[:, 0], pred_kps[:, 1], c='blue', s=50, marker='x', \n",
    "                     linewidths=2, alpha=0.7, label='Predicted')\n",
    "        \n",
    "        # Draw error lines\n",
    "        for i in range(len(gt_kps)):\n",
    "            ax_gt.plot([gt_kps[i, 0], pred_kps[i, 0]], \n",
    "                      [gt_kps[i, 1], pred_kps[i, 1]], \n",
    "                      'r--', alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # Compute errors\n",
    "        errors = np.linalg.norm(pred_kps - gt_kps, axis=1)\n",
    "        mean_error = errors.mean()\n",
    "        ax_gt.set_title(f'Ground Truth vs Predicted\\nMean Error: {mean_error:.2f}px', \n",
    "                       fontsize=14, fontweight='bold')\n",
    "        ax_gt.axis('off')\n",
    "        ax_gt.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"‚úì Saved visualization to {save_path}\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def visualize_feature_similarity(src_img, tgt_img, feature_extractor, kp_idx=0, src_kps=None):\n",
    "    \"\"\"\n",
    "    Visualize feature similarity map for a keypoint.\n",
    "    \n",
    "    Args:\n",
    "        src_img: Source image\n",
    "        tgt_img: Target image\n",
    "        feature_extractor: DenseFeatureExtractor instance\n",
    "        kp_idx: Index of keypoint to visualize\n",
    "        src_kps: Source keypoints [N, 2]\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    src_feat_dict = feature_extractor.extract_features(src_img, return_numpy=True)\n",
    "    tgt_feat_dict = feature_extractor.extract_features(tgt_img, return_numpy=True)\n",
    "    \n",
    "    src_features = src_feat_dict['features']\n",
    "    tgt_features = tgt_feat_dict['features']\n",
    "    \n",
    "    # Get query feature\n",
    "    if src_kps is not None and kp_idx < len(src_kps):\n",
    "        query_feat = feature_extractor.extract_features_at_keypoints(src_img, src_kps[kp_idx:kp_idx+1])\n",
    "    else:\n",
    "        # Use center point\n",
    "        h, w = src_features.shape[:2]\n",
    "        query_feat = src_features[h//2, w//2:w//2+1, :]\n",
    "    \n",
    "    # Compute similarity map\n",
    "    query_norm = query_feat / (np.linalg.norm(query_feat) + 1e-8)\n",
    "    tgt_h, tgt_w, tgt_d = tgt_features.shape\n",
    "    tgt_flat = tgt_features.reshape(-1, tgt_d)\n",
    "    tgt_norm = tgt_flat / (np.linalg.norm(tgt_flat, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    similarity = (query_norm @ tgt_norm.T).reshape(tgt_h, tgt_w)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(src_img)\n",
    "    if src_kps is not None and kp_idx < len(src_kps):\n",
    "        axes[0].scatter(src_kps[kp_idx, 0], src_kps[kp_idx, 1], \n",
    "                       c='red', s=200, marker='*', edgecolors='white', linewidths=2)\n",
    "    axes[0].set_title('Source Image (Query Point)', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(tgt_img)\n",
    "    axes[1].set_title('Target Image', fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    im = axes[2].imshow(similarity, cmap='hot', interpolation='bilinear')\n",
    "    axes[2].set_title('Feature Similarity Map', fontsize=12, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    plt.colorbar(im, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"‚úì Visualization utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef5d26",
   "metadata": {},
   "source": [
    "## 15. Example Usage & Experiments\n",
    "\n",
    "The following cells demonstrate how to use the pipeline. Uncomment and run to experiment:\n",
    "- **Example 1**: Load datasets\n",
    "- **Example 2**: Evaluate on test split\n",
    "- **Example 3**: Visualize matches\n",
    "- **Example 4**: Compare different backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Load a dataset\n",
    "# Uncomment and run after downloading datasets\n",
    "\n",
    "# # Load PF-Pascal test split\n",
    "# pf_pascal = PFPascalDataset(\n",
    "#     root_dir=os.path.join(sd4match_data_dir, 'pf-pascal'),\n",
    "#     split='test'\n",
    "# )\n",
    "# print(f\"PF-Pascal test set: {len(pf_pascal)} pairs\")\n",
    "\n",
    "# # Load SPair-71k test split\n",
    "# spair = SPairDataset(\n",
    "#     root_dir=os.path.join(sd4match_data_dir, 'spair-71k'),\n",
    "#     split='test'\n",
    "# )\n",
    "# print(f\"SPair-71k test set: {len(spair)} pairs\")\n",
    "\n",
    "print(\"‚úì Dataset loading examples defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Evaluate DINOv2 on a dataset\n",
    "# Uncomment and run after loading models and datasets\n",
    "\n",
    "# if dinov2_model is not None:\n",
    "#     # Evaluate on first 50 samples (for quick testing)\n",
    "#     results_dinov2 = evaluate_correspondence(\n",
    "#         model=dinov2_model,\n",
    "#         dataset=pf_pascal,\n",
    "#         backbone='dinov2',\n",
    "#         device=device,\n",
    "#         max_samples=50,\n",
    "#         mutual_nn=False\n",
    "#     )\n",
    "#     \n",
    "#     # Save results\n",
    "#     import json\n",
    "#     results_path = os.path.join(OUTPUT_DIR, 'dinov2_pfpascal_results.json')\n",
    "#     with open(results_path, 'w') as f:\n",
    "#         # Save only serializable parts\n",
    "#         json.dump({\n",
    "#             'mean': results_dinov2['mean'],\n",
    "#             'num_samples': results_dinov2['num_samples'],\n",
    "#             'backbone': results_dinov2['backbone']\n",
    "#         }, f, indent=2)\n",
    "#     print(f\"‚úì Results saved to {results_path}\")\n",
    "\n",
    "print(\"‚úì Evaluation example defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Visualize a single correspondence\n",
    "# Uncomment and run to visualize results\n",
    "\n",
    "# if 'pf_pascal' in locals() and len(pf_pascal) > 0:\n",
    "#     # Get a sample\n",
    "#     sample_idx = 0\n",
    "#     sample = pf_pascal[sample_idx]\n",
    "#     \n",
    "#     # Extract predictions\n",
    "#     feature_extractor = DenseFeatureExtractor(backbone='dinov2', model=dinov2_model, device=device)\n",
    "#     matcher = CorrespondenceMatcher(method='nn', mutual=False)\n",
    "#     \n",
    "#     pred_kps, conf = matcher.match_keypoints(\n",
    "#         sample['source_image'],\n",
    "#         sample['target_image'],\n",
    "#         sample['source_keypoints'],\n",
    "#         feature_extractor\n",
    "#     )\n",
    "#     \n",
    "#     # Visualize\n",
    "#     fig = visualize_matches(\n",
    "#         sample['source_image'],\n",
    "#         sample['target_image'],\n",
    "#         sample['source_keypoints'],\n",
    "#         pred_kps,\n",
    "#         sample['target_keypoints'],\n",
    "#         max_points=15,\n",
    "#         save_path=os.path.join(OUTPUT_DIR, f'match_visualization_{sample_idx}.png')\n",
    "#     )\n",
    "#     plt.show()\n",
    "\n",
    "print(\"‚úì Visualization example defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Compare different backbones\n",
    "# Uncomment to run comparative experiments\n",
    "\n",
    "# def compare_backbones(dataset, max_samples=100):\n",
    "#     \"\"\"Compare DINOv2 vs SAM on a dataset.\"\"\"\n",
    "#     results = {}\n",
    "#     \n",
    "#     # Evaluate DINOv2\n",
    "#     if dinov2_model is not None:\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         print(\"EVALUATING DINOV2\")\n",
    "#         print(\"=\"*60)\n",
    "#         results['dinov2'] = evaluate_correspondence(\n",
    "#             model=dinov2_model,\n",
    "#             dataset=dataset,\n",
    "#             backbone='dinov2',\n",
    "#             device=device,\n",
    "#             max_samples=max_samples,\n",
    "#             mutual_nn=False\n",
    "#         )\n",
    "#     \n",
    "#     # Evaluate SAM\n",
    "#     if sam_model is not None:\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         print(\"EVALUATING SAM\")\n",
    "#         print(\"=\"*60)\n",
    "#         results['sam'] = evaluate_correspondence(\n",
    "#             model=sam_model,\n",
    "#             dataset=dataset,\n",
    "#             backbone='sam',\n",
    "#             device=device,\n",
    "#             max_samples=max_samples,\n",
    "#             mutual_nn=False\n",
    "#         )\n",
    "#     \n",
    "#     # Print comparison\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"COMPARISON SUMMARY\")\n",
    "#     print(\"=\"*60)\n",
    "#     for backbone, res in results.items():\n",
    "#         print(f\"\\n{backbone.upper()}:\")\n",
    "#         for metric, value in res['mean'].items():\n",
    "#             print(f\"  {metric}: {value*100:.2f}%\")\n",
    "#     \n",
    "#     return results\n",
    "# \n",
    "# # Run comparison\n",
    "# # comparison_results = compare_backbones(pf_pascal, max_samples=50)\n",
    "\n",
    "print(\"‚úì Comparison example defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3403f",
   "metadata": {},
   "source": [
    "## 16. Project Summary & Next Steps\n",
    "\n",
    "### ‚úÖ Completed Implementation\n",
    "\n",
    "**Phase 1 - Infrastructure:**\n",
    "- ‚úì DINOv2 ViT-B model loaded and ready\n",
    "- ‚úì SAM ViT-B model loaded and ready  \n",
    "- ‚úì Dataset download utilities for PF-Pascal, PF-Willow, SPair-71k\n",
    "- ‚úì Environment configuration (paths, device detection)\n",
    "\n",
    "**Phase 2 - Core Pipeline:**\n",
    "- ‚úì Dataset loaders (`PFPascalDataset`, `SPairDataset`)\n",
    "- ‚úì Dense feature extraction (`DenseFeatureExtractor`)\n",
    "- ‚úì Correspondence matching (`CorrespondenceMatcher`)\n",
    "  - Nearest neighbor matching\n",
    "  - Mutual nearest neighbor option\n",
    "  - Ratio test support\n",
    "- ‚úì PCK evaluation metrics (`PCKEvaluator`)\n",
    "  - PCK@0.05, PCK@0.10, PCK@0.15\n",
    "  - Bbox and image size normalization\n",
    "- ‚úì End-to-end evaluation pipeline\n",
    "- ‚úì Visualization utilities\n",
    "\n",
    "### üéØ How to Use\n",
    "\n",
    "**Step 1: Ensure all setup cells are run**\n",
    "```python\n",
    "# Run cells 1-5 to set up environment\n",
    "# Run cells for DINOv2 (section 3)\n",
    "# Run cells for SAM (section 5)\n",
    "```\n",
    "\n",
    "**Step 2: Download datasets**\n",
    "```python\n",
    "# The dataset download cell (section 2) attempts automatic download\n",
    "# Or manually download and place in DATA_ROOT/SD4Match/\n",
    "```\n",
    "\n",
    "**Step 3: Load a dataset**\n",
    "```python\n",
    "pf_pascal = PFPascalDataset(\n",
    "    root_dir=os.path.join(sd4match_data_dir, 'pf-pascal'),\n",
    "    split='test'\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 4: Run evaluation**\n",
    "```python\n",
    "results = evaluate_correspondence(\n",
    "    model=dinov2_model,\n",
    "    dataset=pf_pascal,\n",
    "    backbone='dinov2',\n",
    "    device=device,\n",
    "    max_samples=50  # Start with small number\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 5: Visualize results**\n",
    "```python\n",
    "# Use visualization functions to inspect matches\n",
    "```\n",
    "\n",
    "### üìä Evaluation Protocol (Professor's Guidelines)\n",
    "\n",
    "1. **Train on `trn` split** (if doing any training/fine-tuning)\n",
    "2. **Validate on `val` split** for model selection and hyperparameter tuning\n",
    "3. **Report final results ONLY on `test` split**\n",
    "4. **Metrics**: PCK@0.05, PCK@0.10, PCK@0.15\n",
    "5. **Backbones**: Compare DINOv2 ViT-B vs SAM ViT-B\n",
    "\n",
    "### üî¨ Suggested Experiments\n",
    "\n",
    "1. **Baseline Comparison**\n",
    "   - DINOv2 ViT-B vs SAM ViT-B\n",
    "   - With/without mutual nearest neighbor\n",
    "\n",
    "2. **Hyperparameter Tuning** (on val split)\n",
    "   - Matching thresholds\n",
    "   - Feature normalization strategies\n",
    "   - Ratio test thresholds\n",
    "\n",
    "3. **Dataset Analysis**\n",
    "   - Per-category performance\n",
    "   - Effect of viewpoint changes\n",
    "   - Effect of scale changes\n",
    "\n",
    "4. **Advanced Methods** (optional)\n",
    "   - Window soft argmax refinement (GeoAware-SC)\n",
    "   - Multi-scale features\n",
    "   - Feature aggregation strategies\n",
    "\n",
    "### üìù Notes\n",
    "\n",
    "- All code follows professor's recommendations (Base models, official repos, proper splits)\n",
    "- The pipeline is modular - easy to swap backbones or add new methods\n",
    "- Visualization utilities help debug and understand model behavior\n",
    "- Start with small `max_samples` for quick iteration, then scale up\n",
    "\n",
    "### üöÄ Ready to Run!\n",
    "\n",
    "Uncomment the example cells in section 15 to start experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
