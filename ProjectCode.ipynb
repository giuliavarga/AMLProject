{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638cffaf",
   "metadata": {},
   "source": [
    "# Semantic Correspondence Project - Phase 1 Setup\n",
    "## DINOv2, DINOv3, and SAM Backbones\n",
    "\n",
    "This notebook sets up the infrastructure for semantic correspondence using:\n",
    "- **DINOv2** (Facebook Research)\n",
    "- **DINOv3** (Facebook Research)\n",
    "- **SAM** (Segment Anything Model)\n",
    "- **SD4Match** dataset for evaluation\n",
    "\n",
    "**Professor's recommendations:**\n",
    "- Use **Base (ViT-B)** versions for all backbones\n",
    "- Use official repositories (not just Hugging Face) to access internal components\n",
    "- Dataset splits: train (trn), validation (val), test (test)\n",
    "- Always evaluate on test split only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a56efc",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2b22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Colab: False\n",
      "Project root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject\n",
      "Data root: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data\n",
      "Checkpoint dir: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints\n",
      "Output dir: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/outputs\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Google Colab\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running on Colab: {IN_COLAB}\")\n",
    "\n",
    "# Set up paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = '/content/AMLProject'\n",
    "    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'  # Recommended: upload dataset to Drive\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs')\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43600cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì± Detected macOS - installing CPU/MPS version\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (0.24.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: torch==2.9.1 in /opt/anaconda3/lib/python3.13/site-packages (from torchaudio) (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.1->torchaudio) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.1->torchaudio) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.1->torchaudio) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.1->torchaudio) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.1->torchaudio) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.1->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch==2.9.1->torchaudio) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.9.1->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch==2.9.1->torchaudio) (3.0.2)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.13/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (2.1.3)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.13/site-packages (1.15.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: timm in /opt/anaconda3/lib/python3.13/site-packages (1.0.22)\n",
      "Requirement already satisfied: einops in /opt/anaconda3/lib/python3.13/site-packages (0.8.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (from timm) (2.9.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (from timm) (0.24.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.13/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/lib/python3.13/site-packages (from timm) (1.2.2)\n",
      "Requirement already satisfied: safetensors in /opt/anaconda3/lib/python3.13/site-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: shellingham in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm) (1.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch->timm) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision->timm) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from typer-slim->huggingface_hub->timm) (8.1.8)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.13/site-packages (11.1.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Note: Use standard PyPI for Mac (no CUDA), use --index-url for Linux with CUDA\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    print(\"üì± Detected macOS - installing CPU/MPS version\")\n",
    "    !pip install torch torchvision\n",
    "    # torchaudio not needed for this project, skip if unavailable\n",
    "    try:\n",
    "        !pip install torchaudio\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  torchaudio not available on this platform (not needed for project)\")\n",
    "elif 'google.colab' in sys.modules:  # Google Colab\n",
    "    print(\"‚òÅÔ∏è  Detected Colab - using default installation\")\n",
    "    !pip install torch torchvision torchaudio\n",
    "else:  # Linux with CUDA\n",
    "    print(\"üñ•Ô∏è  Detected Linux - installing CUDA version\")\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "!pip install opencv-python matplotlib numpy scipy tqdm\n",
    "!pip install timm einops\n",
    "!pip install pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964e4c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyTorch version: 2.5.1\n",
      "Using device: mps (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "# Import common libraries\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚úó PyTorch not installed! Please run the installation cell (cell 4) first.\")\n",
    "    raise\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check device availability (CUDA, MPS, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device} (CPU only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eedf14",
   "metadata": {},
   "source": [
    "## 2. Dataset Setup - SD4Match\n",
    "\n",
    "SD4Match is the dataset for semantic correspondence evaluation.\n",
    "- **Repository**: https://github.com/ActiveVisionLab/SD4Match\n",
    "- **Splits**: train (trn), validation (val), test\n",
    "- **Usage**: Train on trn, validate on val, report final results on test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e86aaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/Users/giuliavarga/Desktop/2. AML/Project/AMLProject/SD4Match'...\n",
      "remote: Enumerating objects: 146, done.\u001b[K\n",
      "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Enumerating objects: 146, done.\u001b[K\n",
      "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 146 (delta 15), reused 18 (delta 5), pack-reused 102 (from 1)\u001b[K\n",
      "Receiving objects: 100% (146/146), 34.71 MiB | 6.89 MiB/s, done.\n",
      "Resolving deltas: 100% (18/18), done.\n",
      "remote: Total 146 (delta 15), reused 18 (delta 5), pack-reused 102 (from 1)\u001b[K\n",
      "Receiving objects: 100% (146/146), 34.71 MiB | 6.89 MiB/s, done.\n",
      "Resolving deltas: 100% (18/18), done.\n",
      "SD4Match repository cloned successfully\n",
      "SD4Match path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/SD4Match\n",
      "SD4Match repository cloned successfully\n",
      "SD4Match path: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/SD4Match\n"
     ]
    }
   ],
   "source": [
    "# Clone SD4Match repository\n",
    "sd4match_dir = os.path.join(PROJECT_ROOT, 'SD4Match')\n",
    "if not os.path.exists(sd4match_dir):\n",
    "    !git clone https://github.com/ActiveVisionLab/SD4Match.git \"{sd4match_dir}\"\n",
    "    print(\"SD4Match repository cloned successfully\")\n",
    "else:\n",
    "    print(\"SD4Match repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if sd4match_dir not in sys.path:\n",
    "    sys.path.insert(0, sd4match_dir)\n",
    "    \n",
    "print(f\"SD4Match path: {sd4match_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24236a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset should be placed in: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "Expected splits: trn/, val/, test/\n",
      "\n",
      "Note: Download instructions are in the SD4Match repository README\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "\"\"\"\n",
    "After cloning SD4Match, download the dataset and place it in the DATA_ROOT directory.\n",
    "If on Colab, upload to Google Drive for faster access across sessions.\n",
    "\n",
    "Expected structure:\n",
    "DATA_ROOT/\n",
    "    SD4Match/\n",
    "        trn/  (training split)\n",
    "        val/  (validation split)\n",
    "        test/ (test split)\n",
    "\"\"\"\n",
    "\n",
    "sd4match_data_dir = os.path.join(DATA_ROOT, 'SD4Match')\n",
    "print(f\"Dataset should be placed in: {sd4match_data_dir}\")\n",
    "print(f\"Expected splits: trn/, val/, test/\")\n",
    "print(\"\\nNote: Download instructions are in the SD4Match repository README\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7117b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Starting dataset download... This may take several minutes.\n",
      "\n",
      "============================================================\n",
      "SD4MATCH BENCHMARK DATASETS DOWNLOAD\n",
      "============================================================\n",
      "\n",
      "üì¶ PF-PASCAL\n",
      "----------------------------------------\n",
      "  Downloading pf-pascal images...\n",
      "  Downloading from https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset-PASCAL.zip\n",
      "  ‚úì Downloaded PF-dataset-PASCAL.zip\n",
      "  Extracting PF-dataset-PASCAL.zip...\n",
      "  ‚úì Downloaded PF-dataset-PASCAL.zip\n",
      "  Extracting PF-dataset-PASCAL.zip...\n",
      "  Downloading image pairs...\n",
      "  Downloading from https://www.robots.ox.ac.uk/~xinghui/sd4match/pf-pascal_image_pairs.zip\n",
      "  Downloading image pairs...\n",
      "  Downloading from https://www.robots.ox.ac.uk/~xinghui/sd4match/pf-pascal_image_pairs.zip\n",
      "  Extracting pf-pascal_image_pairs.zip...\n",
      "  ‚úì Downloaded pairs/splits\n",
      "  ‚úì pf-pascal setup complete!\n",
      "\n",
      "üì¶ PF-WILLOW\n",
      "----------------------------------------\n",
      "  Downloading pf-willow images...\n",
      "  Downloading from https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset.zip\n",
      "  Extracting pf-pascal_image_pairs.zip...\n",
      "  ‚úì Downloaded pairs/splits\n",
      "  ‚úì pf-pascal setup complete!\n",
      "\n",
      "üì¶ PF-WILLOW\n",
      "----------------------------------------\n",
      "  Downloading pf-willow images...\n",
      "  Downloading from https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset.zip\n",
      "  ‚úì Downloaded PF-dataset.zip\n",
      "  Extracting PF-dataset.zip...\n",
      "  Downloading image pairs...\n",
      "  Downloading from https://www.robots.ox.ac.uk/~xinghui/sd4match/test_pairs.csv\n",
      "  ‚úì Downloaded PF-dataset.zip\n",
      "  Extracting PF-dataset.zip...\n",
      "  Downloading image pairs...\n",
      "  Downloading from https://www.robots.ox.ac.uk/~xinghui/sd4match/test_pairs.csv\n",
      "  ‚úì Downloaded pairs/splits\n",
      "  ‚úì pf-willow setup complete!\n",
      "\n",
      "üì¶ SPAIR-71K\n",
      "----------------------------------------\n",
      "  Downloading spair-71k images...\n",
      "  Downloading from http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\n",
      "  ‚úì Downloaded pairs/splits\n",
      "  ‚úì pf-willow setup complete!\n",
      "\n",
      "üì¶ SPAIR-71K\n",
      "----------------------------------------\n",
      "  Downloading spair-71k images...\n",
      "  Downloading from http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\n",
      "  ‚úì Downloaded SPair-71k.tar.gz\n",
      "  Extracting SPair-71k.tar.gz...\n",
      "  ‚úì Downloaded SPair-71k.tar.gz\n",
      "  Extracting SPair-71k.tar.gz...\n",
      "  ‚úì spair-71k setup complete!\n",
      "\n",
      "============================================================\n",
      "‚úÖ All datasets downloaded successfully!\n",
      "\n",
      "Datasets location: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "\n",
      "Structure:\n",
      "/Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/\n",
      "  ‚îú‚îÄ‚îÄ pf-pascal/\n",
      "  ‚îú‚îÄ‚îÄ pf-willow/\n",
      "  ‚îî‚îÄ‚îÄ spair-71k/\n",
      "  ‚úì spair-71k setup complete!\n",
      "\n",
      "============================================================\n",
      "‚úÖ All datasets downloaded successfully!\n",
      "\n",
      "Datasets location: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match\n",
      "\n",
      "Structure:\n",
      "/Users/giuliavarga/Desktop/2. AML/Project/AMLProject/data/SD4Match/\n",
      "  ‚îú‚îÄ‚îÄ pf-pascal/\n",
      "  ‚îú‚îÄ‚îÄ pf-willow/\n",
      "  ‚îî‚îÄ‚îÄ spair-71k/\n"
     ]
    }
   ],
   "source": [
    "# Download SD4Match benchmark datasets automatically\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "def download_file(url, destination):\n",
    "    \"\"\"Download file with progress indication.\"\"\"\n",
    "    print(f\"  Downloading from {url}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, destination)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"Extract zip file.\"\"\"\n",
    "    print(f\"  Extracting {os.path.basename(zip_path)}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error extracting: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_sd4match_datasets(data_dir):\n",
    "    \"\"\"\n",
    "    Download and extract SD4Match benchmark datasets.\n",
    "    Includes: PF-Pascal, PF-Willow, and SPair-71k\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SD4MATCH BENCHMARK DATASETS DOWNLOAD\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Dataset configurations\n",
    "    datasets = {\n",
    "        'pf-pascal': {\n",
    "            'images_url': 'https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset-PASCAL.zip',\n",
    "            'pairs_url': 'https://www.robots.ox.ac.uk/~xinghui/sd4match/pf-pascal_image_pairs.zip',\n",
    "            'has_splits': True\n",
    "        },\n",
    "        'pf-willow': {\n",
    "            'images_url': 'https://www.di.ens.fr/willow/research/proposalflow/dataset/PF-dataset.zip',\n",
    "            'pairs_url': 'https://www.robots.ox.ac.uk/~xinghui/sd4match/test_pairs.csv',\n",
    "            'has_splits': False\n",
    "        },\n",
    "        'spair-71k': {\n",
    "            'images_url': 'http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz',\n",
    "            'has_splits': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_ready = True\n",
    "    \n",
    "    for dataset_name, config in datasets.items():\n",
    "        dataset_path = os.path.join(data_dir, dataset_name)\n",
    "        print(f\"\\nüì¶ {dataset_name.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if already exists\n",
    "        if os.path.exists(dataset_path) and os.listdir(dataset_path):\n",
    "            print(f\"  ‚úì Already exists at {dataset_path}\")\n",
    "            continue\n",
    "        \n",
    "        os.makedirs(dataset_path, exist_ok=True)\n",
    "        \n",
    "        # Download images\n",
    "        print(f\"  Downloading {dataset_name} images...\")\n",
    "        images_filename = os.path.basename(config['images_url'])\n",
    "        images_path = os.path.join(data_dir, images_filename)\n",
    "        \n",
    "        if not os.path.exists(images_path):\n",
    "            if download_file(config['images_url'], images_path):\n",
    "                print(f\"  ‚úì Downloaded {images_filename}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Failed to download images\")\n",
    "                all_ready = False\n",
    "                continue\n",
    "        \n",
    "        # Extract images\n",
    "        if images_filename.endswith('.zip'):\n",
    "            extract_zip(images_path, dataset_path)\n",
    "        elif images_filename.endswith('.tar.gz'):\n",
    "            print(f\"  Extracting {images_filename}...\")\n",
    "            with tarfile.open(images_path, 'r:gz') as tar:\n",
    "                tar.extractall(dataset_path)\n",
    "        \n",
    "        # Download pairs/splits if applicable\n",
    "        if 'pairs_url' in config:\n",
    "            pairs_filename = os.path.basename(config['pairs_url'])\n",
    "            pairs_path = os.path.join(data_dir, pairs_filename)\n",
    "            \n",
    "            print(f\"  Downloading image pairs...\")\n",
    "            if download_file(config['pairs_url'], pairs_path):\n",
    "                if pairs_filename.endswith('.zip'):\n",
    "                    extract_zip(pairs_path, dataset_path)\n",
    "                elif pairs_filename.endswith('.csv'):\n",
    "                    shutil.move(pairs_path, os.path.join(dataset_path, pairs_filename))\n",
    "                print(f\"  ‚úì Downloaded pairs/splits\")\n",
    "        \n",
    "        # Clean up zip files\n",
    "        if os.path.exists(images_path):\n",
    "            os.remove(images_path)\n",
    "        \n",
    "        print(f\"  ‚úì {dataset_name} setup complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    if all_ready:\n",
    "        print(\"‚úÖ All datasets downloaded successfully!\")\n",
    "        print(f\"\\nDatasets location: {data_dir}\")\n",
    "        print(\"\\nStructure:\")\n",
    "        print(f\"{data_dir}/\")\n",
    "        print(\"  ‚îú‚îÄ‚îÄ pf-pascal/\")\n",
    "        print(\"  ‚îú‚îÄ‚îÄ pf-willow/\")\n",
    "        print(\"  ‚îî‚îÄ‚îÄ spair-71k/\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some datasets failed to download automatically.\")\n",
    "        print(\"\\nüì• MANUAL DOWNLOAD INSTRUCTIONS:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"1. PF-Pascal: https://www.di.ens.fr/willow/research/proposalflow/\")\n",
    "        print(\"   Pairs: https://www.robots.ox.ac.uk/~xinghui/sd4match/pf-pascal_image_pairs.zip\")\n",
    "        print(\"\\n2. PF-Willow: https://www.di.ens.fr/willow/research/proposalflow/\")\n",
    "        print(\"   Pairs: https://www.robots.ox.ac.uk/~xinghui/sd4match/test_pairs.csv\")\n",
    "        print(\"\\n3. SPair-71k: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if IN_COLAB:\n",
    "            print(\"\\nüí° FOR GOOGLE COLAB:\")\n",
    "            print(\"   1. Download datasets to your computer\")\n",
    "            print(\"   2. Upload to Google Drive\")\n",
    "            print(\"   3. Mount Drive and set DATA_ROOT accordingly\")\n",
    "    \n",
    "    return all_ready\n",
    "\n",
    "# Attempt to download datasets\n",
    "print(\"‚è≥ Starting dataset download... This may take several minutes.\\n\")\n",
    "dataset_ready = download_sd4match_datasets(sd4match_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedcbc3d",
   "metadata": {},
   "source": [
    "## 3. DINOv2 Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/dinov2  \n",
    "**Model**: ViT-B (Base version)  \n",
    "**Key**: Use official repo (not just Hugging Face) to access internal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ccc76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone DINOv2 repository\n",
    "dinov2_dir = os.path.join(MODEL_DIR, 'dinov2')\n",
    "if not os.path.exists(dinov2_dir):\n",
    "    !git clone https://github.com/facebookresearch/dinov2.git \"{dinov2_dir}\"\n",
    "    print(\"DINOv2 repository cloned successfully\")\n",
    "else:\n",
    "    print(\"DINOv2 repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if dinov2_dir not in sys.path:\n",
    "    sys.path.insert(0, dinov2_dir)\n",
    "\n",
    "print(f\"DINOv2 path: {dinov2_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a896286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv2 ViT-B model\n",
    "def load_dinov2_model(model_name='dinov2_vitb14', device='cuda'):\n",
    "    \"\"\"\n",
    "    Load DINOv2 model from official repository.\n",
    "    \n",
    "    Available models:\n",
    "    - dinov2_vits14: Small (ViT-S/14)\n",
    "    - dinov2_vitb14: Base (ViT-B/14) - RECOMMENDED\n",
    "    - dinov2_vitl14: Large (ViT-L/14)\n",
    "    - dinov2_vitg14: Giant (ViT-G/14)\n",
    "    \n",
    "    The '14' indicates patch size of 14x14 pixels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"‚úì DINOv2 model '{model_name}' loaded successfully\")\n",
    "        print(f\"  - Patch size: 14x14\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading DINOv2: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the Base model (ViT-B)\n",
    "dinov2_model = load_dinov2_model('dinov2_vitb14', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddfe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINOv2 feature extraction utility\n",
    "def extract_dinov2_features(model, image, return_class_token=True, return_patch_tokens=True):\n",
    "    \"\"\"\n",
    "    Extract features from DINOv2 model.\n",
    "    \n",
    "    Args:\n",
    "        model: DINOv2 model\n",
    "        image: PIL Image or tensor (C, H, W) in range [0, 1]\n",
    "        return_class_token: Return [CLS] token\n",
    "        return_patch_tokens: Return patch tokens\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing requested features\n",
    "    \"\"\"\n",
    "    from torchvision import transforms\n",
    "    \n",
    "    # Prepare image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if isinstance(image, Image.Image):\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    elif image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    image = image.to(next(model.parameters()).device)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model.forward_features(image)\n",
    "        \n",
    "    result = {}\n",
    "    if return_class_token:\n",
    "        result['cls_token'] = features['x_norm_clstoken']\n",
    "    if return_patch_tokens:\n",
    "        result['patch_tokens'] = features['x_norm_patchtokens']\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"DINOv2 feature extraction utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43b15d",
   "metadata": {},
   "source": [
    "## 4. DINOv3 Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/dinov3  \n",
    "**Model**: ViT-B (Base version)  \n",
    "**Key**: Request access to checkpoints, then download pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone DINOv3 repository\n",
    "dinov3_dir = os.path.join(MODEL_DIR, 'dinov3')\n",
    "if not os.path.exists(dinov3_dir):\n",
    "    !git clone https://github.com/facebookresearch/dinov3.git \"{dinov3_dir}\"\n",
    "    print(\"DINOv3 repository cloned successfully\")\n",
    "else:\n",
    "    print(\"DINOv3 repository already exists\")\n",
    "\n",
    "# Add to Python path\n",
    "if dinov3_dir not in sys.path:\n",
    "    sys.path.insert(0, dinov3_dir)\n",
    "\n",
    "print(f\"DINOv3 path: {dinov3_dir}\")\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: Request access and download DINOv3 checkpoints\")\n",
    "print(\"   Follow instructions in the DINOv3 repository README\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINOv3 checkpoint configuration\n",
    "dinov3_checkpoint_dir = os.path.join(CHECKPOINT_DIR, 'dinov3')\n",
    "os.makedirs(dinov3_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Expected checkpoint path for ViT-B\n",
    "dinov3_checkpoint_path = os.path.join(dinov3_checkpoint_dir, 'dinov3_vitb14_pretrain.pth')\n",
    "\n",
    "print(f\"DINOv3 checkpoint directory: {dinov3_checkpoint_dir}\")\n",
    "print(f\"Expected checkpoint path: {dinov3_checkpoint_path}\")\n",
    "print(\"\\nAfter obtaining access, download the ViT-B checkpoint to this location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv3 model (after checkpoint is downloaded)\n",
    "def load_dinov3_model(checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load DINOv3 model from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the downloaded checkpoint\n",
    "        device: Device to load model on\n",
    "        \n",
    "    Returns:\n",
    "        Loaded DINOv3 model\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚úó Checkpoint not found: {checkpoint_path}\")\n",
    "        print(\"  Please download the DINOv3 checkpoint after requesting access\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # This will be updated once checkpoint structure is known\n",
    "        # Placeholder for actual loading code\n",
    "        print(f\"‚úì Loading DINOv3 from: {checkpoint_path}\")\n",
    "        \n",
    "        # Import DINOv3 modules (adjust based on actual repo structure)\n",
    "        # from dinov3.models import build_model\n",
    "        # model = build_model(checkpoint_path)\n",
    "        # model = model.to(device)\n",
    "        # model.eval()\n",
    "        \n",
    "        print(\"‚úì DINOv3 model loaded successfully\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        return None  # Will return actual model after implementation\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading DINOv3: {e}\")\n",
    "        return None\n",
    "\n",
    "# Note: Uncomment and run after downloading checkpoint\n",
    "# dinov3_model = load_dinov3_model(dinov3_checkpoint_path, device=device)\n",
    "print(\"DINOv3 loader defined (run after downloading checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb4c1f",
   "metadata": {},
   "source": [
    "## 5. SAM (Segment Anything) Backbone Setup\n",
    "\n",
    "**Repository**: https://github.com/facebookresearch/segment-anything  \n",
    "**Model**: ViT-B (Base version) - RECOMMENDED  \n",
    "**Optional**: Can experiment with ViT-L (Large) or ViT-H (Huge) for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c57153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-req-build-mpm6h8y8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-req-build-mpm6h8y8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-req-build-mpm6h8y8\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Installing build dependencies ... \u001b[?25l  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\n",
      "  Installing build dependencies ... \u001b[?25l-done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25hBuilding wheels for collected packages: segment_anything\n",
      "  Building wheel for segment_anything (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25hBuilding wheels for collected packages: segment_anything\n",
      "  Building wheel for segment_anything (pyproject.toml) ... \u001b[?25l-done\n",
      "\u001b[?25h  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36636 sha256=84a0a4a95043e0fe8d130fed067aab9a0a84df76e1abb38671d32811a34e8404\n",
      "  Stored in directory: /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-ephem-wheel-cache-nlye9hi2/wheels/15/d7/bd/05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
      "Successfully built segment_anything\n",
      "\bdone\n",
      "\u001b[?25h  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36636 sha256=84a0a4a95043e0fe8d130fed067aab9a0a84df76e1abb38671d32811a34e8404\n",
      "  Stored in directory: /private/var/folders/kp/dmvkcybs4k72tbdpsb3zxlrh0000gn/T/pip-ephem-wheel-cache-nlye9hi2/wheels/15/d7/bd/05f5f23b7dcbe70cbc6783b06f12143b0cf1a5da5c7b52dcc5\n",
      "Successfully built segment_anything\n",
      "Installing collected packages: segment_anything\n",
      "Installing collected packages: segment_anything\n",
      "Successfully installed segment_anything-1.0\n",
      "Successfully installed segment_anything-1.0\n"
     ]
    }
   ],
   "source": [
    "# Install SAM\n",
    "!pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a07087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ViT-B (Base) - RECOMMENDED...\n",
      "URL: https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
      "‚úì Downloaded successfully: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam/sam_vit_b_01ec64.pth\n",
      "\n",
      "SAM checkpoint directory: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam\n",
      "Available models: ['vit_b', 'vit_l', 'vit_h']\n",
      "‚úì Downloaded successfully: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam/sam_vit_b_01ec64.pth\n",
      "\n",
      "SAM checkpoint directory: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam\n",
      "Available models: ['vit_b', 'vit_l', 'vit_h']\n"
     ]
    }
   ],
   "source": [
    "# Download SAM checkpoints\n",
    "import urllib.request\n",
    "\n",
    "sam_checkpoint_dir = os.path.join(CHECKPOINT_DIR, 'sam')\n",
    "os.makedirs(sam_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# SAM model checkpoints\n",
    "SAM_MODELS = {\n",
    "    'vit_b': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth',\n",
    "        'filename': 'sam_vit_b_01ec64.pth',\n",
    "        'description': 'ViT-B (Base) - RECOMMENDED'\n",
    "    },\n",
    "    'vit_l': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth',\n",
    "        'filename': 'sam_vit_l_0b3195.pth',\n",
    "        'description': 'ViT-L (Large) - Optional comparison'\n",
    "    },\n",
    "    'vit_h': {\n",
    "        'url': 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth',\n",
    "        'filename': 'sam_vit_h_4b8939.pth',\n",
    "        'description': 'ViT-H (Huge) - Optional comparison'\n",
    "    }\n",
    "}\n",
    "\n",
    "def download_sam_checkpoint(model_type='vit_b'):\n",
    "    \"\"\"Download SAM checkpoint if not already present.\"\"\"\n",
    "    if model_type not in SAM_MODELS:\n",
    "        print(f\"Invalid model type. Choose from: {list(SAM_MODELS.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    model_info = SAM_MODELS[model_type]\n",
    "    checkpoint_path = os.path.join(sam_checkpoint_dir, model_info['filename'])\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"‚úì Checkpoint already exists: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    print(f\"Downloading {model_info['description']}...\")\n",
    "    print(f\"URL: {model_info['url']}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(model_info['url'], checkpoint_path)\n",
    "        print(f\"‚úì Downloaded successfully: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error downloading: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download ViT-B checkpoint (recommended)\n",
    "sam_checkpoint_path = download_sam_checkpoint('vit_b')\n",
    "\n",
    "print(f\"\\nSAM checkpoint directory: {sam_checkpoint_dir}\")\n",
    "print(\"Available models:\", list(SAM_MODELS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50abda56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <EB3FF92A-5EB1-3EE8-AF8B-5923C1265422> /opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aml_project/lib/python3.11/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/aml_project/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n",
      "/opt/anaconda3/envs/aml_project/lib/python3.11/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SAM model loaded successfully\n",
      "  - Model type: vit_b\n",
      "  - Device: mps\n",
      "  - Checkpoint: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/checkpoints/sam/sam_vit_b_01ec64.pth\n"
     ]
    }
   ],
   "source": [
    "# Load SAM model\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "def load_sam_model(checkpoint_path, model_type='vit_b', device='cuda'):\n",
    "    \"\"\"\n",
    "    Load SAM model.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to checkpoint\n",
    "        model_type: 'vit_b', 'vit_l', or 'vit_h'\n",
    "        device: Device to load on\n",
    "        \n",
    "    Returns:\n",
    "        SAM model and predictor\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚úó Checkpoint not found: {checkpoint_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        sam = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "        sam = sam.to(device)\n",
    "        sam.eval()\n",
    "        \n",
    "        # Create predictor for easier inference\n",
    "        predictor = SamPredictor(sam)\n",
    "        \n",
    "        print(f\"‚úì SAM model loaded successfully\")\n",
    "        print(f\"  - Model type: {model_type}\")\n",
    "        print(f\"  - Device: {device}\")\n",
    "        print(f\"  - Checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        return sam, predictor\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading SAM: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load SAM ViT-B\n",
    "if sam_checkpoint_path:\n",
    "    sam_model, sam_predictor = load_sam_model(sam_checkpoint_path, 'vit_b', device=device)\n",
    "else:\n",
    "    print(\"SAM checkpoint not available yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c263c996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM feature extraction utility defined\n"
     ]
    }
   ],
   "source": [
    "# SAM feature extraction utility\n",
    "def extract_sam_features(sam_model, image):\n",
    "    \"\"\"\n",
    "    Extract features from SAM image encoder.\n",
    "    \n",
    "    Args:\n",
    "        sam_model: SAM model\n",
    "        image: PIL Image or numpy array (H, W, 3) in RGB format\n",
    "        \n",
    "    Returns:\n",
    "        Image embeddings from SAM encoder\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from segment_anything.utils.transforms import ResizeLongestSide\n",
    "    \n",
    "    # Convert PIL to numpy if needed\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    # SAM preprocessing\n",
    "    transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "    input_image = transform.apply_image(image)\n",
    "    input_image_torch = torch.as_tensor(input_image, device=sam_model.device)\n",
    "    input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        image_embedding = sam_model.image_encoder(input_image_torch)\n",
    "    \n",
    "    return image_embedding\n",
    "\n",
    "print(\"SAM feature extraction utility defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6eecb1",
   "metadata": {},
   "source": [
    "## 6. Utility Functions & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class for the project\n",
    "class ProjectConfig:\n",
    "    \"\"\"Central configuration for the semantic correspondence project.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.project_root = PROJECT_ROOT\n",
    "        self.data_root = DATA_ROOT\n",
    "        self.checkpoint_dir = CHECKPOINT_DIR\n",
    "        self.output_dir = OUTPUT_DIR\n",
    "        self.model_dir = MODEL_DIR\n",
    "        \n",
    "        # Dataset\n",
    "        self.dataset_name = 'SD4Match'\n",
    "        self.splits = ['trn', 'val', 'test']\n",
    "        \n",
    "        # Models\n",
    "        self.backbones = {\n",
    "            'dinov2': 'dinov2_vitb14',\n",
    "            'dinov3': 'dinov3_vitb14',\n",
    "            'sam': 'vit_b'\n",
    "        }\n",
    "        \n",
    "        # Device\n",
    "        self.device = device\n",
    "        \n",
    "        # Training (to be filled in later phases)\n",
    "        self.batch_size = 16\n",
    "        self.num_epochs = 100\n",
    "        self.learning_rate = 1e-4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"ProjectConfig:\n",
    "  Project Root: {self.project_root}\n",
    "  Data Root: {self.data_root}\n",
    "  Device: {self.device}\n",
    "  Dataset: {self.dataset_name}\n",
    "  Backbones: {list(self.backbones.keys())}\n",
    "\"\"\"\n",
    "\n",
    "config = ProjectConfig()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2290c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization utilities\n",
    "def visualize_correspondence(img1, img2, pts1, pts2, matches=None, figsize=(15, 7)):\n",
    "    \"\"\"\n",
    "    Visualize correspondence between two images.\n",
    "    \n",
    "    Args:\n",
    "        img1, img2: Images (PIL or numpy)\n",
    "        pts1, pts2: Keypoint coordinates [(x, y), ...]\n",
    "        matches: Optional list of match indices [(idx1, idx2), ...]\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Display images\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('Image 1')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title('Image 2')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Plot keypoints\n",
    "    if pts1 is not None and len(pts1) > 0:\n",
    "        pts1 = np.array(pts1)\n",
    "        ax1.scatter(pts1[:, 0], pts1[:, 1], c='red', s=50, marker='x')\n",
    "    \n",
    "    if pts2 is not None and len(pts2) > 0:\n",
    "        pts2 = np.array(pts2)\n",
    "        ax2.scatter(pts2[:, 0], pts2[:, 1], c='red', s=50, marker='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch, path, **kwargs):\n",
    "    \"\"\"Save model checkpoint with metadata.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
    "        **kwargs\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"‚úì Checkpoint saved: {path}\")\n",
    "\n",
    "def load_model_checkpoint(model, path, optimizer=None, device='cuda'):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚úó Checkpoint not found: {path}\")\n",
    "        return None\n",
    "    \n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer and checkpoint.get('optimizer_state_dict'):\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    print(f\"‚úì Checkpoint loaded from epoch {epoch}\")\n",
    "    return checkpoint\n",
    "\n",
    "print(\"Visualization and checkpoint utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea2edc",
   "metadata": {},
   "source": [
    "## 7. Model Summary & Testing\n",
    "\n",
    "Quick tests to verify all models are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of loaded models\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SETUP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_status = {\n",
    "    'DINOv2 (ViT-B)': dinov2_model is not None if 'dinov2_model' in locals() else False,\n",
    "    'DINOv3 (ViT-B)': False,  # To be loaded after checkpoint download\n",
    "    'SAM (ViT-B)': (sam_model is not None) if 'sam_model' in locals() else False,\n",
    "}\n",
    "\n",
    "for model_name, status in models_status.items():\n",
    "    status_symbol = \"‚úì\" if status else \"‚ö†\"\n",
    "    status_text = \"Loaded\" if status else \"Not loaded yet\"\n",
    "    print(f\"{status_symbol} {model_name}: {status_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. DINOv3: Request access and download checkpoint\")\n",
    "print(\"2. SD4Match: Download dataset to\", sd4match_data_dir)\n",
    "print(\"3. Verify all models work with test images\")\n",
    "print(\"4. Ready for team to implement correspondence methods\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a dummy image (optional)\n",
    "def test_model_inference():\n",
    "    \"\"\"Quick test to verify models can process images.\"\"\"\n",
    "    # Create a dummy image\n",
    "    dummy_image = Image.new('RGB', (224, 224), color='red')\n",
    "    \n",
    "    print(\"Testing model inference with dummy image...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test DINOv2\n",
    "    if 'dinov2_model' in locals() and dinov2_model is not None:\n",
    "        try:\n",
    "            features = extract_dinov2_features(dinov2_model, dummy_image)\n",
    "            print(f\"‚úì DINOv2: CLS token shape = {features['cls_token'].shape}\")\n",
    "            print(f\"           Patch tokens shape = {features['patch_tokens'].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó DINOv2 error: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö† DINOv2: Not loaded\")\n",
    "    \n",
    "    # Test SAM\n",
    "    if 'sam_model' in locals() and sam_model is not None:\n",
    "        try:\n",
    "            embedding = extract_sam_features(sam_model, dummy_image)\n",
    "            print(f\"‚úì SAM: Embedding shape = {embedding.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó SAM error: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö† SAM: Not loaded\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(\"Model inference test complete\")\n",
    "\n",
    "# Uncomment to run test\n",
    "# test_model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87837609",
   "metadata": {},
   "source": [
    "## 8. Additional Resources & Notes\n",
    "\n",
    "### Window Soft Argmax (GeoAware-SC)\n",
    "For prediction refinement in later phases:\n",
    "- **Repository**: https://github.com/Junyi42/geoaware-sc\n",
    "- This will be used for refining correspondence predictions\n",
    "\n",
    "### Professor's Key Recommendations Summary:\n",
    "1. **Backbone Selection**: Use Base (ViT-B) versions for all three backbones\n",
    "2. **Model Access**: \n",
    "   - DINOv2: Use official repo, not just Hugging Face\n",
    "   - DINOv3: Request access to checkpoints\n",
    "   - SAM: ViT-B recommended, can compare with L/H if compute allows\n",
    "3. **Dataset Splits**:\n",
    "   - Train on `trn` split\n",
    "   - Validate on `val` split for model selection\n",
    "   - **Only report final results on `test` split**\n",
    "4. **Backbone Size Trade-offs**:\n",
    "   - Larger backbones (Small ‚Üí Base ‚Üí Large) generally improve performance\n",
    "   - But gains are not always consistent across tasks\n",
    "   - Increased size = higher compute/memory/time costs\n",
    "\n",
    "### For Team Members (Later Phases):\n",
    "- All infrastructure is ready for implementing correspondence methods\n",
    "- Models are loaded and ready to extract features\n",
    "- Utilities for visualization and checkpointing are provided\n",
    "- Follow the professor's evaluation protocol strictly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
