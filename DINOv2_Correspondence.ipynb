{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93279fdf",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup & Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect environment and configure paths\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"✓ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"✓ Running locally\")\n",
    "\n",
    "# Set up paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = '/content/AMLProject'\n",
    "    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'\n",
    "else:\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
    "\n",
    "# Create necessary directories\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'dinov2')\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "\n",
    "for directory in [CHECKPOINT_DIR, OUTPUT_DIR, MODEL_DIR, DATA_ROOT]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa5d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def window_soft_argmax(similarity, H, W, window=7, tau=0.05):\n",
    "    \"\"\"\n",
    "    Window soft-argmax for sub-pixel coordinate prediction.\n",
    "    \n",
    "    Args:\n",
    "        similarity: [N, H*W] or [N, H, W] similarity scores\n",
    "        H, W: Grid dimensions\n",
    "        window: Window size around peak (odd number)\n",
    "        tau: Temperature for softmax (lower = sharper)\n",
    "    \n",
    "    Returns:\n",
    "        [N, 2] tensor with (y, x) coordinates in patch space\n",
    "    \"\"\"\n",
    "    if similarity.dim() == 2:\n",
    "        N = similarity.size(0)\n",
    "        sim2d = similarity.view(N, H, W)\n",
    "    elif similarity.dim() == 3:\n",
    "        N = similarity.size(0)\n",
    "        sim2d = similarity\n",
    "    else:\n",
    "        raise ValueError(\"similarity must be [N,H*W] or [N,H,W]\")\n",
    "    \n",
    "    r = window // 2\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        s = sim2d[i]  # [H, W]\n",
    "        \n",
    "        # Find peak with argmax\n",
    "        idx = torch.argmax(s)\n",
    "        y0 = (idx // W).item()\n",
    "        x0 = (idx % W).item()\n",
    "        \n",
    "        # Extract window around peak\n",
    "        y1, y2 = max(y0 - r, 0), min(y0 + r + 1, H)\n",
    "        x1, x2 = max(x0 - r, 0), min(x0 + r + 1, W)\n",
    "        \n",
    "        sub = s[y1:y2, x1:x2]\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.arange(y1, y2, device=s.device, dtype=torch.float32),\n",
    "            torch.arange(x1, x2, device=s.device, dtype=torch.float32),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        \n",
    "        # Soft-argmax within window\n",
    "        wts = torch.softmax(sub.flatten() / tau, dim=0).view_as(sub)\n",
    "        y_hat = (wts * yy).sum()\n",
    "        x_hat = (wts * xx).sum()\n",
    "        \n",
    "        preds.append(torch.stack([y_hat, x_hat]))\n",
    "    \n",
    "    return torch.stack(preds, dim=0)  # [N, 2]\n",
    "\n",
    "\n",
    "def unfreeze_last_k_blocks(model, k, blocks_attr='blocks'):\n",
    "    \"\"\"\n",
    "    Unfreeze the last k transformer blocks of a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The backbone model\n",
    "        k: Number of last blocks to unfreeze\n",
    "        blocks_attr: Attribute name for blocks (default 'blocks')\n",
    "    \n",
    "    Returns:\n",
    "        List of trainable parameters\n",
    "    \"\"\"\n",
    "    # Freeze all parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Unfreeze last k blocks\n",
    "    blocks = getattr(model, blocks_attr)\n",
    "    for block in blocks[-k:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "    \n",
    "    # Return trainable parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f\"Unfroze last {k} blocks: {len(trainable_params)} trainable parameters\")\n",
    "    \n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "def compute_keypoint_loss(sim2d, H, W, gt_xy_px, patch_size, use_soft=True, window=7, tau=0.05):\n",
    "    \"\"\"\n",
    "    Compute loss from similarity map to ground truth keypoint.\n",
    "    \n",
    "    Args:\n",
    "        sim2d: [H, W] similarity map\n",
    "        H, W: Grid dimensions\n",
    "        gt_xy_px: [2] ground truth coordinates in pixels (y, x)\n",
    "        patch_size: Patch size for coordinate conversion\n",
    "        use_soft: Use soft-argmax (True) or argmax (False)\n",
    "        window, tau: Soft-argmax parameters\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss\n",
    "    \"\"\"\n",
    "    if use_soft:\n",
    "        pred_xy_patch = window_soft_argmax(sim2d[None], H, W, window, tau)[0]\n",
    "    else:\n",
    "        idx = sim2d.argmax()\n",
    "        pred_xy_patch = torch.stack([idx // W, idx % W]).float()\n",
    "    \n",
    "    pred_xy_px = (pred_xy_patch + 0.5) * patch_size\n",
    "    \n",
    "    return F.smooth_l1_loss(pred_xy_px, gt_xy_px)\n",
    "\n",
    "\n",
    "print(\"✓ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SPair-71k dataset with keypoint annotations for correspondence learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='trn', category=None, image_size=224, subset=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir: Path to SPair-71k root directory\n",
    "            split: 'trn', 'val', or 'test'\n",
    "            category: Specific category or None for all\n",
    "            image_size: Target image size for resizing\n",
    "            subset: If int, use only first N samples (for debugging)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.category = category\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Load pairs\n",
    "        self.pairs = self._load_pairs()\n",
    "        if subset is not None:\n",
    "            self.pairs = self.pairs[:subset]\n",
    "        \n",
    "        # Image transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(f\"SPair-71k {split} dataset: {len(self.pairs)} pairs loaded\")\n",
    "    \n",
    "    def _load_pairs(self):\n",
    "        \"\"\"Load pair annotations from layout files.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        # Get layout files\n",
    "        layout_dir = os.path.join(self.root_dir, 'Layout', self.split)\n",
    "        \n",
    "        if not os.path.exists(layout_dir):\n",
    "            print(f\"Warning: Layout directory not found: {layout_dir}\")\n",
    "            return pairs\n",
    "        \n",
    "        # Get all categories or specific one\n",
    "        if self.category:\n",
    "            categories = [self.category]\n",
    "        else:\n",
    "            categories = [d for d in os.listdir(layout_dir) \n",
    "                         if os.path.isdir(os.path.join(layout_dir, d))]\n",
    "        \n",
    "        # Load pairs from each category\n",
    "        for cat in categories:\n",
    "            cat_dir = os.path.join(layout_dir, cat)\n",
    "            if not os.path.exists(cat_dir):\n",
    "                continue\n",
    "            \n",
    "            for fname in os.listdir(cat_dir):\n",
    "                if not fname.endswith('.json'):\n",
    "                    continue\n",
    "                \n",
    "                json_path = os.path.join(cat_dir, fname)\n",
    "                try:\n",
    "                    with open(json_path, 'r') as f:\n",
    "                        pair_data = json.load(f)\n",
    "                    \n",
    "                    # Extract pair info\n",
    "                    pair = {\n",
    "                        'category': cat,\n",
    "                        'src_img': pair_data['src_imname'],\n",
    "                        'tgt_img': pair_data['trg_imname'],\n",
    "                        'src_kps': np.array(pair_data['src_kps']).reshape(-1, 2),\n",
    "                        'tgt_kps': np.array(pair_data['trg_kps']).reshape(-1, 2),\n",
    "                        'src_bbox': pair_data.get('src_bndbox', None),\n",
    "                        'tgt_bbox': pair_data.get('trg_bndbox', None),\n",
    "                    }\n",
    "                    pairs.append(pair)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {json_path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        # Load images\n",
    "        src_img_path = os.path.join(self.root_dir, 'JPEGImages', \n",
    "                                    pair['category'], pair['src_img'])\n",
    "        tgt_img_path = os.path.join(self.root_dir, 'JPEGImages',\n",
    "                                    pair['category'], pair['tgt_img'])\n",
    "        \n",
    "        src_img_pil = Image.open(src_img_path).convert('RGB')\n",
    "        tgt_img_pil = Image.open(tgt_img_path).convert('RGB')\n",
    "        \n",
    "        # Original sizes\n",
    "        src_w, src_h = src_img_pil.size\n",
    "        tgt_w, tgt_h = tgt_img_pil.size\n",
    "        \n",
    "        # Scale keypoints to match resized images\n",
    "        src_kps = pair['src_kps'].copy().astype(float)\n",
    "        tgt_kps = pair['tgt_kps'].copy().astype(float)\n",
    "        \n",
    "        src_kps[:, 0] *= self.image_size / src_w\n",
    "        src_kps[:, 1] *= self.image_size / src_h\n",
    "        tgt_kps[:, 0] *= self.image_size / tgt_w\n",
    "        tgt_kps[:, 1] *= self.image_size / tgt_h\n",
    "        \n",
    "        # Transform images\n",
    "        src_img = self.transform(src_img_pil)\n",
    "        tgt_img = self.transform(tgt_img_pil)\n",
    "        \n",
    "        # Bounding boxes (scaled)\n",
    "        if pair['src_bbox'] is not None:\n",
    "            src_bbox = np.array(pair['src_bbox'])\n",
    "            src_bbox[0::2] *= self.image_size / src_w\n",
    "            src_bbox[1::2] *= self.image_size / src_h\n",
    "            src_bbox_wh = np.array([src_bbox[2] - src_bbox[0], \n",
    "                                   src_bbox[3] - src_bbox[1]])\n",
    "        else:\n",
    "            src_bbox_wh = np.array([self.image_size, self.image_size])\n",
    "        \n",
    "        if pair['tgt_bbox'] is not None:\n",
    "            tgt_bbox = np.array(pair['tgt_bbox'])\n",
    "            tgt_bbox[0::2] *= self.image_size / tgt_w\n",
    "            tgt_bbox[1::2] *= self.image_size / tgt_h\n",
    "            tgt_bbox_wh = np.array([tgt_bbox[2] - tgt_bbox[0],\n",
    "                                   tgt_bbox[3] - tgt_bbox[1]])\n",
    "        else:\n",
    "            tgt_bbox_wh = np.array([self.image_size, self.image_size])\n",
    "        \n",
    "        return {\n",
    "            'src_img': src_img,\n",
    "            'tgt_img': tgt_img,\n",
    "            'src_kps': torch.from_numpy(src_kps).float(),\n",
    "            'tgt_kps': torch.from_numpy(tgt_kps).float(),\n",
    "            'src_bbox_wh': torch.from_numpy(src_bbox_wh).float(),\n",
    "            'tgt_bbox_wh': torch.from_numpy(tgt_bbox_wh).float(),\n",
    "            'category': pair['category'],\n",
    "            'pair_id': idx\n",
    "        }\n",
    "\n",
    "\n",
    "def create_spair_dataloaders(root_dir, batch_size=1, num_workers=2, \n",
    "                             train_subset=None, val_subset=None):\n",
    "    \"\"\"\n",
    "    Create SPair-71k dataloaders for training and validation.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Path to SPair-71k directory\n",
    "        batch_size: Batch size (typically 1 for correspondence)\n",
    "        num_workers: Number of data loading workers\n",
    "        train_subset: If int, limit training samples\n",
    "        val_subset: If int, limit validation samples\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader\n",
    "    \"\"\"\n",
    "    train_dataset = SPairDataset(root_dir, split='trn', subset=train_subset)\n",
    "    val_dataset = SPairDataset(root_dir, split='val', subset=val_subset)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "print(\"✓ SPair-71k dataloader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffbc17d",
   "metadata": {},
   "source": [
    "## SPair-71k Dataloader\n",
    "\n",
    "Complete dataloader for SPair-71k with keypoint annotations for finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef77559",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Window soft-argmax for sub-pixel refinement and finetuning utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION FLAGS ==========\n",
    "# Set these flags to control behavior\n",
    "ENABLE_FINETUNING = False  # Set True to enable light finetuning of last layers\n",
    "USE_SOFT_ARGMAX = False    # Set True to use window soft-argmax instead of argmax\n",
    "\n",
    "# Finetuning hyperparameters (only used if ENABLE_FINETUNING=True)\n",
    "FINETUNE_K_LAYERS = 2      # Number of last transformer blocks to unfreeze {1, 2, 4}\n",
    "FINETUNE_LR = 1e-5         # Learning rate\n",
    "FINETUNE_WD = 1e-4         # Weight decay\n",
    "FINETUNE_EPOCHS = 3        # Number of training epochs\n",
    "FINETUNE_BATCH_SIZE = 1    # Batch size for training\n",
    "FINETUNE_TRAIN_SUBSET = None  # None for full training set, or int for subset\n",
    "\n",
    "# Soft-argmax hyperparameters (only used if USE_SOFT_ARGMAX=True)\n",
    "SOFT_WINDOW = 7            # Window size around peak (odd number: 5, 7, 9)\n",
    "SOFT_TAU = 0.05            # Softmax temperature (lower = sharper)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  ENABLE_FINETUNING = {ENABLE_FINETUNING}\")\n",
    "print(f\"  USE_SOFT_ARGMAX = {USE_SOFT_ARGMAX}\")\n",
    "if ENABLE_FINETUNING:\n",
    "    print(f\"  Finetuning: k={FINETUNE_K_LAYERS}, lr={FINETUNE_LR}, epochs={FINETUNE_EPOCHS}\")\n",
    "if USE_SOFT_ARGMAX:\n",
    "    print(f\"  Soft-argmax: window={SOFT_WINDOW}, tau={SOFT_TAU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ae993",
   "metadata": {},
   "source": [
    "## Configuration Flags\n",
    "\n",
    "Set these flags to control the pipeline behavior:\n",
    "- `ENABLE_FINETUNING`: Enable light finetuning of last transformer blocks\n",
    "- `USE_SOFT_ARGMAX`: Use window soft-argmax instead of argmax for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee3242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import subprocess\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "packages = [\n",
    "    'torch',\n",
    "    'torchvision', \n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'opencv-python',\n",
    "    'pillow',\n",
    "    'scipy',\n",
    "    'tqdm',\n",
    "    'pandas',\n",
    "    'scikit-learn'\n",
    "]\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', '--upgrade'] + packages)\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141df073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import importlib\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"✓ Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70efda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset path and import SPair dataset class\n",
    "repo_path = os.path.join(PROJECT_ROOT, 'SD4Match')\n",
    "\n",
    "if not os.path.exists(repo_path):\n",
    "    print(\"WARNING: SD4Match repository not found!\")\n",
    "    print(\"Please ensure the SD4Match repository is cloned in the project directory.\")\n",
    "else:\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "    print(f\"✓ SD4Match path added: {repo_path}\")\n",
    "\n",
    "# Import dataset class\n",
    "try:\n",
    "    module = importlib.import_module(\"dataset.spair\")\n",
    "    SPairDataset = getattr(module, \"SPairDataset\")\n",
    "    print(\"✓ SPairDataset class imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Import Error: {e}\")\n",
    "    print(\"Make sure SD4Match repository is properly cloned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8702dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SPair-71k dataset (if not already present)\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm import tqdm as tqdm_requests\n",
    "\n",
    "data_path = os.path.join(DATA_ROOT, 'SPair-71k')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading SPair-71k dataset...\")\n",
    "    url = \"http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
    "    tar_path = os.path.join(DATA_ROOT, 'SPair-71k.tar.gz')\n",
    "    \n",
    "    # Download with progress bar\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(tar_path, 'wb') as f, tqdm_requests(\n",
    "        desc='Downloading',\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = f.write(data)\n",
    "            pbar.update(size)\n",
    "    \n",
    "    print(\"\\nExtracting...\")\n",
    "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "        tar.extractall(DATA_ROOT)\n",
    "    \n",
    "    # Cleanup\n",
    "    os.remove(tar_path)\n",
    "    print(\"✓ Extraction complete\")\n",
    "else:\n",
    "    print(f\"✓ SPair-71k dataset already exists at {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for dataset\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        class DatasetConfig:\n",
    "            def __init__(self):\n",
    "                self.ROOT = DATA_ROOT\n",
    "                self.NAME = 'spair'\n",
    "                self.CATEGORY = 'cat'\n",
    "                self.SIZE = 224\n",
    "                self.IMG_SIZE = 224\n",
    "                self.MEAN = [0.485, 0.456, 0.406]\n",
    "                self.STD = [0.229, 0.224, 0.225]\n",
    "        self.DATASET = DatasetConfig()\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"✓ Configuration created\")\n",
    "print(f\"Dataset root: {cfg.DATASET.ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedd96a",
   "metadata": {},
   "source": [
    "## Section 2: DINOv2 Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DINOv2 ViT-B/14 model\n",
    "print(\"Loading DINOv2 ViT-B/14 model...\")\n",
    "\n",
    "try:\n",
    "    # Try loading from torch hub\n",
    "    dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "    dinov2_model = dinov2_model.to(device)\n",
    "    dinov2_model.eval()\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully!\")\n",
    "    print(f\"Model type: DINOv2 ViT-B/14\")\n",
    "    print(f\"Feature dimension: {dinov2_model.embed_dim}\")\n",
    "    print(f\"Patch size: 14x14\")\n",
    "    print(f\"Image size: 224x224 → {224//14}x{224//14} = 256 patches\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading DINOv2: {e}\")\n",
    "    print(\"Make sure you have internet connection for the first load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efd03e",
   "metadata": {},
   "source": [
    "## Section 3: Feature Extractor Class\n",
    "\n",
    "DINOv2 extracts dense patch features that capture semantic information. We'll create a feature extractor class that:\n",
    "- Extracts 16×16 grid of features (patch tokens)\n",
    "- Handles coordinate mapping between image and feature space\n",
    "- Provides L2-normalized features for similarity computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc26a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract dense spatial features from DINOv2 for correspondence matching.\n",
    "    \n",
    "    Features:\n",
    "    - Input: 224×224 RGB images\n",
    "    - Output: 16×16×768 feature maps (for ViT-B/14)\n",
    "    - Features are L2-normalized for cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Feature dimensions\n",
    "        self.patch_size = 14\n",
    "        self.img_size = 224\n",
    "        self.feat_h = self.feat_w = self.img_size // self.patch_size  # 16\n",
    "        \n",
    "    def extract_features(self, img):\n",
    "        \"\"\"\n",
    "        Extract dense feature map from an image.\n",
    "        \n",
    "        Args:\n",
    "            img: PIL Image or tensor [3, H, W]\n",
    "            \n",
    "        Returns:\n",
    "            features: torch.Tensor [1, feat_h*feat_w, dim]\n",
    "            features_2d: torch.Tensor [1, dim, feat_h, feat_w]\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        else:\n",
    "            img_tensor = img.unsqueeze(0).to(self.device) if img.dim() == 3 else img.to(self.device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            # Get patch tokens (excluding CLS token)\n",
    "            features_dict = self.model.forward_features(img_tensor)\n",
    "            features = features_dict['x_norm_patchtokens']  # [1, num_patches, dim]\n",
    "            \n",
    "            # Reshape to spatial grid\n",
    "            B, N, D = features.shape\n",
    "            features_2d = features.reshape(B, self.feat_h, self.feat_w, D)\n",
    "            features_2d = features_2d.permute(0, 3, 1, 2)  # [1, dim, feat_h, feat_w]\n",
    "            \n",
    "            # L2 normalize for cosine similarity\n",
    "            features = F.normalize(features, p=2, dim=-1)\n",
    "            features_2d = F.normalize(features_2d, p=2, dim=1)\n",
    "        \n",
    "        return features, features_2d\n",
    "    \n",
    "    def extract_keypoint_features(self, img, keypoints):\n",
    "        \"\"\"\n",
    "        Extract features at specific keypoint locations.\n",
    "        \n",
    "        Args:\n",
    "            img: PIL Image or tensor\n",
    "            keypoints: numpy array [N, 2] in image coordinates (x, y)\n",
    "            \n",
    "        Returns:\n",
    "            kp_features: torch.Tensor [N, dim]\n",
    "        \"\"\"\n",
    "        _, features_2d = self.extract_features(img)  # [1, dim, feat_h, feat_w]\n",
    "        \n",
    "        # Map image coordinates to feature coordinates\n",
    "        feat_coords = self.map_coords_to_features(keypoints)\n",
    "        \n",
    "        # Extract features using bilinear interpolation\n",
    "        kp_features = []\n",
    "        for x, y in feat_coords:\n",
    "            if 0 <= x < self.feat_w and 0 <= y < self.feat_h:\n",
    "                # Use bilinear interpolation for sub-pixel accuracy\n",
    "                x0, y0 = int(np.floor(x)), int(np.floor(y))\n",
    "                x1, y1 = min(x0 + 1, self.feat_w - 1), min(y0 + 1, self.feat_h - 1)\n",
    "                \n",
    "                # Interpolation weights\n",
    "                wx = x - x0\n",
    "                wy = y - y0\n",
    "                \n",
    "                # Bilinear interpolation\n",
    "                feat = (1 - wx) * (1 - wy) * features_2d[0, :, y0, x0] + \\\n",
    "                       wx * (1 - wy) * features_2d[0, :, y0, x1] + \\\n",
    "                       (1 - wx) * wy * features_2d[0, :, y1, x0] + \\\n",
    "                       wx * wy * features_2d[0, :, y1, x1]\n",
    "                \n",
    "                kp_features.append(feat)\n",
    "            else:\n",
    "                # Out of bounds - use zero vector\n",
    "                kp_features.append(torch.zeros(features_2d.shape[1], device=self.device))\n",
    "        \n",
    "        kp_features = torch.stack(kp_features)\n",
    "        kp_features = F.normalize(kp_features, p=2, dim=-1)\n",
    "        \n",
    "        return kp_features\n",
    "    \n",
    "    def map_coords_to_features(self, coords):\n",
    "        \"\"\"\n",
    "        Map image coordinates to feature map coordinates.\n",
    "        \n",
    "        Args:\n",
    "            coords: numpy array [N, 2] in image space (x, y)\n",
    "            \n",
    "        Returns:\n",
    "            feat_coords: numpy array [N, 2] in feature space\n",
    "        \"\"\"\n",
    "        scale_x = self.feat_w / self.img_size\n",
    "        scale_y = self.feat_h / self.img_size\n",
    "        \n",
    "        feat_coords = coords.copy()\n",
    "        feat_coords[:, 0] = coords[:, 0] * scale_x\n",
    "        feat_coords[:, 1] = coords[:, 1] * scale_y\n",
    "        \n",
    "        return feat_coords\n",
    "    \n",
    "    def map_features_to_coords(self, feat_coords):\n",
    "        \"\"\"\n",
    "        Map feature coordinates back to image space.\n",
    "        \n",
    "        Args:\n",
    "            feat_coords: numpy array [N, 2] in feature space\n",
    "            \n",
    "        Returns:\n",
    "            img_coords: numpy array [N, 2] in image space (x, y)\n",
    "        \"\"\"\n",
    "        scale_x = self.img_size / self.feat_w\n",
    "        scale_y = self.img_size / self.feat_h\n",
    "        \n",
    "        img_coords = feat_coords.copy()\n",
    "        img_coords[:, 0] = feat_coords[:, 0] * scale_x\n",
    "        img_coords[:, 1] = feat_coords[:, 1] * scale_y\n",
    "        \n",
    "        return img_coords\n",
    "\n",
    "# Create feature extractor\n",
    "feature_extractor = DINOv2FeatureExtractor(dinov2_model, device)\n",
    "print(\"✓ Feature extractor created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c8ef9",
   "metadata": {},
   "source": [
    "## Section 4: Test Feature Extraction with Visualization\n",
    "\n",
    "Let's test the feature extractor on a sample image pair and visualize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e4f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "print(\"Loading SPair-71k test dataset...\")\n",
    "try:\n",
    "    dataset = SPairDataset(cfg, 'test', 'cat')\n",
    "    print(f\"✓ Successfully loaded {len(dataset)} test pairs\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Make sure dataset is properly extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample and extract features\n",
    "sample_idx = 0\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "print(f\"Extracting features for sample {sample_idx}...\")\n",
    "\n",
    "# Get images and keypoints\n",
    "src_img = sample['src_img']\n",
    "tgt_img = sample['trg_img']\n",
    "src_kps = sample['src_kps']\n",
    "tgt_kps = sample['trg_kps']\n",
    "\n",
    "print(f\"Source keypoints shape: {src_kps.shape}\")\n",
    "print(f\"Target keypoints shape: {tgt_kps.shape}\")\n",
    "\n",
    "# Extract dense features\n",
    "src_features, src_features_2d = feature_extractor.extract_features(src_img)\n",
    "tgt_features, tgt_features_2d = feature_extractor.extract_features(tgt_img)\n",
    "\n",
    "print(f\"\\nFeature shapes:\")\n",
    "print(f\"Source features: {src_features.shape}\")  # [1, 256, 768]\n",
    "print(f\"Source features 2D: {src_features_2d.shape}\")  # [1, 768, 16, 16]\n",
    "print(f\"Target features: {tgt_features.shape}\")\n",
    "print(f\"Target features 2D: {tgt_features_2d.shape}\")\n",
    "\n",
    "# Denormalize function for visualization\n",
    "def denorm_show(img_tensor):\n",
    "    img = img_tensor.permute(1, 2, 0).numpy()\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    return np.clip(img, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc363a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_FINETUNING:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LIGHT FINETUNING ENABLED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Path to SPair-71k\n",
    "    SPAIR_ROOT = os.path.join(DATA_ROOT, 'SPair-71k')\n",
    "    \n",
    "    if not os.path.exists(SPAIR_ROOT):\n",
    "        print(f\"\\n⚠️  SPair-71k not found at: {SPAIR_ROOT}\")\n",
    "        print(\"   Please download SPair-71k and place it in the data directory\")\n",
    "        print(\"   Download from: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
    "    else:\n",
    "        # Create dataloaders\n",
    "        print(f\"\\nLoading SPair-71k from: {SPAIR_ROOT}\")\n",
    "        train_loader, val_loader = create_spair_dataloaders(\n",
    "            SPAIR_ROOT,\n",
    "            batch_size=FINETUNE_BATCH_SIZE,\n",
    "            num_workers=2,\n",
    "            train_subset=FINETUNE_TRAIN_SUBSET,\n",
    "            val_subset=500  # Use 500 samples for validation\n",
    "        )\n",
    "        \n",
    "        # Unfreeze last k blocks\n",
    "        trainable_params = unfreeze_last_k_blocks(dinov2_model, FINETUNE_K_LAYERS, blocks_attr='blocks')\n",
    "        \n",
    "        # Setup optimizer\n",
    "        optimizer = torch.optim.AdamW(trainable_params, lr=FINETUNE_LR, weight_decay=FINETUNE_WD)\n",
    "        \n",
    "        print(f\"\\nFinetuning configuration:\")\n",
    "        print(f\"  k={FINETUNE_K_LAYERS} layers\")\n",
    "        print(f\"  lr={FINETUNE_LR}, wd={FINETUNE_WD}\")\n",
    "        print(f\"  epochs={FINETUNE_EPOCHS}\")\n",
    "        print(f\"  batch_size={FINETUNE_BATCH_SIZE}\")\n",
    "        print(f\"  train samples: {len(train_loader.dataset)}\")\n",
    "        print(f\"  val samples: {len(val_loader.dataset)}\")\n",
    "        \n",
    "        # Get patch size from model\n",
    "        patch_size = 14  # DINOv2 ViT-B/14\n",
    "        \n",
    "        # Training loop\n",
    "        dinov2_model.train()\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(FINETUNE_EPOCHS):\n",
    "            # Training\n",
    "            epoch_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                src_img = batch['src_img'].cuda()\n",
    "                tgt_img = batch['tgt_img'].cuda()\n",
    "                src_kps = batch['src_kps'].cuda()\n",
    "                tgt_kps = batch['tgt_kps'].cuda()\n",
    "                \n",
    "                # Extract features from model\n",
    "                with torch.no_grad() if epoch == 0 else torch.enable_grad():\n",
    "                    # Get patch tokens\n",
    "                    src_out = dinov2_model.forward_features(src_img)\n",
    "                    tgt_out = dinov2_model.forward_features(tgt_img)\n",
    "                    \n",
    "                    # Extract patch tokens (skip CLS token)\n",
    "                    if isinstance(src_out, dict):\n",
    "                        src_feats = src_out['x_norm_patchtokens']\n",
    "                        tgt_feats = tgt_out['x_norm_patchtokens']\n",
    "                    else:\n",
    "                        src_feats = src_out[:, 1:, :]  # Skip CLS\n",
    "                        tgt_feats = tgt_out[:, 1:, :]\n",
    "                \n",
    "                B = src_img.size(0)\n",
    "                # Calculate grid size\n",
    "                num_patches = src_feats.size(1)\n",
    "                grid_size = int(np.sqrt(num_patches))\n",
    "                Hs = Ws = Ht = Wt = grid_size\n",
    "                \n",
    "                # Compute loss over keypoints\n",
    "                batch_loss = 0.0\n",
    "                num_kps = 0\n",
    "                \n",
    "                for b in range(B):\n",
    "                    src_f = src_feats[b]  # [Ns, C]\n",
    "                    tgt_f = tgt_feats[b]  # [Nt, C]\n",
    "                    \n",
    "                    # Normalize features\n",
    "                    src_f = F.normalize(src_f, dim=-1)\n",
    "                    tgt_f = F.normalize(tgt_f, dim=-1)\n",
    "                    \n",
    "                    # For each source keypoint\n",
    "                    N_kp = src_kps[b].size(0)\n",
    "                    for kp_idx in range(N_kp):\n",
    "                        # Map keypoint to patch index\n",
    "                        src_x = src_kps[b][kp_idx, 0].item()\n",
    "                        src_y = src_kps[b][kp_idx, 1].item()\n",
    "                        \n",
    "                        # Convert to patch coordinates\n",
    "                        src_px = int(src_x / patch_size)\n",
    "                        src_py = int(src_y / patch_size)\n",
    "                        \n",
    "                        # Clamp to valid range\n",
    "                        src_px = max(0, min(Ws - 1, src_px))\n",
    "                        src_py = max(0, min(Hs - 1, src_py))\n",
    "                        \n",
    "                        src_patch_idx = src_py * Ws + src_px\n",
    "                        \n",
    "                        # Compute similarity with target\n",
    "                        sim = torch.matmul(tgt_f, src_f[src_patch_idx])  # [Nt]\n",
    "                        sim_2d = sim.view(Ht, Wt)\n",
    "                        \n",
    "                        # Compute loss\n",
    "                        gt_xy = tgt_kps[b][kp_idx]  # [2] in (x, y) format\n",
    "                        gt_yx = torch.stack([gt_xy[1], gt_xy[0]])  # Convert to (y, x)\n",
    "                        \n",
    "                        loss = compute_keypoint_loss(\n",
    "                            sim_2d, Ht, Wt, gt_yx, patch_size,\n",
    "                            use_soft=True, window=SOFT_WINDOW, tau=SOFT_TAU\n",
    "                        )\n",
    "                        batch_loss += loss\n",
    "                        num_kps += 1\n",
    "                \n",
    "                if num_kps > 0:\n",
    "                    batch_loss = batch_loss / num_kps\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_loss += batch_loss.item()\n",
    "                    num_batches += 1\n",
    "                \n",
    "                if (batch_idx + 1) % 50 == 0:\n",
    "                    avg_batch_loss = epoch_loss / max(1, num_batches)\n",
    "                    print(f\"  Epoch {epoch+1}/{FINETUNE_EPOCHS}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {avg_batch_loss:.4f}\")\n",
    "            \n",
    "            avg_train_loss = epoch_loss / max(1, num_batches)\n",
    "            \n",
    "            # Validation\n",
    "            dinov2_model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    src_img = batch['src_img'].cuda()\n",
    "                    tgt_img = batch['tgt_img'].cuda()\n",
    "                    src_kps = batch['src_kps'].cuda()\n",
    "                    tgt_kps = batch['tgt_kps'].cuda()\n",
    "                    \n",
    "                    # Extract features\n",
    "                    src_out = dinov2_model.forward_features(src_img)\n",
    "                    tgt_out = dinov2_model.forward_features(tgt_img)\n",
    "                    \n",
    "                    if isinstance(src_out, dict):\n",
    "                        src_feats = src_out['x_norm_patchtokens']\n",
    "                        tgt_feats = tgt_out['x_norm_patchtokens']\n",
    "                    else:\n",
    "                        src_feats = src_out[:, 1:, :]\n",
    "                        tgt_feats = tgt_out[:, 1:, :]\n",
    "                    \n",
    "                    num_patches = src_feats.size(1)\n",
    "                    grid_size = int(np.sqrt(num_patches))\n",
    "                    \n",
    "                    # Compute validation loss (same as training)\n",
    "                    batch_val_loss = 0.0\n",
    "                    num_kps = 0\n",
    "                    \n",
    "                    for b in range(src_img.size(0)):\n",
    "                        src_f = F.normalize(src_feats[b], dim=-1)\n",
    "                        tgt_f = F.normalize(tgt_feats[b], dim=-1)\n",
    "                        \n",
    "                        for kp_idx in range(src_kps[b].size(0)):\n",
    "                            src_x = int(src_kps[b][kp_idx, 0].item() / patch_size)\n",
    "                            src_y = int(src_kps[b][kp_idx, 1].item() / patch_size)\n",
    "                            src_x = max(0, min(grid_size - 1, src_x))\n",
    "                            src_y = max(0, min(grid_size - 1, src_y))\n",
    "                            \n",
    "                            src_patch_idx = src_y * grid_size + src_x\n",
    "                            sim = torch.matmul(tgt_f, src_f[src_patch_idx])\n",
    "                            sim_2d = sim.view(grid_size, grid_size)\n",
    "                            \n",
    "                            gt_xy = tgt_kps[b][kp_idx]\n",
    "                            gt_yx = torch.stack([gt_xy[1], gt_xy[0]])\n",
    "                            \n",
    "                            loss = compute_keypoint_loss(\n",
    "                                sim_2d, grid_size, grid_size, gt_yx, patch_size,\n",
    "                                use_soft=True, window=SOFT_WINDOW, tau=SOFT_TAU\n",
    "                            )\n",
    "                            batch_val_loss += loss\n",
    "                            num_kps += 1\n",
    "                    \n",
    "                    if num_kps > 0:\n",
    "                        val_loss += (batch_val_loss / num_kps).item()\n",
    "                        val_batches += 1\n",
    "            \n",
    "            avg_val_loss = val_loss / max(1, val_batches)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}/{FINETUNE_EPOCHS} Summary:\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                checkpoint_path = os.path.join(OUTPUT_DIR, f'dinov2_finetuned_k{FINETUNE_K_LAYERS}_best.pth')\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': dinov2_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'config': {\n",
    "                        'k_layers': FINETUNE_K_LAYERS,\n",
    "                        'lr': FINETUNE_LR,\n",
    "                        'wd': FINETUNE_WD,\n",
    "                        'soft_window': SOFT_WINDOW,\n",
    "                        'soft_tau': SOFT_TAU\n",
    "                    }\n",
    "                }, checkpoint_path)\n",
    "                print(f\"  ✓ Best model saved to: {checkpoint_path}\")\n",
    "            \n",
    "            dinov2_model.train()\n",
    "        \n",
    "        dinov2_model.eval()\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINETUNING COMPLETED!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Finetuning disabled. Using pretrained weights only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f033fa2",
   "metadata": {},
   "source": [
    "## Light Finetuning (Optional)\n",
    "\n",
    "If `ENABLE_FINETUNING=True`, this section finetunes the last k transformer blocks on SPair-71k with keypoint supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images with keypoints\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "# Source image\n",
    "axes[0].imshow(denorm_show(src_img))\n",
    "valid_src_kps = src_kps[src_kps[:, 0] >= 0]\n",
    "axes[0].scatter(valid_src_kps[:, 0], valid_src_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3, label='Keypoints')\n",
    "axes[0].set_title(f'Source Image\\n{len(valid_src_kps)} keypoints', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "axes[0].legend()\n",
    "\n",
    "# Target image\n",
    "axes[1].imshow(denorm_show(tgt_img))\n",
    "valid_tgt_kps = tgt_kps[tgt_kps[:, 0] >= 0]\n",
    "axes[1].scatter(valid_tgt_kps[:, 0], valid_tgt_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3, label='Keypoints')\n",
    "axes[1].set_title(f'Target Image\\n{len(valid_tgt_kps)} keypoints', fontsize=12, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'sample_images_with_keypoints.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualization saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b67112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DINOv2 features using PCA\n",
    "print(\"Visualizing DINOv2 features with PCA...\")\n",
    "\n",
    "# Get features as numpy arrays\n",
    "src_feat_np = src_features[0].cpu().numpy()  # [256, 768]\n",
    "tgt_feat_np = tgt_features[0].cpu().numpy()  # [256, 768]\n",
    "\n",
    "# Apply PCA to reduce to 3 components for RGB visualization\n",
    "pca = PCA(n_components=3)\n",
    "src_pca = pca.fit_transform(src_feat_np)  # [256, 3]\n",
    "tgt_pca = pca.transform(tgt_feat_np)  # [256, 3]\n",
    "\n",
    "# Reshape to spatial grid\n",
    "src_pca_img = src_pca.reshape(16, 16, 3)\n",
    "tgt_pca_img = tgt_pca.reshape(16, 16, 3)\n",
    "\n",
    "# Normalize to [0, 1] for visualization\n",
    "src_pca_img = (src_pca_img - src_pca_img.min()) / (src_pca_img.max() - src_pca_img.min())\n",
    "tgt_pca_img = (tgt_pca_img - tgt_pca_img.min()) / (tgt_pca_img.max() - tgt_pca_img.min())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Original images\n",
    "axes[0, 0].imshow(denorm_show(src_img))\n",
    "axes[0, 0].set_title('Source Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(denorm_show(tgt_img))\n",
    "axes[0, 1].set_title('Target Image', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# PCA visualizations\n",
    "axes[1, 0].imshow(src_pca_img)\n",
    "axes[1, 0].set_title(f'Source DINOv2 Features (PCA)\\nExplained variance: {pca.explained_variance_ratio_.sum():.2%}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(tgt_pca_img)\n",
    "axes[1, 1].set_title(f'Target DINOv2 Features (PCA)\\nExplained variance: {pca.explained_variance_ratio_.sum():.2%}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'pca_feature_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ PCA captures {pca.explained_variance_ratio_.sum():.2%} of feature variance\")\n",
    "print(\"Colors represent semantic regions learned by DINOv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute patch-to-patch similarity matrix\n",
    "print(\"Computing patch-to-patch similarity...\")\n",
    "\n",
    "# Normalize features for cosine similarity\n",
    "src_feat_norm = F.normalize(torch.from_numpy(src_feat_np), dim=1)  # [256, 768]\n",
    "tgt_feat_norm = F.normalize(torch.from_numpy(tgt_feat_np), dim=1)  # [256, 768]\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = torch.mm(src_feat_norm, tgt_feat_norm.t()).numpy()  # [256, 256]\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Source image\n",
    "axes[0].imshow(denorm_show(src_img))\n",
    "axes[0].scatter(valid_src_kps[:, 0], valid_src_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3)\n",
    "axes[0].set_title(f'Source Image\\n({len(valid_src_kps)} keypoints)', fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Similarity heatmap\n",
    "im = axes[1].imshow(similarity_matrix, cmap='hot', aspect='auto')\n",
    "axes[1].set_title(f'Patch-to-Patch Similarity\\n(DINOv2 Features)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Target Patches')\n",
    "axes[1].set_ylabel('Source Patches')\n",
    "plt.colorbar(im, ax=axes[1], label='Cosine Similarity')\n",
    "\n",
    "# Target image\n",
    "axes[2].imshow(denorm_show(tgt_img))\n",
    "axes[2].scatter(valid_tgt_kps[:, 0], valid_tgt_kps[:, 1], \n",
    "               c='red', s=100, marker='x', linewidths=3)\n",
    "axes[2].set_title(f'Target Image\\n({len(valid_tgt_kps)} keypoints)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'similarity_heatmap.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSimilarity statistics:\")\n",
    "print(f\"  Mean: {similarity_matrix.mean():.4f}\")\n",
    "print(f\"  Max: {similarity_matrix.max():.4f}\")\n",
    "print(f\"  Min: {similarity_matrix.min():.4f}\")\n",
    "print(f\"  High similarity (>0.9): {(similarity_matrix > 0.9).sum()} patch pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1de480",
   "metadata": {},
   "source": [
    "## Section 5: Correspondence Matcher\n",
    "\n",
    "Now let's implement the correspondence matcher that finds matching keypoints between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac370531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrespondenceMatcher:\n",
    "    \"\"\"\n",
    "    Find correspondences between source and target images using feature similarity.\n",
    "    \n",
    "    Methods:\n",
    "    - Nearest Neighbor (NN): Find target point with highest similarity\n",
    "    - Mutual Nearest Neighbors (MNN): Enforce bidirectional consistency\n",
    "    - Ratio Test: Reject ambiguous matches (Lowe's ratio test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mutual_nn=False, ratio_threshold=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mutual_nn: If True, only keep mutual nearest neighbors\n",
    "            ratio_threshold: If set, apply ratio test (e.g., 0.8)\n",
    "        \"\"\"\n",
    "        self.mutual_nn = mutual_nn\n",
    "        self.ratio_threshold = ratio_threshold\n",
    "    \n",
    "    def match_keypoints(self, src_features, tgt_features_2d, src_keypoints, \n",
    "                       feature_extractor):\n",
    "        \"\"\"\n",
    "        Find correspondences for source keypoints in target image.\n",
    "        \n",
    "        Args:\n",
    "            src_features: Source keypoint features [N, dim]\n",
    "            tgt_features_2d: Target dense features [1, dim, H, W]\n",
    "            src_keypoints: Source keypoint coordinates [N, 2]\n",
    "            feature_extractor: Feature extractor instance\n",
    "            \n",
    "        Returns:\n",
    "            pred_keypoints: Predicted target coordinates [N, 2]\n",
    "            confidences: Match confidence scores [N]\n",
    "        \"\"\"\n",
    "        N = src_features.shape[0]\n",
    "        _, D, H, W = tgt_features_2d.shape\n",
    "        \n",
    "        # Reshape target features to [H*W, D]\n",
    "        tgt_features_flat = tgt_features_2d.reshape(D, H * W).t()  # [H*W, D]\n",
    "        \n",
    "        # Compute similarity: [N, H*W]\n",
    "        similarities = torch.mm(src_features, tgt_features_flat.t())\n",
    "        \n",
    "        # Find best matches\n",
    "        max_sims, max_indices = similarities.max(dim=1)\n",
    "        \n",
    "        # Apply ratio test if specified\n",
    "        if self.ratio_threshold is not None:\n",
    "            # Get second best matches\n",
    "            sorted_sims, _ = similarities.sort(dim=1, descending=True)\n",
    "            ratios = sorted_sims[:, 0] / (sorted_sims[:, 1] + 1e-8)\n",
    "            \n",
    "            # Mark low-confidence matches\n",
    "            valid_mask = ratios > self.ratio_threshold\n",
    "            max_sims = max_sims * valid_mask.float()\n",
    "        \n",
    "        # Convert flat indices to 2D coordinates\n",
    "        pred_y = (max_indices // W).float()\n",
    "        pred_x = (max_indices % W).float()\n",
    "        pred_coords_feat = torch.stack([pred_x, pred_y], dim=1).cpu().numpy()\n",
    "        \n",
    "        # Map to image coordinates\n",
    "        pred_keypoints = feature_extractor.map_features_to_coords(pred_coords_feat)\n",
    "        confidences = max_sims.cpu().numpy()\n",
    "        \n",
    "        # Apply mutual nearest neighbors if specified\n",
    "        if self.mutual_nn:\n",
    "            # Find reverse matches (target → source)\n",
    "            reverse_sims = similarities.t()  # [H*W, N]\n",
    "            _, reverse_indices = reverse_sims.max(dim=1)\n",
    "            \n",
    "            # Check mutual consistency\n",
    "            forward_indices = max_indices.cpu().numpy()\n",
    "            reverse_map = reverse_indices.cpu().numpy()\n",
    "            \n",
    "            for i in range(N):\n",
    "                target_idx = forward_indices[i]\n",
    "                if reverse_map[target_idx] != i:\n",
    "                    # Not mutually consistent\n",
    "                    confidences[i] = 0.0\n",
    "        \n",
    "        return pred_keypoints, confidences\n",
    "    \n",
    "    def match_images(self, src_img, tgt_img, src_keypoints, feature_extractor):\n",
    "        \"\"\"\n",
    "        Complete matching pipeline for an image pair.\n",
    "        \n",
    "        Args:\n",
    "            src_img: Source image tensor\n",
    "            tgt_img: Target image tensor\n",
    "            src_keypoints: Source keypoint coordinates [N, 2]\n",
    "            feature_extractor: Feature extractor instance\n",
    "            \n",
    "        Returns:\n",
    "            pred_keypoints: Predicted target keypoints [N, 2]\n",
    "            confidences: Match confidence scores [N]\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        src_kp_features = feature_extractor.extract_keypoint_features(src_img, src_keypoints)\n",
    "        _, tgt_features_2d = feature_extractor.extract_features(tgt_img)\n",
    "        \n",
    "        # Match\n",
    "        pred_keypoints, confidences = self.match_keypoints(\n",
    "            src_kp_features, tgt_features_2d, src_keypoints, feature_extractor\n",
    "        )\n",
    "        \n",
    "        return pred_keypoints, confidences\n",
    "\n",
    "# Create matcher\n",
    "matcher = CorrespondenceMatcher(mutual_nn=False, ratio_threshold=None)\n",
    "print(\"✓ Correspondence matcher created\")\n",
    "print(f\"  Mutual NN: {matcher.mutual_nn}\")\n",
    "print(f\"  Ratio test: {matcher.ratio_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136c491",
   "metadata": {},
   "source": [
    "## Section 6: PCK Evaluator\n",
    "\n",
    "Implement PCK (Percentage of Correct Keypoints) evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCKEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate correspondence quality using PCK (Percentage of Correct Keypoints).\n",
    "    \n",
    "    A keypoint is correct if:\n",
    "        ||predicted - ground_truth|| ≤ α × bbox_diagonal\n",
    "    \n",
    "    Standard thresholds: α ∈ {0.05, 0.10, 0.15}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_values=[0.05, 0.10, 0.15]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha_values: List of PCK thresholds\n",
    "        \"\"\"\n",
    "        self.alpha_values = alpha_values\n",
    "    \n",
    "    def compute_pck(self, pred_kps, gt_kps, bbox=None, img_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        Compute PCK for a single image pair.\n",
    "        \n",
    "        Args:\n",
    "            pred_kps: Predicted keypoints [N, 2]\n",
    "            gt_kps: Ground truth keypoints [N, 2]\n",
    "            bbox: Bounding box [4] as [x1, y1, x2, y2] (optional)\n",
    "            img_size: Image size (H, W) for normalization if no bbox\n",
    "            \n",
    "        Returns:\n",
    "            pck_dict: Dictionary with PCK@alpha for each threshold\n",
    "            distances: Normalized distances for each keypoint\n",
    "        \"\"\"\n",
    "        # Filter valid keypoints (ground truth with positive coordinates)\n",
    "        valid_mask = (gt_kps[:, 0] >= 0) & (gt_kps[:, 1] >= 0)\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            # No valid keypoints\n",
    "            return {f'pck@{alpha:.2f}': 0.0 for alpha in self.alpha_values}, np.array([])\n",
    "        \n",
    "        pred_valid = pred_kps[valid_mask]\n",
    "        gt_valid = gt_kps[valid_mask]\n",
    "        \n",
    "        # Compute distances\n",
    "        distances = np.linalg.norm(pred_valid - gt_valid, axis=1)\n",
    "        \n",
    "        # Compute normalization factor\n",
    "        if bbox is not None:\n",
    "            # Use bounding box diagonal\n",
    "            bbox_w = bbox[2] - bbox[0]\n",
    "            bbox_h = bbox[3] - bbox[1]\n",
    "            norm_factor = np.sqrt(bbox_w ** 2 + bbox_h ** 2)\n",
    "        else:\n",
    "            # Use image diagonal\n",
    "            norm_factor = np.sqrt(img_size[0] ** 2 + img_size[1] ** 2)\n",
    "        \n",
    "        # Normalize distances\n",
    "        normalized_distances = distances / (norm_factor + 1e-8)\n",
    "        \n",
    "        # Compute PCK for each threshold\n",
    "        pck_dict = {}\n",
    "        for alpha in self.alpha_values:\n",
    "            correct = (normalized_distances <= alpha).sum()\n",
    "            pck = correct / len(normalized_distances)\n",
    "            pck_dict[f'pck@{alpha:.2f}'] = pck\n",
    "        \n",
    "        return pck_dict, normalized_distances\n",
    "    \n",
    "    def evaluate_dataset(self, predictions, ground_truths, bboxes=None):\n",
    "        \"\"\"\n",
    "        Evaluate PCK over entire dataset.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of predicted keypoints [N_samples, N_kps, 2]\n",
    "            ground_truths: List of ground truth keypoints [N_samples, N_kps, 2]\n",
    "            bboxes: List of bounding boxes (optional)\n",
    "            \n",
    "        Returns:\n",
    "            avg_pck: Dictionary with average PCK across dataset\n",
    "            per_sample_pck: List of per-sample PCK dictionaries\n",
    "        \"\"\"\n",
    "        per_sample_pck = []\n",
    "        all_distances = []\n",
    "        \n",
    "        for i in range(len(predictions)):\n",
    "            bbox = bboxes[i] if bboxes is not None else None\n",
    "            pck_dict, distances = self.compute_pck(predictions[i], ground_truths[i], bbox)\n",
    "            per_sample_pck.append(pck_dict)\n",
    "            all_distances.extend(distances.tolist())\n",
    "        \n",
    "        # Compute average PCK\n",
    "        avg_pck = {}\n",
    "        for alpha in self.alpha_values:\n",
    "            key = f'pck@{alpha:.2f}'\n",
    "            avg_pck[key] = np.mean([sample[key] for sample in per_sample_pck])\n",
    "        \n",
    "        return avg_pck, per_sample_pck, np.array(all_distances)\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = PCKEvaluator(alpha_values=[0.05, 0.10, 0.15])\n",
    "print(\"✓ PCK evaluator created\")\n",
    "print(f\"  Alpha values: {evaluator.alpha_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4e7a6",
   "metadata": {},
   "source": [
    "## Section 7: Test on Sample\n",
    "\n",
    "Let's test the complete pipeline on our sample image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd176449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match keypoints for the sample\n",
    "print(\"Matching keypoints for sample...\")\n",
    "\n",
    "# Get valid keypoints\n",
    "valid_src_mask = (src_kps[:, 0] >= 0) & (src_kps[:, 1] >= 0)\n",
    "valid_src_kps = src_kps[valid_src_mask]\n",
    "\n",
    "print(f\"Source keypoints: {len(valid_src_kps)}\")\n",
    "\n",
    "# Perform matching\n",
    "pred_kps, confidences = matcher.match_images(\n",
    "    src_img, tgt_img, valid_src_kps, feature_extractor\n",
    ")\n",
    "\n",
    "print(f\"Predicted keypoints: {pred_kps.shape}\")\n",
    "print(f\"Confidences: min={confidences.min():.3f}, max={confidences.max():.3f}, mean={confidences.mean():.3f}\")\n",
    "\n",
    "# Evaluate\n",
    "valid_tgt_kps = tgt_kps[valid_src_mask]\n",
    "pck_dict, distances = evaluator.compute_pck(pred_kps, valid_tgt_kps)\n",
    "\n",
    "print(\"\\nPCK Results:\")\n",
    "for key, value in pck_dict.items():\n",
    "    print(f\"  {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDistance statistics:\")\n",
    "print(f\"  Min: {distances.min():.4f}\")\n",
    "print(f\"  Max: {distances.max():.4f}\")\n",
    "print(f\"  Mean: {distances.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(distances):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrespondenceMatcher:\n",
    "    \"\"\"\n",
    "    Keypoint correspondence matcher using feature similarity.\n",
    "    \n",
    "    Methods:\n",
    "    - Nearest Neighbor (NN): Find target point with highest similarity\n",
    "    - Mutual Nearest Neighbors (MNN): Enforce bidirectional consistency\n",
    "    - Ratio Test: Reject ambiguous matches (Lowe's ratio test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mutual_nn=False, ratio_threshold=None, use_soft_argmax=False, \n",
    "                 soft_window=7, soft_tau=0.05):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mutual_nn: If True, only keep mutual nearest neighbors\n",
    "            ratio_threshold: If set, apply ratio test (e.g., 0.8)\n",
    "            use_soft_argmax: If True, use window soft-argmax for sub-pixel refinement\n",
    "            soft_window: Window size for soft-argmax (odd number)\n",
    "            soft_tau: Temperature for soft-argmax\n",
    "        \"\"\"\n",
    "        self.mutual_nn = mutual_nn\n",
    "        self.ratio_threshold = ratio_threshold\n",
    "        self.use_soft_argmax = use_soft_argmax\n",
    "        self.soft_window = soft_window\n",
    "        self.soft_tau = soft_tau\n",
    "    \n",
    "    def match_keypoints(self, src_features, tgt_features_2d, src_keypoints, \n",
    "                       feature_extractor):\n",
    "        \"\"\"\n",
    "        Find correspondences for source keypoints in target image.\n",
    "        \n",
    "        Args:\n",
    "            src_features: Source keypoint features [N, dim]\n",
    "            tgt_features_2d: Target dense features [1, dim, H, W]\n",
    "            src_keypoints: Source keypoint coordinates [N, 2]\n",
    "            feature_extractor: Feature extractor instance\n",
    "            \n",
    "        Returns:\n",
    "            pred_keypoints: Predicted target coordinates [N, 2]\n",
    "            confidences: Match confidence scores [N]\n",
    "        \"\"\"\n",
    "        N = src_features.shape[0]\n",
    "        _, D, H, W = tgt_features_2d.shape\n",
    "        \n",
    "        # Reshape target features to [H*W, D]\n",
    "        tgt_features_flat = tgt_features_2d.reshape(D, H * W).t()  # [H*W, D]\n",
    "        \n",
    "        # Compute similarity: [N, H*W]\n",
    "        similarities = torch.mm(src_features, tgt_features_flat.t())\n",
    "        \n",
    "        # Find best matches\n",
    "        if self.use_soft_argmax:\n",
    "            # Use window soft-argmax for sub-pixel refinement\n",
    "            pred_coords_patch = window_soft_argmax(\n",
    "                similarities, H, W, \n",
    "                window=self.soft_window, \n",
    "                tau=self.soft_tau\n",
    "            )  # [N, 2] in (y, x) patch coordinates\n",
    "            \n",
    "            pred_y = pred_coords_patch[:, 0]\n",
    "            pred_x = pred_coords_patch[:, 1]\n",
    "            \n",
    "            # Get confidence from peak similarity\n",
    "            max_sims, _ = similarities.max(dim=1)\n",
    "        else:\n",
    "            # Standard argmax\n",
    "            max_sims, max_indices = similarities.max(dim=1)\n",
    "            \n",
    "            # Convert flat indices to 2D coordinates\n",
    "            pred_y = (max_indices // W).float()\n",
    "            pred_x = (max_indices % W).float()\n",
    "        \n",
    "        # Apply ratio test if specified\n",
    "        if self.ratio_threshold is not None:\n",
    "            # Get second best matches\n",
    "            sorted_sims, _ = similarities.sort(dim=1, descending=True)\n",
    "            ratios = sorted_sims[:, 0] / (sorted_sims[:, 1] + 1e-8)\n",
    "            \n",
    "            # Mark low-confidence matches\n",
    "            valid_mask = ratios > self.ratio_threshold\n",
    "            max_sims = max_sims * valid_mask.float()\n",
    "        \n",
    "        pred_coords_feat = torch.stack([pred_x, pred_y], dim=1).cpu().numpy()\n",
    "        \n",
    "        # Map to image coordinates\n",
    "        pred_keypoints = feature_extractor.map_features_to_coords(pred_coords_feat)\n",
    "        confidences = max_sims.cpu().numpy()\n",
    "        \n",
    "        # Apply mutual nearest neighbors if specified\n",
    "        if self.mutual_nn:\n",
    "            # Find reverse matches (target → source)\n",
    "            reverse_sims = similarities.t()  # [H*W, N]\n",
    "            _, reverse_indices = reverse_sims.max(dim=1)\n",
    "            \n",
    "            # Check mutual consistency (only for argmax)\n",
    "            if not self.use_soft_argmax:\n",
    "                forward_indices = (pred_y * W + pred_x).long().cpu().numpy()\n",
    "                reverse_map = reverse_indices.cpu().numpy()\n",
    "                \n",
    "                for i in range(N):\n",
    "                    target_idx = forward_indices[i]\n",
    "                    if reverse_map[target_idx] != i:\n",
    "                        # Not mutually consistent\n",
    "                        confidences[i] = 0.0\n",
    "        \n",
    "        return pred_keypoints, confidences\n",
    "    \n",
    "    def match_images(self, src_img, tgt_img, src_keypoints, feature_extractor):\n",
    "        \"\"\"\n",
    "        Complete matching pipeline for an image pair.\n",
    "        \n",
    "        Args:\n",
    "            src_img: Source image tensor\n",
    "            tgt_img: Target image tensor\n",
    "            src_keypoints: Source keypoint coordinates [N, 2]\n",
    "            feature_extractor: Feature extractor instance\n",
    "            \n",
    "        Returns:\n",
    "            pred_keypoints: Predicted target keypoints [N, 2]\n",
    "            confidences: Match confidence scores [N]\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        src_kp_features = feature_extractor.extract_keypoint_features(src_img, src_keypoints)\n",
    "        _, tgt_features_2d = feature_extractor.extract_features(tgt_img)\n",
    "        \n",
    "        # Match\n",
    "        pred_keypoints, confidences = self.match_keypoints(\n",
    "            src_kp_features, tgt_features_2d, src_keypoints, feature_extractor\n",
    "        )\n",
    "        \n",
    "        return pred_keypoints, confidences\n",
    "\n",
    "# Create matcher with configuration flags\n",
    "matcher = CorrespondenceMatcher(\n",
    "    mutual_nn=False, \n",
    "    ratio_threshold=None,\n",
    "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
    "    soft_window=SOFT_WINDOW if USE_SOFT_ARGMAX else 7,\n",
    "    soft_tau=SOFT_TAU if USE_SOFT_ARGMAX else 0.05\n",
    ")\n",
    "print(f\"✓ Correspondence matcher created (soft-argmax={'enabled' if USE_SOFT_ARGMAX else 'disabled'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a6342",
   "metadata": {},
   "source": [
    "## Section 8: Full Dataset Evaluation\n",
    "\n",
    "Now let's evaluate on the complete test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_dataset(dataset, feature_extractor, matcher, evaluator, \n",
    "                       max_samples=None, save_visualizations=False):\n",
    "    \"\"\"\n",
    "    Evaluate correspondence on entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: SPairDataset instance\n",
    "        feature_extractor: DINOv2FeatureExtractor instance\n",
    "        matcher: CorrespondenceMatcher instance\n",
    "        evaluator: PCKEvaluator instance\n",
    "        max_samples: Maximum samples to evaluate (None = all)\n",
    "        save_visualizations: Whether to save sample visualizations\n",
    "        \n",
    "    Returns:\n",
    "        results: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating on {len(dataset)} samples...\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    all_confidences = []\n",
    "    \n",
    "    num_samples = min(max_samples, len(dataset)) if max_samples else len(dataset)\n",
    "    \n",
    "    for idx in tqdm(range(num_samples), desc=\"Evaluating\"):\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        src_img = sample['src_img']\n",
    "        tgt_img = sample['trg_img']\n",
    "        src_kps = sample['src_kps']\n",
    "        tgt_kps = sample['trg_kps']\n",
    "        \n",
    "        # Get valid keypoints\n",
    "        valid_mask = (src_kps[:, 0] >= 0) & (src_kps[:, 1] >= 0)\n",
    "        valid_src_kps = src_kps[valid_mask]\n",
    "        valid_tgt_kps = tgt_kps[valid_mask]\n",
    "        \n",
    "        if len(valid_src_kps) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Match\n",
    "        pred_kps, confidences = matcher.match_images(\n",
    "            src_img, tgt_img, valid_src_kps, feature_extractor\n",
    "        )\n",
    "        \n",
    "        all_predictions.append(pred_kps)\n",
    "        all_ground_truths.append(valid_tgt_kps)\n",
    "        all_confidences.append(confidences)\n",
    "        \n",
    "        # Save visualization for first few samples\n",
    "        if save_visualizations and idx < 5:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "            \n",
    "            axes[0].imshow(denorm_show(src_img))\n",
    "            axes[0].scatter(valid_src_kps[:, 0], valid_src_kps[:, 1], \n",
    "                           c='red', s=100, marker='x', linewidths=3)\n",
    "            axes[0].set_title(f'Source (Sample {idx})', fontsize=12, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(denorm_show(tgt_img))\n",
    "            axes[1].scatter(valid_tgt_kps[:, 0], valid_tgt_kps[:, 1], \n",
    "                           c='lime', s=100, marker='o', alpha=0.6, linewidths=2, \n",
    "                           edgecolors='darkgreen', label='GT')\n",
    "            axes[1].scatter(pred_kps[:, 0], pred_kps[:, 1], \n",
    "                           c='red', s=80, marker='x', linewidths=2, label='Pred')\n",
    "            axes[1].set_title(f'Target (Sample {idx})', fontsize=12, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(OUTPUT_DIR, f'match_sample_{idx}.png'), \n",
    "                       dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nComputing PCK metrics...\")\n",
    "    avg_pck, per_sample_pck, all_distances = evaluator.evaluate_dataset(\n",
    "        all_predictions, all_ground_truths\n",
    "    )\n",
    "    \n",
    "    # Compute additional statistics\n",
    "    all_confidences_flat = np.concatenate(all_confidences)\n",
    "    \n",
    "    results = {\n",
    "        'avg_pck': avg_pck,\n",
    "        'num_samples': num_samples,\n",
    "        'num_keypoints': len(all_distances),\n",
    "        'avg_confidence': float(all_confidences_flat.mean()),\n",
    "        'distance_stats': {\n",
    "            'mean': float(all_distances.mean()),\n",
    "            'std': float(all_distances.std()),\n",
    "            'median': float(np.median(all_distances)),\n",
    "            'min': float(all_distances.min()),\n",
    "            'max': float(all_distances.max())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation on small subset first (for testing)\n",
    "print(\"Running evaluation on 50 samples...\")\n",
    "results = evaluate_on_dataset(\n",
    "    dataset=dataset,\n",
    "    feature_extractor=feature_extractor,\n",
    "    matcher=matcher,\n",
    "    evaluator=evaluator,\n",
    "    max_samples=50,\n",
    "    save_visualizations=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS (50 samples)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPCK Metrics:\")\n",
    "for key, value in results['avg_pck'].items():\n",
    "    print(f\"  {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Samples evaluated: {results['num_samples']}\")\n",
    "print(f\"  Total keypoints: {results['num_keypoints']}\")\n",
    "print(f\"  Average confidence: {results['avg_confidence']:.4f}\")\n",
    "\n",
    "print(f\"\\nDistance Statistics:\")\n",
    "for key, value in results['distance_stats'].items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results_path = os.path.join(OUTPUT_DIR, 'evaluation_results_subset.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\n✓ Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349ae27",
   "metadata": {},
   "source": [
    "## Section 9: Full Evaluation (Optional)\n",
    "\n",
    "Uncomment and run this cell to evaluate on the complete test set. This will take longer (~30-60 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2700a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Full dataset evaluation\n",
    "# print(\"Running FULL evaluation on all test samples...\")\n",
    "# print(\"This will take approximately 30-60 minutes...\\n\")\n",
    "\n",
    "# results_full = evaluate_on_dataset(\n",
    "#     dataset=dataset,\n",
    "#     feature_extractor=feature_extractor,\n",
    "#     matcher=matcher,\n",
    "#     evaluator=evaluator,\n",
    "#     max_samples=None,  # Use all samples\n",
    "#     save_visualizations=False  # Don't save all visualizations\n",
    "# )\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"FULL EVALUATION RESULTS\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"\\nPCK Metrics:\")\n",
    "# for key, value in results_full['avg_pck'].items():\n",
    "#     print(f\"  {key}: {value:.4f} ({value*100:.2f}%)\")\n",
    "\n",
    "# print(f\"\\nDataset Statistics:\")\n",
    "# print(f\"  Samples evaluated: {results_full['num_samples']}\")\n",
    "# print(f\"  Total keypoints: {results_full['num_keypoints']}\")\n",
    "# print(f\"  Average confidence: {results_full['avg_confidence']:.4f}\")\n",
    "\n",
    "# # Save results\n",
    "# results_path = os.path.join(OUTPUT_DIR, 'evaluation_results_full.json')\n",
    "# with open(results_path, 'w') as f:\n",
    "#     json.dump(results_full, f, indent=2)\n",
    "# print(f\"\\n✓ Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cbfcb7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we accomplished:\n",
    "\n",
    "1. **Environment Setup**: Cross-platform compatible setup (Windows/Linux/macOS/Colab)\n",
    "\n",
    "2. **DINOv2 Feature Extraction**:\n",
    "   - Model: DINOv2 ViT-B/14\n",
    "   - Features: 16×16×768 dense feature maps\n",
    "   - Preprocessing: ImageNet normalization\n",
    "   - Output: L2-normalized features for cosine similarity\n",
    "\n",
    "3. **Feature Visualization**:\n",
    "   - PCA visualization showing semantic structure\n",
    "   - Patch-to-patch similarity heatmaps\n",
    "   - Feature quality analysis\n",
    "\n",
    "4. **Correspondence Matching**:\n",
    "   - Nearest neighbor matching with bilinear interpolation\n",
    "   - Optional mutual nearest neighbors\n",
    "   - Optional ratio test for ambiguous matches\n",
    "\n",
    "5. **PCK Evaluation**:\n",
    "   - Standard thresholds: 0.05, 0.10, 0.15\n",
    "   - Bounding box normalization\n",
    "   - Per-sample and aggregate metrics\n",
    "\n",
    "6. **Results** (50 samples):\n",
    "   - See evaluation results above\n",
    "   - Visualizations saved to `outputs/dinov2/`\n",
    "\n",
    "### Key Insights:\n",
    "- DINOv2 learns rich semantic features without supervision\n",
    "- Features capture object parts and semantic regions\n",
    "- Performance varies by object category and pose variation\n",
    "- Higher similarity scores correlate with better matches\n",
    "\n",
    "### Next Steps:\n",
    "1. **Run full evaluation** (uncomment Section 9)\n",
    "2. **Try different matching strategies**:\n",
    "   - Enable mutual nearest neighbors: `matcher = CorrespondenceMatcher(mutual_nn=True)`\n",
    "   - Enable ratio test: `matcher = CorrespondenceMatcher(ratio_threshold=0.8)`\n",
    "3. **Compare with other backbones** (DINOv3, SAM)\n",
    "4. **Analyze per-category performance**\n",
    "5. **Try larger models** (dinov2_vitl14, dinov2_vitg14)\n",
    "\n",
    "### Files Generated:\n",
    "- `outputs/dinov2/sample_images_with_keypoints.png`\n",
    "- `outputs/dinov2/pca_feature_visualization.png`\n",
    "- `outputs/dinov2/similarity_heatmap.png`\n",
    "- `outputs/dinov2/sample_matching_result.png`\n",
    "- `outputs/dinov2/match_sample_*.png` (first 5 samples)\n",
    "- `outputs/dinov2/evaluation_results_subset.json`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
