{
    "cells":  [
                  {
                      "cell_type":  "markdown",
                      "id":  "cd13a90c",
                      "metadata":  {
                                       "id":  "cd13a90c"
                                   },
                      "source":  [
                                     "# DINOv3 Dense Correspondence Pipeline\n",
                                     "\n",
                                     "This notebook implements a complete pipeline for evaluating DINOv3 on the **SPair-71k** dense correspondence task, using Google Colab. The pipeline includes:\n",
                                     "\n",
                                     "1. **Model Loading**: Automatic detection and loading of DINOv3 weights\n",
                                     "2. **Feature Extraction**: Dense feature extraction from image pairs using vision transformer backbones\n",
                                     "3. **Correspondence Matching**: Computing correspondences between image pairs through feature similarity\n",
                                     "4. **Evaluation**: Computing PCK (Percentage of Correct Keypoints) metrics on the SPair-71k validation/test sets\n",
                                     "5. **Optional Finetuning**: Light finetuning of the last transformer blocks on SPair-71k training data\n",
                                     "6. **Optional Soft-Argmax**: Sub-pixel coordinate refinement using window-based soft-argmax"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "source":  [
                                     "# ALL IMPORTS:\n",
                                     "from google.colab import drive\n",
                                     "import os, sys, shutil, importlib\n",
                                     "import torch\n",
                                     "import torch.nn.functional as F\n",
                                     "import numpy as np\n",
                                     "import requests\n",
                                     "import zipfile\n",
                                     "from torch.utils.data import Dataset, DataLoader\n",
                                     "from PIL import Image\n",
                                     "import json\n",
                                     "from torchvision import transforms\n",
                                     "import requests\n",
                                     "import tarfile\n",
                                     "from tqdm import tqdm as tqdm_requests\n",
                                     "import random\n",
                                     "import subprocess\n",
                                     "import timm\n",
                                     "from pathlib import Path\n",
                                     "import matplotlib.pyplot as plt\n",
                                     "from tqdm.auto import tqdm\n"
                                 ],
                      "metadata":  {
                                       "id":  "_VJoIsz5Q97E"
                                   },
                      "id":  "_VJoIsz5Q97E",
                      "execution_count":  1,
                      "outputs":  [

                                  ]
                  },
                  {
                      "cell_type":  "code",
                      "source":  [
                                     "# Mount Google Drive\n",
                                     "drive.mount(\u0027/content/drive\u0027)\n",
                                     "\n",
                                     "# Define Project and Data Roots (must be consistent with previous setup)\n",
                                     "PROJECT_ROOT = \u0027/content/AMLProject\u0027\n",
                                     "DATA_ROOT = \u0027/content/drive/MyDrive/AMLProject/data\u0027\n",
                                     "\n",
                                     "# Create necessary directories if they don\u0027t exist\n",
                                     "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, \u0027checkpoints\u0027)\n",
                                     "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \u0027outputs\u0027, \u0027dinov3\u0027) # Adjust for DINOv3 outputs\n",
                                     "MODEL_DIR = os.path.join(PROJECT_ROOT, \u0027models\u0027)\n",
                                     "\n",
                                     "for directory in [CHECKPOINT_DIR, OUTPUT_DIR, MODEL_DIR, DATA_ROOT]:\n",
                                     "    os.makedirs(directory, exist_ok=True)\n",
                                     "\n",
                                     "print(f\"Project root: {PROJECT_ROOT}\")\n",
                                     "print(f\"Data root: {DATA_ROOT}\")\n",
                                     "print(f\"Output directory for DINOv3: {OUTPUT_DIR}\")\n"
                                 ],
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "jl2Jdk_idnk5",
                                       "outputId":  "925f1969-a142-4357-af56-d1455ddf6fea"
                                   },
                      "id":  "jl2Jdk_idnk5",
                      "execution_count":  2,
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
                                                       "Project root: /content/AMLProject\n",
                                                       "Data root: /content/drive/MyDrive/AMLProject/data\n",
                                                       "Output directory for DINOv3: /content/AMLProject/outputs/dinov3\n"
                                                   ]
                                      }
                                  ]
                  },
                  {
                      "cell_type":  "code",
                      "source":  [
                                     "# --- Configuring project path ---\n",
                                     "PROJECT_ROOT = \"/content/drive/MyDrive/AMLProject\"\n",
                                     "repo_path = os.path.join(PROJECT_ROOT, \"SD4Match\")\n",
                                     "\n",
                                     "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
                                     "\n",
                                     "# --- Downlaoding SD4Match if missing ---\n",
                                     "if not os.path.exists(repo_path):\n",
                                     "    print(\"ðŸ“¦ Downloading SD4Match repository from ActiveVisionLab...\")\n",
                                     "\n",
                                     "    zip_url = \"https://github.com/ActiveVisionLab/SD4Match/archive/refs/heads/main.zip\"\n",
                                     "    zip_path = os.path.join(PROJECT_ROOT, \"SD4Match.zip\")\n",
                                     "\n",
                                     "    # download\n",
                                     "\n",
                                     "    r = requests.get(zip_url, stream=True)\n",
                                     "    with open(zip_path, \"wb\") as f:\n",
                                     "        for chunk in r.iter_content(chunk_size=8192):\n",
                                     "            f.write(chunk)\n",
                                     "\n",
                                     "    # zip extraction\n",
                                     "\n",
                                     "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
                                     "        z.extractall(PROJECT_ROOT)\n",
                                     "\n",
                                     "    # move and rename\n",
                                     "    extracted = os.path.join(PROJECT_ROOT, \"SD4Match-main\")\n",
                                     "    if os.path.exists(repo_path):\n",
                                     "        shutil.rmtree(repo_path)\n",
                                     "    os.rename(extracted, repo_path)\n",
                                     "    os.remove(zip_path)\n",
                                     "\n",
                                     "    print(\"âœ“ Repository SD4Match ready at\", repo_path)\n",
                                     "\n",
                                     "if repo_path not in sys.path:\n",
                                     "    sys.path.append(repo_path)\n",
                                     "\n",
                                     "try:\n",
                                     "    module = importlib.import_module(\"dataset.spair\")\n",
                                     "    SPairDataset = getattr(module, \"SPairDataset\")\n",
                                     "    print(\"âœ“ SPairDataset imported successfully!\")\n",
                                     "except Exception as e:\n",
                                     "    print(\"âŒ Import Error:\", e)\n"
                                 ],
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "lc9O5wiady-Z",
                                       "outputId":  "48dfe437-8933-4053-8f43-31a8e05b65e1"
                                   },
                      "id":  "lc9O5wiady-Z",
                      "execution_count":  3,
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "âœ“ SPairDataset imported successfully!\n"
                                                   ]
                                      }
                                  ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "50c25544",
                      "metadata":  {
                                       "id":  "50c25544"
                                   },
                      "source":  [
                                     "## Utility Functions\n",
                                     "\n",
                                     "Window soft-argmax for sub-pixel refinement and finetuning utilities."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  4,
                      "id":  "b683a63c",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "b683a63c",
                                       "outputId":  "5d39ebd1-b4dc-4565-9ea6-012a4b7f4099"
                                   },
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "âœ“ Utility functions loaded\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "def window_soft_argmax(similarity, H, W, window=7, tau=0.05):\n",
                                     "    \"\"\"\n",
                                     "    Window soft-argmax for sub-pixel coordinate prediction.\n",
                                     "\n",
                                     "    Args:\n",
                                     "        similarity: [N, H*W] or [N, H, W] similarity scores\n",
                                     "        H, W: Grid dimensions\n",
                                     "        window: Window size around peak (odd number)\n",
                                     "        tau: Temperature for softmax (lower = sharper)\n",
                                     "\n",
                                     "    Returns:\n",
                                     "        [N, 2] tensor with (y, x) coordinates in patch space\n",
                                     "    \"\"\"\n",
                                     "    if similarity.dim() == 2:\n",
                                     "        N = similarity.size(0)\n",
                                     "        sim2d = similarity.view(N, H, W)\n",
                                     "    elif similarity.dim() == 3:\n",
                                     "        N = similarity.size(0)\n",
                                     "        sim2d = similarity\n",
                                     "    else:\n",
                                     "        raise ValueError(\"similarity must be [N,H*W] or [N,H,W]\")\n",
                                     "\n",
                                     "    r = window // 2\n",
                                     "    preds = []\n",
                                     "\n",
                                     "    for i in range(N):\n",
                                     "        s = sim2d[i]  # [H, W]\n",
                                     "\n",
                                     "        # Find peak with argmax\n",
                                     "        idx = torch.argmax(s)\n",
                                     "        y0 = (idx // W).item()\n",
                                     "        x0 = (idx % W).item()\n",
                                     "\n",
                                     "        # Extract window around peak\n",
                                     "        y1, y2 = max(y0 - r, 0), min(y0 + r + 1, H)\n",
                                     "        x1, x2 = max(x0 - r, 0), min(x0 + r + 1, W)\n",
                                     "\n",
                                     "        sub = s[y1:y2, x1:x2]\n",
                                     "\n",
                                     "        # Create coordinate grids\n",
                                     "        yy, xx = torch.meshgrid(\n",
                                     "            torch.arange(y1, y2, device=s.device, dtype=torch.float32),\n",
                                     "            torch.arange(x1, x2, device=s.device, dtype=torch.float32),\n",
                                     "            indexing=\u0027ij\u0027\n",
                                     "        )\n",
                                     "\n",
                                     "        # Soft-argmax within window\n",
                                     "        wts = torch.softmax(sub.flatten() / tau, dim=0).view_as(sub)\n",
                                     "        y_hat = (wts * yy).sum()\n",
                                     "        x_hat = (wts * xx).sum()\n",
                                     "\n",
                                     "        preds.append(torch.stack([y_hat, x_hat]))\n",
                                     "\n",
                                     "    return torch.stack(preds, dim=0)  # [N, 2]\n",
                                     "\n",
                                     "\n",
                                     "def unfreeze_last_k_blocks(model, k, blocks_attr=\u0027blocks\u0027):\n",
                                     "    \"\"\"\n",
                                     "    Unfreeze the last k transformer blocks of a model.\n",
                                     "\n",
                                     "    Args:\n",
                                     "        model: The backbone model\n",
                                     "        k: Number of last blocks to unfreeze\n",
                                     "        blocks_attr: Attribute name for blocks (default \u0027blocks\u0027)\n",
                                     "\n",
                                     "    Returns:\n",
                                     "        List of trainable parameters\n",
                                     "    \"\"\"\n",
                                     "    # Freeze all parameters\n",
                                     "    for p in model.parameters():\n",
                                     "        p.requires_grad = False\n",
                                     "\n",
                                     "    # Unfreeze last k blocks\n",
                                     "    blocks = getattr(model, blocks_attr)\n",
                                     "    for block in blocks[-k:]:\n",
                                     "        for p in block.parameters():\n",
                                     "            p.requires_grad = True\n",
                                     "\n",
                                     "    # Return trainable parameters\n",
                                     "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
                                     "    print(f\"Unfroze last {k} blocks: {len(trainable_params)} trainable parameters\")\n",
                                     "\n",
                                     "    return trainable_params\n",
                                     "\n",
                                     "\n",
                                     "def compute_keypoint_loss(sim2d, H, W, gt_xy_px, patch_size, use_soft=True, window=7, tau=0.05):\n",
                                     "    \"\"\"\n",
                                     "    Compute loss from similarity map to ground truth keypoint.\n",
                                     "\n",
                                     "    Args:\n",
                                     "        sim2d: [H, W] similarity map\n",
                                     "        H, W: Grid dimensions\n",
                                     "        gt_xy_px: [2] ground truth coordinates in pixels (y, x)\n",
                                     "        patch_size: Patch size for coordinate conversion\n",
                                     "        use_soft: Use soft-argmax (True) or argmax (False)\n",
                                     "        window, tau: Soft-argmax parameters\n",
                                     "\n",
                                     "    Returns:\n",
                                     "        Scalar loss\n",
                                     "    \"\"\"\n",
                                     "    if use_soft:\n",
                                     "        pred_xy_patch = window_soft_argmax(sim2d[None], H, W, window, tau)[0]\n",
                                     "    else:\n",
                                     "        idx = sim2d.argmax()\n",
                                     "        pred_xy_patch = torch.stack([idx // W, idx % W]).float()\n",
                                     "\n",
                                     "    pred_xy_px = (pred_xy_patch + 0.5) * patch_size\n",
                                     "\n",
                                     "    return F.smooth_l1_loss(pred_xy_px, gt_xy_px)\n",
                                     "\n",
                                     "\n",
                                     "print(\"âœ“ Utility functions loaded\")"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "495c312d",
                      "metadata":  {
                                       "id":  "495c312d"
                                   },
                      "source":  [
                                     "## SPair-71k Dataloader:\n",
                                     "Complete dataloader for SPair-71k with keypoint annotations for finetuning."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  5,
                      "id":  "669244e2",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "669244e2",
                                       "outputId":  "89e9443f-cd17-424b-a902-f6c99e84e718"
                                   },
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "âœ“ SPair-71k dataloader ready\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "class SPairDataset(Dataset):\n",
                                     "    \"\"\"SPair-71k dataset with keypoint annotations for correspondence learning.\"\"\"\n",
                                     "\n",
                                     "    def __init__(self, root_dir, split=\u0027trn\u0027, category=None, image_size=224, subset=None):\n",
                                     "        self.root_dir = root_dir\n",
                                     "        self.split = split\n",
                                     "        self.category = category\n",
                                     "        self.image_size = image_size\n",
                                     "\n",
                                     "        self.pairs = self._load_pairs()\n",
                                     "        if subset is not None:\n",
                                     "            self.pairs = self.pairs[:subset]\n",
                                     "\n",
                                     "        self.transform = transforms.Compose([\n",
                                     "            transforms.Resize((image_size, image_size)),\n",
                                     "            transforms.ToTensor(),\n",
                                     "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
                                     "                               std=[0.229, 0.224, 0.225])\n",
                                     "        ])\n",
                                     "\n",
                                     "        print(f\"SPair-71k {split} dataset: {len(self.pairs)} pairs loaded\")\n",
                                     "\n",
                                     "    def _load_pairs(self):\n",
                                     "        pairs = []\n",
                                     "        # SPair-71k: JSON files are directly in PairAnnotation/{split}/ directory\n",
                                     "        layout_dir = os.path.join(self.root_dir, \u0027PairAnnotation\u0027, self.split)\n",
                                     "\n",
                                     "        if not os.path.exists(layout_dir):\n",
                                     "            print(f\"Warning: PairAnnotation directory not found: {layout_dir}\")\n",
                                     "            return pairs\n",
                                     "\n",
                                     "        # Read all JSON files directly from the split directory\n",
                                     "        for fname in os.listdir(layout_dir):\n",
                                     "            if not fname.endswith(\u0027.json\u0027):\n",
                                     "                continue\n",
                                     "\n",
                                     "            json_path = os.path.join(layout_dir, fname)\n",
                                     "            try:\n",
                                     "                with open(json_path, \u0027r\u0027) as f:\n",
                                     "                    pair_data = json.load(f)\n",
                                     "\n",
                                     "                cat = pair_data.get(\u0027category\u0027, \u0027unknown\u0027)\n",
                                     "\n",
                                     "                # Filter by category if specified\n",
                                     "                if self.category and cat != self.category:\n",
                                     "                    continue\n",
                                     "\n",
                                     "                pair = {\n",
                                     "                    \u0027category\u0027: cat,\n",
                                     "                    \u0027src_img\u0027: pair_data[\u0027src_imname\u0027],\n",
                                     "                    \u0027tgt_img\u0027: pair_data[\u0027trg_imname\u0027],\n",
                                     "                    \u0027src_kps\u0027: np.array(pair_data[\u0027src_kps\u0027]).reshape(-1, 2),\n",
                                     "                    \u0027tgt_kps\u0027: np.array(pair_data[\u0027trg_kps\u0027]).reshape(-1, 2),\n",
                                     "                    \u0027src_bbox\u0027: pair_data.get(\u0027src_bndbox\u0027, None),\n",
                                     "                    \u0027tgt_bbox\u0027: pair_data.get(\u0027trg_bndbox\u0027, None),\n",
                                     "                }\n",
                                     "                pairs.append(pair)\n",
                                     "\n",
                                     "            except Exception as e:\n",
                                     "                continue\n",
                                     "\n",
                                     "        return pairs\n",
                                     "\n",
                                     "    def __len__(self):\n",
                                     "        return len(self.pairs)\n",
                                     "\n",
                                     "    def __getitem__(self, idx):\n",
                                     "        pair = self.pairs[idx]\n",
                                     "\n",
                                     "        src_img_path = os.path.join(self.root_dir, \u0027JPEGImages\u0027,\n",
                                     "                                    pair[\u0027category\u0027], pair[\u0027src_img\u0027])\n",
                                     "        tgt_img_path = os.path.join(self.root_dir, \u0027JPEGImages\u0027,\n",
                                     "                                    pair[\u0027category\u0027], pair[\u0027tgt_img\u0027])\n",
                                     "\n",
                                     "        src_img_pil = Image.open(src_img_path).convert(\u0027RGB\u0027)\n",
                                     "        tgt_img_pil = Image.open(tgt_img_path).convert(\u0027RGB\u0027)\n",
                                     "\n",
                                     "        src_w, src_h = src_img_pil.size\n",
                                     "        tgt_w, tgt_h = tgt_img_pil.size\n",
                                     "\n",
                                     "        src_kps = pair[\u0027src_kps\u0027].copy().astype(float)\n",
                                     "        tgt_kps = pair[\u0027tgt_kps\u0027].copy().astype(float)\n",
                                     "\n",
                                     "        src_kps[:, 0] *= self.image_size / src_w\n",
                                     "        src_kps[:, 1] *= self.image_size / src_h\n",
                                     "        tgt_kps[:, 0] *= self.image_size / tgt_w\n",
                                     "        tgt_kps[:, 1] *= self.image_size / tgt_h\n",
                                     "\n",
                                     "        src_img = self.transform(src_img_pil)\n",
                                     "        tgt_img = self.transform(tgt_img_pil)\n",
                                     "\n",
                                     "        if pair[\u0027src_bbox\u0027] is not None:\n",
                                     "            src_bbox = np.array(pair[\u0027src_bbox\u0027], dtype=float)\n",
                                     "            src_bbox[0::2] *= self.image_size / src_w\n",
                                     "            src_bbox[1::2] *= self.image_size / src_h\n",
                                     "            src_bbox_wh = np.array([src_bbox[2] - src_bbox[0], src_bbox[3] - src_bbox[1]])\n",
                                     "        else:\n",
                                     "            src_bbox_wh = np.array([self.image_size, self.image_size])\n",
                                     "\n",
                                     "        if pair[\u0027tgt_bbox\u0027] is not None:\n",
                                     "            tgt_bbox = np.array(pair[\u0027tgt_bbox\u0027], dtype=float)\n",
                                     "            tgt_bbox[0::2] *= self.image_size / tgt_w\n",
                                     "            tgt_bbox[1::2] *= self.image_size / tgt_h\n",
                                     "            tgt_bbox_wh = np.array([tgt_bbox[2] - tgt_bbox[0], tgt_bbox[3] - tgt_bbox[1]])\n",
                                     "        else:\n",
                                     "            tgt_bbox_wh = np.array([self.image_size, self.image_size])\n",
                                     "\n",
                                     "        return {\n",
                                     "            \u0027src_img\u0027: src_img,\n",
                                     "            \u0027tgt_img\u0027: tgt_img,\n",
                                     "            \u0027src_kps\u0027: torch.from_numpy(src_kps).float(),\n",
                                     "            \u0027tgt_kps\u0027: torch.from_numpy(tgt_kps).float(),\n",
                                     "            \u0027src_bbox_wh\u0027: torch.from_numpy(src_bbox_wh).float(),\n",
                                     "            \u0027tgt_bbox_wh\u0027: torch.from_numpy(tgt_bbox_wh).float(),\n",
                                     "            \u0027category\u0027: pair[\u0027category\u0027],\n",
                                     "            \u0027pair_id\u0027: idx\n",
                                     "        }\n",
                                     "\n",
                                     "\n",
                                     "def create_spair_dataloaders(root_dir, batch_size=1, num_workers=2,\n",
                                     "                             train_subset=None, val_subset=None):\n",
                                     "    train_dataset = SPairDataset(root_dir, split=\u0027trn\u0027, subset=train_subset)\n",
                                     "    val_dataset = SPairDataset(root_dir, split=\u0027val\u0027, subset=val_subset)\n",
                                     "\n",
                                     "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
                                     "                             num_workers=num_workers, pin_memory=True)\n",
                                     "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
                                     "                           num_workers=num_workers, pin_memory=True)\n",
                                     "\n",
                                     "    return train_loader, val_loader\n",
                                     "\n",
                                     "\n",
                                     "print(\"âœ“ SPair-71k dataloader ready\")"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "source":  [
                                     "# Download SPair-71k dataset (if not already present)\n",
                                     "\n",
                                     "data_path = os.path.join(DATA_ROOT, \u0027SPair-71k\u0027)\n",
                                     "\n",
                                     "if not os.path.exists(data_path):\n",
                                     "    print(\"Downloading SPair-71k dataset...\")\n",
                                     "    url = \"http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
                                     "    tar_path = os.path.join(DATA_ROOT, \u0027SPair-71k.tar.gz\u0027)\n",
                                     "\n",
                                     "    # Download with progress bar\n",
                                     "    response = requests.get(url, stream=True)\n",
                                     "    total_size = int(response.headers.get(\u0027content-length\u0027, 0))\n",
                                     "\n",
                                     "    with open(tar_path, \u0027wb\u0027) as f, tqdm_requests(\n",
                                     "        desc=\u0027Downloading\u0027,\n",
                                     "        total=total_size,\n",
                                     "        unit=\u0027B\u0027,\n",
                                     "        unit_scale=True,\n",
                                     "        unit_divisor=1024,\n",
                                     "    ) as pbar:\n",
                                     "        for data in response.iter_content(chunk_size=1024):\n",
                                     "            size = f.write(data)\n",
                                     "            pbar.update(size)\n",
                                     "\n",
                                     "    print(\"\\nExtracting...\")\n",
                                     "    with tarfile.open(tar_path, \u0027r:gz\u0027) as tar:\n",
                                     "        tar.extractall(DATA_ROOT)\n",
                                     "\n",
                                     "    # Cleanup\n",
                                     "    os.remove(tar_path)\n",
                                     "    print(\"âœ“ Extraction complete\")\n",
                                     "else:\n",
                                     "    print(f\"âœ“ SPair-71k dataset already exists at {data_path}\")"
                                 ],
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "ZrVK5uXoGWhS",
                                       "outputId":  "6581b9e1-ed0e-4045-9aef-21d1afc49f3c"
                                   },
                      "id":  "ZrVK5uXoGWhS",
                      "execution_count":  6,
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "âœ“ SPair-71k dataset already exists at /content/drive/MyDrive/AMLProject/data/SPair-71k\n"
                                                   ]
                                      }
                                  ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "37e9fd00",
                      "metadata":  {
                                       "id":  "37e9fd00"
                                   },
                      "source":  [
                                     "## Configuration Flags\n",
                                     "\n",
                                     "Set these flags to control the pipeline behavior:\n",
                                     "- `ENABLE_FINETUNING`: Enable light finetuning of last transformer blocks\n",
                                     "- `USE_SOFT_ARGMAX`: Use window soft-argmax instead of argmax for prediction"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  7,
                      "id":  "33a55962",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "33a55962",
                                       "outputId":  "70bbaa61-0e67-4a46-be71-8a7499dfdacc"
                                   },
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "Configuration:\n",
                                                       "  ENABLE_FINETUNING = False\n",
                                                       "  USE_SOFT_ARGMAX = False\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# ========== CONFIGURATION FLAGS ==========\n",
                                     "# Set these flags to control behavior\n",
                                     "ENABLE_FINETUNING = False  # Set True to enable light finetuning of last layers\n",
                                     "USE_SOFT_ARGMAX = False    # Set True to use window soft-argmax instead of argmax\n",
                                     "\n",
                                     "# Finetuning hyperparameters (only used if ENABLE_FINETUNING=True)\n",
                                     "FINETUNE_K_LAYERS = 2      # Number of last transformer blocks to unfreeze {1, 2, 4}\n",
                                     "FINETUNE_LR = 1e-5         # Learning rate\n",
                                     "FINETUNE_WD = 1e-4         # Weight decay\n",
                                     "FINETUNE_EPOCHS = 3        # Number of training epochs\n",
                                     "FINETUNE_BATCH_SIZE = 1    # Batch size for training\n",
                                     "FINETUNE_TRAIN_SUBSET = None  # None for full training set, or int for subset\n",
                                     "\n",
                                     "# Soft-argmax hyperparameters (only used if USE_SOFT_ARGMAX=True)\n",
                                     "SOFT_WINDOW = 7            # Window size around peak (odd number: 5, 7, 9)\n",
                                     "SOFT_TAU = 0.05            # Softmax temperature (lower = sharper)\n",
                                     "\n",
                                     "print(f\"Configuration:\")\n",
                                     "print(f\"  ENABLE_FINETUNING = {ENABLE_FINETUNING}\")\n",
                                     "print(f\"  USE_SOFT_ARGMAX = {USE_SOFT_ARGMAX}\")\n",
                                     "if ENABLE_FINETUNING:\n",
                                     "    print(f\"  Finetuning: k={FINETUNE_K_LAYERS}, lr={FINETUNE_LR}, epochs={FINETUNE_EPOCHS}\")\n",
                                     "if USE_SOFT_ARGMAX:\n",
                                     "    print(f\"  Soft-argmax: window={SOFT_WINDOW}, tau={SOFT_TAU}\")"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  8,
                      "id":  "bb67222d",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "bb67222d",
                                       "outputId":  "ce574649-e75c-438b-a077-b079b7c62fca"
                                   },
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "Environment: colab\n",
                                                       "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
                                                       "PROJECT_ROOT: /content/drive/MyDrive/AMLProject\n",
                                                       "DATA_ROOT: /content/drive/MyDrive/AMLProject/data\n",
                                                       "MODEL_ROOT: /content/drive/MyDrive/AMLProject/models\n",
                                                       "OUTPUT_ROOT: /content/drive/MyDrive/AMLProject/outputs\n",
                                                       "Seed set to 42\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Extract and Normalize Common Utility Functions\n",
                                     "\n",
                                     "def set_seed(seed=42):\n",
                                     "    random.seed(seed)\n",
                                     "    np.random.seed(seed)\n",
                                     "    torch.manual_seed(seed)\n",
                                     "    if torch.cuda.is_available():\n",
                                     "        torch.cuda.manual_seed_all(seed)\n",
                                     "    print(f\"Seed set to {seed}\")\n",
                                     "\n",
                                     "def detect_env():\n",
                                     "    try:\n",
                                     "        import google.colab\n",
                                     "        return \u0027colab\u0027\n",
                                     "    except Exception:\n",
                                     "        return \u0027local\u0027\n",
                                     "\n",
                                     "ENV = detect_env()\n",
                                     "print(\u0027Environment:\u0027, ENV)\n",
                                     "\n",
                                     "def get_paths():\n",
                                     "    if ENV == \u0027colab\u0027:\n",
                                     "        from google.colab import drive\n",
                                     "        drive.mount(\u0027/content/drive\u0027)\n",
                                     "        root = \u0027/content/drive/MyDrive/AMLProject\u0027\n",
                                     "        data_root = os.path.join(root, \u0027data\u0027)\n",
                                     "        model_root = os.path.join(root, \u0027models\u0027)\n",
                                     "        out_root = os.path.join(root, \u0027outputs\u0027)\n",
                                     "    else:\n",
                                     "        root = os.getcwd()\n",
                                     "        data_root = os.path.join(root, \u0027data\u0027)\n",
                                     "        model_root = os.path.join(root, \u0027models\u0027)\n",
                                     "        out_root = os.path.join(root, \u0027outputs\u0027)\n",
                                     "    os.makedirs(data_root, exist_ok=True)\n",
                                     "    os.makedirs(model_root, exist_ok=True)\n",
                                     "    os.makedirs(out_root, exist_ok=True)\n",
                                     "    return root, data_root, model_root, out_root\n",
                                     "\n",
                                     "PROJECT_ROOT, DATA_ROOT, MODEL_ROOT, OUTPUT_ROOT = get_paths()\n",
                                     "print(\u0027PROJECT_ROOT:\u0027, PROJECT_ROOT)\n",
                                     "print(\u0027DATA_ROOT:\u0027, DATA_ROOT)\n",
                                     "print(\u0027MODEL_ROOT:\u0027, MODEL_ROOT)\n",
                                     "print(\u0027OUTPUT_ROOT:\u0027, OUTPUT_ROOT)\n",
                                     "set_seed(42)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "8fa80e1b",
                                       "outputId":  "3d0a85a2-413b-4237-8816-2e4ffa2f38b8"
                                   },
                      "source":  [
                                     "# Clone DINOv3 repository if it doesn\u0027t exist\n",
                                     "\n",
                                     "# Assuming PROJECT_ROOT is defined from previous cells, e.g., /content/drive/MyDrive/AMLProject\n",
                                     "dinov3_repo_path = os.path.join(PROJECT_ROOT, \u0027dinov3\u0027) # Or os.path.join(MODEL_ROOT, \u0027dinov3\u0027)\n",
                                     "\n",
                                     "if not os.path.exists(dinov3_repo_path):\n",
                                     "    print(f\"ðŸ“¦ Cloning DINOv3 repository to {dinov3_repo_path}...\")\n",
                                     "    # Make sure the parent directory exists\n",
                                     "    os.makedirs(dinov3_repo_path, exist_ok=True)\n",
                                     "    # Use a subprocess to clone, as !git clone might not work reliably with paths containing spaces\n",
                                     "    try:\n",
                                     "        subprocess.run([\u0027git\u0027, \u0027clone\u0027, \u0027https://github.com/facebookresearch/dinov3.git\u0027, dinov3_repo_path], check=True)\n",
                                     "        print(\"âœ“ Repository DINOv3 cloned successfully!\")\n",
                                     "    except subprocess.CalledProcessError as e:\n",
                                     "        print(f\"âŒ Error cloning DINOv3 repository: {e}\")\n",
                                     "        print(\"Please check your internet connection or the repository URL.\")\n",
                                     "else:\n",
                                     "    print(f\"âœ“ Repository DINOv3 already exists at {dinov3_repo_path}\")\n",
                                     "\n",
                                     "# Add the cloned repo to sys.path if not already there\n",
                                     "if dinov3_repo_path not in sys.path:\n",
                                     "    sys.path.insert(0, dinov3_repo_path)\n",
                                     "    print(f\"Added {dinov3_repo_path} to sys.path\")\n"
                                 ],
                      "id":  "8fa80e1b",
                      "execution_count":  9,
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "âœ“ Repository DINOv3 already exists at /content/drive/MyDrive/AMLProject/dinov3\n",
                                                       "Added /content/drive/MyDrive/AMLProject/dinov3 to sys.path\n"
                                                   ]
                                      }
                                  ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  10,
                      "id":  "0b001c33",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "0b001c33",
                                       "outputId":  "1e1234bc-5fb9-41e0-e22d-bda5810f562b"
                                   },
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "Device: cuda\n",
                                                       "DINOv3 repo: /content/drive/MyDrive/AMLProject/dinov3\n",
                                                       "DINOv3 weight: /content/drive/MyDrive/AMLProject/models/dinov3_vitb16.pth\n",
                                                       "Loaded DINOv3 weights. Missing=0 Unexpected=37\n",
                                                       "âœ“ Model type: DINOv3 ViT-B/16 (official)\n",
                                                       "Feature extractor ready. Mock mode: False image_size: 224 patch: 16\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# Model: Auto-detect weights, fallback, and mock outputs\n",
                                     "\n",
                                     "# Fallback paths if previous cell not run\n",
                                     "if \u0027PROJECT_ROOT\u0027 not in globals() or \u0027MODEL_ROOT\u0027 not in globals():\n",
                                     "    PROJECT_ROOT = os.getcwd()\n",
                                     "    MODEL_ROOT = os.path.join(PROJECT_ROOT, \u0027models\u0027)\n",
                                     "    OUTPUT_ROOT = os.path.join(PROJECT_ROOT, \u0027outputs\u0027)\n",
                                     "    os.makedirs(MODEL_ROOT, exist_ok=True)\n",
                                     "    os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
                                     "    print(\u0027Initialized default paths (previous cell not run).\u0027)\n",
                                     "\n",
                                     "# Device detection (CUDA, MPS, CPU)\n",
                                     "if torch.cuda.is_available():\n",
                                     "    device = torch.device(\u0027cuda\u0027)\n",
                                     "elif hasattr(torch.backends, \u0027mps\u0027) and torch.backends.mps.is_available():\n",
                                     "    device = torch.device(\u0027mps\u0027)\n",
                                     "else:\n",
                                     "    device = torch.device(\u0027cpu\u0027)\n",
                                     "print(\u0027Device:\u0027, device)\n",
                                     "\n",
                                     "# Auto-detect official DINOv3 repo and weights\n",
                                     "def find_dinov3_repo_and_weights():\n",
                                     "    candidates_repo = [\n",
                                     "        Path(MODEL_ROOT) / \u0027dinov3\u0027,\n",
                                     "        Path(PROJECT_ROOT) / \u0027models\u0027 / \u0027dinov3\u0027,\n",
                                     "        Path(PROJECT_ROOT) / \u0027dinov3\u0027\n",
                                     "    ]\n",
                                     "    repo_path = next((str(p) for p in candidates_repo if p.exists()), None)\n",
                                     "    if repo_path and repo_path not in sys.path:\n",
                                     "        sys.path.append(repo_path)\n",
                                     "\n",
                                     "    candidates_weights = [\n",
                                     "        Path(MODEL_ROOT) / \u0027dinov3_vitb16.pth\u0027,\n",
                                     "        Path(MODEL_ROOT) / \u0027dinov3\u0027 / \u0027dinov3_vitb16.pth\u0027,\n",
                                     "        Path(PROJECT_ROOT) / \u0027models\u0027 / \u0027dinov3_vitb16.pth\u0027,\n",
                                     "        Path(PROJECT_ROOT) / \u0027models\u0027 / \u0027dinov3\u0027 / \u0027dinov3_vitb16.pth\u0027,\n",
                                     "        Path(PROJECT_ROOT) / \u0027checkpoints\u0027 / \u0027dinov3\u0027 / \u0027dinov3_vitb16.pth\u0027,\n",
                                     "        Path(PROJECT_ROOT) / \u0027weights\u0027 / \u0027dinov3_vitb16.pth\u0027\n",
                                     "    ]\n",
                                     "    weight_path = next((str(p) for p in candidates_weights if p.exists()), None)\n",
                                     "    return repo_path, weight_path\n",
                                     "\n",
                                     "# Re-initialize dinov3_model and model_type before searching to clear previous state\n",
                                     "dinov3_model = None\n",
                                     "model_type = None\n",
                                     "\n",
                                     "repo_path, weight_path = find_dinov3_repo_and_weights()\n",
                                     "print(\u0027DINOv3 repo:\u0027, repo_path)\n",
                                     "print(\u0027DINOv3 weight:\u0027, weight_path)\n",
                                     "\n",
                                     "# Try official DINOv3 if both repo and weights are present\n",
                                     "try:\n",
                                     "    if repo_path:\n",
                                     "        # Ensure the dinov3 library can be imported\n",
                                     "        try:\n",
                                     "            from dinov3.models.vision_transformer import vit_base\n",
                                     "        except ImportError:\n",
                                     "            print(f\"Warning: DINOv3 repository found at {repo_path} but module \u0027dinov3.models.vision_transformer\u0027 could not be imported. Please ensure the repository is correctly set up.\")\n",
                                     "            vit_base = None\n",
                                     "\n",
                                     "        if vit_base:\n",
                                     "            dinov3_model = vit_base(patch_size=16)\n",
                                     "            if weight_path:\n",
                                     "                ckpt = torch.load(weight_path, map_location=\u0027cpu\u0027)\n",
                                     "                missing, unexpected = dinov3_model.load_state_dict(ckpt, strict=False)\n",
                                     "                print(f\u0027Loaded DINOv3 weights. Missing={len(missing)} Unexpected={len(unexpected)}\u0027)\n",
                                     "                model_type = \u0027DINOv3 ViT-B/16 (official)\u0027\n",
                                     "            else:\n",
                                     "                print(\u0027Weights not found; using mock outputs with uninitialized model.\u0027)\n",
                                     "                model_type = \u0027DINOv3 ViT-B/16 (no weights, mock)\u0027\n",
                                     "            dinov3_model.to(device).eval()\n",
                                     "    if not dinov3_model:\n",
                                     "        raise Exception(\u0027Official DINOv3 model or weights not found/loadable.\u0027)\n",
                                     "except Exception as e:\n",
                                     "    print(\u0027âŒ ERROR: Official DINOv3 model could not be loaded.\u0027)\n",
                                     "    print(f\u0027   Details: {e}\u0027)\n",
                                     "    print(\u0027\\nðŸ“‹ Required setup:\u0027)\n",
                                     "    print(\u0027   1. Clone DINOv3 repository to models/dinov3/\u0027)\n",
                                     "    print(\u0027   2. Download DINOv3 weights (dinov3_vitb16.pth) to models/ or checkpoints/\u0027)\n",
                                     "    print(\u0027\\nExiting - DINOv3 is required for this notebook.\u0027)\n",
                                     "    raise RuntimeError(\u0027DINOv3 model not available. Please follow the setup instructions above.\u0027)\n",
                                     "\n",
                                     "print(\u0027âœ“ Model type:\u0027, model_type)\n",
                                     "\n",
                                     "def _model_image_size(model):\n",
                                     "    cfg = getattr(model, \u0027default_cfg\u0027, {})\n",
                                     "    size = cfg.get(\u0027input_size\u0027, (3, 224, 224))\n",
                                     "    if isinstance(size, (list, tuple)) and len(size) == 3:\n",
                                     "        return size[1]\n",
                                     "    return 224\n",
                                     "\n",
                                     "def _model_patch_size(model):\n",
                                     "    ps = getattr(getattr(model, \u0027patch_embed\u0027, None), \u0027patch_size\u0027, (14, 14))\n",
                                     "    if isinstance(ps, (list, tuple)) and len(ps) \u003e= 1:\n",
                                     "        return ps[0]\n",
                                     "    return 14\n",
                                     "\n",
                                     "# Unified feature extractor with optional mock when weights missing\n",
                                     "class UnifiedFeatureExtractor:\n",
                                     "    def __init__(self, model, device):\n",
                                     "        self.model = model\n",
                                     "        self.device = device\n",
                                     "        self.image_size = _model_image_size(model)\n",
                                     "        self.patch_size = _model_patch_size(model)\n",
                                     "        self.feat_dim = getattr(getattr(model, \u0027num_features\u0027, None), \u0027real\u0027, None) or getattr(model, \u0027num_features\u0027, 768)\n",
                                     "        import torchvision.transforms as transforms\n",
                                     "        from PIL import Image\n",
                                     "        self.Image = Image\n",
                                     "        self.transforms = transforms.Compose([\n",
                                     "            transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
                                     "            transforms.CenterCrop(self.image_size),\n",
                                     "            transforms.ToTensor(),\n",
                                     "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                                     "        ])\n",
                                     "        # Corrected logic for use_mock: activate mock if the model itself is None\n",
                                     "        self.use_mock = (self.model is None) or (model_type is not None and \u0027mock\u0027 in model_type)\n",
                                     "\n",
                                     "    def preprocess(self, img):\n",
                                     "        if isinstance(img, torch.Tensor): # Check if input is already a Tensor\n",
                                     "            # Assume it\u0027s already preprocessed by the dataset (normalized, resized, ToTensor)\n",
                                     "            # Just ensure it has a batch dimension and is on the correct device\n",
                                     "            if img.dim() == 3: # (C, H, W)\n",
                                     "                x = img.unsqueeze(0).to(self.device)\n",
                                     "            elif img.dim() == 4: # (B, C, H, W)\n",
                                     "                x = img.to(self.device)\n",
                                     "            else:\n",
                                     "                raise ValueError(f\"Unexpected tensor dimension for image input: {img.dim()}\")\n",
                                     "            return x # Return directly if it\u0027s already a Tensor\n",
                                     "        elif isinstance(img, self.Image.Image):\n",
                                     "            pil = img\n",
                                     "        else: # Assume numpy array if not PIL or Tensor\n",
                                     "            pil = self.Image.fromarray(img)\n",
                                     "        return self.transforms(pil).unsqueeze(0).to(self.device)\n",
                                     "\n",
                                     "    @torch.no_grad()\n",
                                     "    def extract(self, img, normalize=True):\n",
                                     "        x = self.preprocess(img)\n",
                                     "        if self.use_mock:\n",
                                     "            grid = (self.image_size // self.patch_size)\n",
                                     "            torch.manual_seed(42)\n",
                                     "            feats = torch.randn(1, grid, grid, self.feat_dim)\n",
                                     "        else:\n",
                                     "            out = self.model.forward_features(x)\n",
                                     "            if isinstance(out, dict) and \u0027x_norm_patchtokens\u0027 in out:\n",
                                     "                tokens = out[\u0027x_norm_patchtokens\u0027]\n",
                                     "            elif isinstance(out, dict) and \u0027x_norm_clstoken\u0027 in out and \u0027x_norm_patchtokens\u0027 in out:\n",
                                     "                tokens = out[\u0027x_norm_patchtokens\u0027]\n",
                                     "            else:\n",
                                     "                tokens = out[:, 1:, :] if out.dim() == 3 else out\n",
                                     "            grid = int(tokens.shape[1] ** 0.5)\n",
                                     "            feats = tokens.view(1, grid, grid, self.feat_dim)\n",
                                     "        if normalize:\n",
                                     "            feats = F.normalize(feats, p=2, dim=-1)\n",
                                     "        info = {\n",
                                     "            \u0027feature_size\u0027: (feats.shape[2], feats.shape[1]),\n",
                                     "            \u0027processed_size\u0027: (self.image_size, self.image_size),\n",
                                     "            \u0027patch_size\u0027: self.patch_size,\n",
                                     "            \u0027feat_dim\u0027: self.feat_dim\n",
                                     "        }\n",
                                     "        return feats.cpu(), info\n",
                                     "\n",
                                     "feature_extractor = UnifiedFeatureExtractor(dinov3_model, device)\n",
                                     "print(\u0027Feature extractor ready. Mock mode:\u0027, feature_extractor.use_mock, \u0027image_size:\u0027, feature_extractor.image_size, \u0027patch:\u0027, feature_extractor.patch_size)\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  11,
                      "id":  "7270c573",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "7270c573",
                                       "outputId":  "aa23f44a-1c7f-4d13-9048-c5dc2ce01ef0"
                                   },
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "Finetuning disabled. Using pretrained weights only.\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "if ENABLE_FINETUNING:\n",
                                     "    print(\"=\" * 60)\n",
                                     "    print(\"LIGHT FINETUNING ENABLED\")\n",
                                     "    print(\"=\" * 60)\n",
                                     "\n",
                                     "    # Check if we have real model (not mock)\n",
                                     "    if dinov3_model is None or feature_extractor.use_mock:\n",
                                     "        print(\"\\nâš ï¸  Cannot finetune: model is in mock mode or not loaded\")\n",
                                     "        print(\"   Please ensure DINOv3 weights are available\")\n",
                                     "    else:\n",
                                     "        # Path to SPair-71k\n",
                                     "        SPAIR_ROOT = os.path.join(DATA_ROOT if \u0027DATA_ROOT\u0027 in globals() else os.path.join(PROJECT_ROOT, \u0027data\u0027), \u0027SPair-71k\u0027)\n",
                                     "\n",
                                     "        if not os.path.exists(SPAIR_ROOT):\n",
                                     "            print(f\"\\nâš ï¸  SPair-71k not found at: {SPAIR_ROOT}\")\n",
                                     "            print(\"   Download from: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
                                     "        else:\n",
                                     "            print(f\"\\nLoading SPair-71k from: {SPAIR_ROOT}\")\n",
                                     "            train_loader, val_loader = create_spair_dataloaders(\n",
                                     "                SPAIR_ROOT,\n",
                                     "                batch_size=FINETUNE_BATCH_SIZE,\n",
                                     "                num_workers=2,\n",
                                     "                train_subset=FINETUNE_TRAIN_SUBSET,\n",
                                     "                val_subset=500\n",
                                     "            )\n",
                                     "\n",
                                     "            # Unfreeze last k blocks\n",
                                     "            trainable_params = unfreeze_last_k_blocks(dinov3_model, FINETUNE_K_LAYERS, blocks_attr=\u0027blocks\u0027)\n",
                                     "\n",
                                     "            # Setup optimizer\n",
                                     "            optimizer = torch.optim.AdamW(trainable_params, lr=FINETUNE_LR, weight_decay=FINETUNE_WD)\n",
                                     "\n",
                                     "            print(f\"\\nFinetuning configuration:\")\n",
                                     "            print(f\"  Model: {model_type}\")\n",
                                     "            print(f\"  k={FINETUNE_K_LAYERS} layers\")\n",
                                     "            print(f\"  lr={FINETUNE_LR}, wd={FINETUNE_WD}\")\n",
                                     "            print(f\"  epochs={FINETUNE_EPOCHS}\")\n",
                                     "            print(f\"  train samples: {len(train_loader.dataset)}\")\n",
                                     "            print(f\"  val samples: {len(val_loader.dataset)}\")\n",
                                     "\n",
                                     "            patch_size = feature_extractor.patch_size  # 14 or 16\n",
                                     "\n",
                                     "            # Training\n",
                                     "            dinov3_model.train()\n",
                                     "            best_val_loss = float(\u0027inf\u0027)\n",
                                     "\n",
                                     "            for epoch in range(FINETUNE_EPOCHS):\n",
                                     "                epoch_loss = 0.0\n",
                                     "                num_batches = 0\n",
                                     "\n",
                                     "                for batch_idx, batch in enumerate(train_loader):\n",
                                     "                    src_img = batch[\u0027src_img\u0027].to(device)\n",
                                     "                    tgt_img = batch[\u0027tgt_img\u0027].to(device)\n",
                                     "                    src_kps = batch[\u0027src_kps\u0027].to(device)\n",
                                     "                    tgt_kps = batch[\u0027tgt_kps\u0027].to(device)\n",
                                     "\n",
                                     "                    # Extract features\n",
                                     "                    src_out = dinov3_model.get_intermediate_layers(src_img, n=1, return_class_token=False)[0]\n",
                                     "                    tgt_out = dinov3_model.get_intermediate_layers(tgt_img, n=1, return_class_token=False)[0]\n",
                                     "\n",
                                     "                    B = src_img.size(0)\n",
                                     "                    num_patches = src_out.size(1)\n",
                                     "                    grid_size = int(np.sqrt(num_patches))\n",
                                     "\n",
                                     "                    batch_loss = 0.0\n",
                                     "                    num_kps = 0\n",
                                     "\n",
                                     "                    for b in range(B):\n",
                                     "                        src_f = F.normalize(src_out[b], dim=-1)\n",
                                     "                        tgt_f = F.normalize(tgt_out[b], dim=-1)\n",
                                     "\n",
                                     "                        for kp_idx in range(src_kps[b].size(0)):\n",
                                     "                            src_x = int(src_kps[b][kp_idx, 0].item() / patch_size)\n",
                                     "                            src_y = int(src_kps[b][kp_idx, 1].item() / patch_size)\n",
                                     "                            src_x = max(0, min(grid_size - 1, src_x))\n",
                                     "                            src_y = max(0, min(grid_size - 1, src_y))\n",
                                     "\n",
                                     "                            src_patch_idx = src_y * grid_size + src_x\n",
                                     "                            sim = torch.matmul(tgt_f, src_f[src_patch_idx])\n",
                                     "                            sim_2d = sim.view(grid_size, grid_size)\n",
                                     "\n",
                                     "                            gt_xy = tgt_kps[b][kp_idx]\n",
                                     "                            gt_yx = torch.stack([gt_xy[1], gt_xy[0]])\n",
                                     "\n",
                                     "                            loss = compute_keypoint_loss(\n",
                                     "                                sim_2d, grid_size, grid_size, gt_yx, patch_size,\n",
                                     "                                use_soft=True, window=SOFT_WINDOW, tau=SOFT_TAU\n",
                                     "                            )\n",
                                     "                            batch_loss += loss\n",
                                     "                            num_kps += 1\n",
                                     "\n",
                                     "                    if num_kps \u003e 0:\n",
                                     "                        batch_loss = batch_loss / num_kps\n",
                                     "                        optimizer.zero_grad()\n",
                                     "                        batch_loss.backward()\n",
                                     "                        optimizer.step()\n",
                                     "\n",
                                     "                        epoch_loss += batch_loss.item()\n",
                                     "                        num_batches += 1\n",
                                     "\n",
                                     "                    if (batch_idx + 1) % 50 == 0:\n",
                                     "                        print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {epoch_loss/max(1,num_batches):.4f}\")\n",
                                     "\n",
                                     "                avg_train_loss = epoch_loss / max(1, num_batches)\n",
                                     "\n",
                                     "                # Validation\n",
                                     "                dinov3_model.eval()\n",
                                     "                val_loss = 0.0\n",
                                     "                val_batches = 0\n",
                                     "\n",
                                     "                with torch.no_grad():\n",
                                     "                    for batch in val_loader:\n",
                                     "                        src_img = batch[\u0027src_img\u0027].to(device)\n",
                                     "                        tgt_img = batch[\u0027tgt_img\u0027].to(device)\n",
                                     "                        src_kps = batch[\u0027src_kps\u0027].to(device)\n",
                                     "                        tgt_kps = batch[\u0027tgt_kps\u0027].to(device)\n",
                                     "\n",
                                     "                        src_out = dinov3_model.get_intermediate_layers(src_img, n=1, return_class_token=False)[0]\n",
                                     "                        tgt_out = dinov3_model.get_intermediate_layers(tgt_img, n=1, return_class_token=False)[0]\n",
                                     "\n",
                                     "                        num_patches = src_out.size(1)\n",
                                     "                        grid_size = int(np.sqrt(num_patches))\n",
                                     "\n",
                                     "                        batch_val_loss = 0.0\n",
                                     "                        num_kps = 0\n",
                                     "\n",
                                     "                        for b in range(src_img.size(0)):\n",
                                     "                            src_f = F.normalize(src_out[b], dim=-1)\n",
                                     "                            tgt_f = F.normalize(tgt_out[b], dim=-1)\n",
                                     "\n",
                                     "                            for kp_idx in range(src_kps[b].size(0)):\n",
                                     "                                src_x = int(src_kps[b][kp_idx, 0].item() / patch_size)\n",
                                     "                                src_y = int(src_kps[b][kp_idx, 1].item() / patch_size)\n",
                                     "                                src_x = max(0, min(grid_size - 1, src_x))\n",
                                     "                                src_y = max(0, min(grid_size - 1, src_y))\n",
                                     "\n",
                                     "                                src_patch_idx = src_y * grid_size + src_x\n",
                                     "                                sim = torch.matmul(tgt_f, src_f[src_patch_idx])\n",
                                     "                                sim_2d = sim.view(grid_size, grid_size)\n",
                                     "\n",
                                     "                                gt_xy = tgt_kps[b][kp_idx]\n",
                                     "                                gt_yx = torch.stack([gt_xy[1], gt_xy[0]])\n",
                                     "\n",
                                     "                                loss = compute_keypoint_loss(\n",
                                     "                                    sim_2d, grid_size, grid_size, gt_yx, patch_size,\n",
                                     "                                    use_soft=True, window=SOFT_WINDOW, tau=SOFT_TAU\n",
                                     "                                )\n",
                                     "                                batch_val_loss += loss\n",
                                     "                                num_kps += 1\n",
                                     "\n",
                                     "                        if num_kps \u003e 0:\n",
                                     "                            val_loss += (batch_val_loss / num_kps).item()\n",
                                     "                            val_batches += 1\n",
                                     "\n",
                                     "                avg_val_loss = val_loss / max(1, val_batches)\n",
                                     "\n",
                                     "                print(f\"\\nEpoch {epoch+1}/{FINETUNE_EPOCHS}:\")\n",
                                     "                print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
                                     "\n",
                                     "                # Save best\n",
                                     "                if avg_val_loss \u003c best_val_loss:\n",
                                     "                    best_val_loss = avg_val_loss\n",
                                     "                    ckpt_path = os.path.join(OUTPUT_ROOT, f\u0027dinov3_finetuned_k{FINETUNE_K_LAYERS}_best.pth\u0027)\n",
                                     "                    torch.save({\n",
                                     "                        \u0027model_state_dict\u0027: dinov3_model.state_dict(),\n",
                                     "                        \u0027config\u0027: {\u0027k\u0027: FINETUNE_K_LAYERS, \u0027lr\u0027: FINETUNE_LR}\n",
                                     "                    }, ckpt_path)\n",
                                     "                    print(f\"  âœ“ Best model saved\")\n",
                                     "\n",
                                     "                dinov3_model.train()\n",
                                     "\n",
                                     "            dinov3_model.eval()\n",
                                     "            print(f\"\\nâœ… Finetuning completed! Best val loss: {best_val_loss:.4f}\")\n",
                                     "\n",
                                     "else:\n",
                                     "    print(\"Finetuning disabled. Using pretrained weights only.\")"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "998c62c6",
                      "metadata":  {
                                       "id":  "998c62c6"
                                   },
                      "source":  [
                                     "## Light Finetuning (Optional)\n",
                                     "\n",
                                     "If `ENABLE_FINETUNING=True`, this section finetunes the last k transformer blocks on SPair-71k with keypoint supervision."
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  12,
                      "id":  "168a5a39",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/"
                                                 },
                                       "id":  "168a5a39",
                                       "outputId":  "d0fd84de-b1c3-4e7e-88ed-14b44099e4a8"
                                   },
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "Matcher (soft-argmax=disabled), PCK, and visualization ready.\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# 4) Correspondence matching, PCK, and visualization (works with mock)\n",
                                     "\n",
                                     "class CorrespondenceMatcher:\n",
                                     "    def __init__(self, mutual_nn=False, use_soft_argmax=False, soft_window=7, soft_tau=0.05):\n",
                                     "        self.mutual_nn = mutual_nn\n",
                                     "        self.use_soft_argmax = use_soft_argmax\n",
                                     "        self.soft_window = soft_window\n",
                                     "        self.soft_tau = soft_tau\n",
                                     "\n",
                                     "    def match(self, src_feats, tgt_feats):\n",
                                     "        H, W, D = tgt_feats.shape[1], tgt_feats.shape[2], tgt_feats.shape[3]\n",
                                     "        src_flat = src_feats.view(-1, D)\n",
                                     "        tgt_flat = tgt_feats.view(-1, D)\n",
                                     "\n",
                                     "        if self.use_soft_argmax:\n",
                                     "            # Compute similarity on device\n",
                                     "            sim = torch.matmul(src_flat, tgt_flat.T)  # [N, H*W]\n",
                                     "\n",
                                     "            # Use window soft-argmax\n",
                                     "            pred_coords = window_soft_argmax(\n",
                                     "                sim, H, W,\n",
                                     "                window=self.soft_window,\n",
                                     "                tau=self.soft_tau\n",
                                     "            )  # [N, 2] in (y, x) patch coordinates\n",
                                     "\n",
                                     "            x = pred_coords[:, 1].cpu().numpy()\n",
                                     "            y = pred_coords[:, 0].cpu().numpy()\n",
                                     "        else:\n",
                                     "            # Original argmax approach\n",
                                     "            sim = src_flat.numpy() @ tgt_flat.numpy().T\n",
                                     "            best = np.argmax(sim, axis=1)\n",
                                     "            y = best // W\n",
                                     "            x = best % W\n",
                                     "\n",
                                     "        return np.stack([x, y], axis=1)\n",
                                     "\n",
                                     "def pck(pred_kps, gt_kps, alpha=0.1, img_wh=(224,224)):\n",
                                     "    if len(pred_kps)==0 or len(gt_kps)==0:\n",
                                     "        return 0.0\n",
                                     "    d = np.linalg.norm(pred_kps - gt_kps, axis=1)\n",
                                     "    norm = np.sqrt(img_wh[0]**2 + img_wh[1]**2)\n",
                                     "    thr = alpha*norm\n",
                                     "    return float((d\u003c=thr).mean())\n",
                                     "\n",
                                     "def visualize(src_img, tgt_img, src_kps, pred_kps):\n",
                                     "    fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
                                     "    ax[0].imshow(src_img)\n",
                                     "    ax[0].scatter(src_kps[:,0], src_kps[:,1], c=\u0027r\u0027, s=40)\n",
                                     "    ax[0].set_title(\u0027Source\u0027)\n",
                                     "    ax[0].axis(\u0027off\u0027)\n",
                                     "    ax[1].imshow(tgt_img)\n",
                                     "    ax[1].scatter(pred_kps[:,0], pred_kps[:,1], c=\u0027b\u0027, s=40)\n",
                                     "    ax[1].set_title(\u0027Target (Pred)\u0027)\n",
                                     "    ax[1].axis(\u0027off\u0027)\n",
                                     "    plt.tight_layout()\n",
                                     "    return fig\n",
                                     "print(f\u0027Matcher (soft-argmax={\u0027enabled\u0027 if USE_SOFT_ARGMAX else \u0027disabled\u0027}), PCK, and visualization ready.\u0027)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  13,
                      "id":  "19838d23",
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/",
                                                     "height":  558
                                                 },
                                       "id":  "19838d23",
                                       "outputId":  "5f1a332d-d789-4013-ccc9-f5ee2abfb652"
                                   },
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "PCK@0.1: 0.2500 (mock data)\n"
                                                   ]
                                      },
                                      {
                                          "output_type":  "display_data",
                                          "data":  {
                                                       "text/plain":  [
                                                                          "\u003cFigure size 1000x500 with 2 Axes\u003e"
                                                                      ],
                                                       "image/png":  "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\n"
                                                   },
                                          "metadata":  {

                                                       }
                                      },
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "âœ“ End-to-end sanity check completed.\n"
                                                   ]
                                      }
                                  ],
                      "source":  [
                                     "# 5) End-to-end sanity check with mock data\n",
                                     "\n",
                                     "# Create synthetic images and keypoints\n",
                                     "src = Image.new(\u0027RGB\u0027, (224,224), color=(200,200,220))\n",
                                     "tgt = Image.new(\u0027RGB\u0027, (224,224), color=(210,190,200))\n",
                                     "src_kps = np.array([[30,30],[60,120],[150,80],[200,200]], dtype=float)\n",
                                     "gt_kps  = np.array([[40,40],[70,130],[160,90],[210,210]], dtype=float)  # fake GT\n",
                                     "\n",
                                     "# Extract features (mock if weights missing)\n",
                                     "src_feats, _ = feature_extractor.extract(src)  # (1,H,W,D)\n",
                                     "tgt_feats, _ = feature_extractor.extract(tgt)\n",
                                     "src_feats = src_feats[0]\n",
                                     "tgt_feats = tgt_feats[0]\n",
                                     "H, W, D = src_feats.shape\n",
                                     "\n",
                                     "# Map keypoints to feature grid\n",
                                     "patch = feature_extractor.patch_size\n",
                                     "def kps_to_feat(kps):\n",
                                     "    k = kps.copy()\n",
                                     "    k[:,0] = np.clip(np.round(k[:,0] / patch), 0, W-1)\n",
                                     "    k[:,1] = np.clip(np.round(k[:,1] / patch), 0, H-1)\n",
                                     "    return k.astype(int)\n",
                                     "\n",
                                     "src_feat_idx = kps_to_feat(src_kps)\n",
                                     "tgt_feat_idx_gt = kps_to_feat(gt_kps)\n",
                                     "\n",
                                     "# Gather source descriptors at keypoints\n",
                                     "src_desc = src_feats[src_feat_idx[:,1], src_feat_idx[:,0], :]  # (K,D)\n",
                                     "\n",
                                     "# Match only for keypoints with configuration\n",
                                     "matcher = CorrespondenceMatcher(\n",
                                     "    mutual_nn=False,\n",
                                     "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
                                     "    soft_window=SOFT_WINDOW if USE_SOFT_ARGMAX else 7,\n",
                                     "    soft_tau=SOFT_TAU if USE_SOFT_ARGMAX else 0.05\n",
                                     ")\n",
                                     "pred_feat_coords = matcher.match(src_desc[None, None, ...], tgt_feats[None, ...])\n",
                                     "pred_feat_coords = pred_feat_coords[:len(src_kps)]  # (K,2)\n",
                                     "\n",
                                     "# Map feature coords back to image pixels (center of patch)\n",
                                     "pred_kps = np.stack([pred_feat_coords[:,0]*patch + patch//2,\n",
                                     "                      pred_feat_coords[:,1]*patch + patch//2], axis=1).astype(float)\n",
                                     "\n",
                                     "# Compute PCK\n",
                                     "alpha_val = 0.1\n",
                                     "pck_val = pck(pred_kps, gt_kps, alpha=alpha_val, img_wh=(224,224))\n",
                                     "print(f\u0027PCK@{alpha_val}: {pck_val:.4f} (mock data)\u0027)\n",
                                     "\n",
                                     "# Visualize\n",
                                     "fig = visualize(np.array(src), np.array(tgt), src_kps, pred_kps)\n",
                                     "plt.show()\n",
                                     "print(\u0027âœ“ End-to-end sanity check completed.\u0027)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "source":  [
                                     "# Helper function to denormalize images for visualization\n",
                                     "def denorm_show(img_tensor):\n",
                                     "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
                                     "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
                                     "    img = img_tensor * std + mean\n",
                                     "    img = torch.clamp(img, 0, 1)\n",
                                     "    return img.permute(1, 2, 0).cpu().numpy()\n",
                                     "\n",
                                     "\n",
                                     "class PCKEvaluator:\n",
                                     "    \"\"\"Evaluator for PCK metrics at different alpha thresholds.\"\"\"\n",
                                     "\n",
                                     "    def __init__(self, alphas=[0.05, 0.10, 0.15]):\n",
                                     "        self.alphas = alphas\n",
                                     "\n",
                                     "    def compute_pck(self, pred_kps, gt_kps, alpha, img_size=224):\n",
                                     "        \"\"\"Compute PCK for given alpha threshold.\"\"\"\n",
                                     "        distances = np.linalg.norm(pred_kps - gt_kps, axis=1)\n",
                                     "        # Compute normalization factor (same as DINOv2)\n",
                                     "        norm_factor = np.sqrt(img_size**2 + img_size**2)\n",
                                     "\n",
                                     "        # Normalize distances (same as DINOv2)\n",
                                     "        normalized_distances = distances / (norm_factor + 1e-8)\n",
                                     "\n",
                                     "        # Compare with alpha threshold\n",
                                     "        threshold = alpha\n",
                                     "        return (normalized_distances \u003c= threshold).astype(float), normalized_distances\n",
                                     "\n",
                                     "    def evaluate_dataset(self, all_predictions, all_ground_truths):\n",
                                     "        \"\"\"Evaluate PCK across entire dataset.\"\"\"\n",
                                     "        all_correct = {alpha: [] for alpha in self.alphas}\n",
                                     "        all_distances = []\n",
                                     "\n",
                                     "        for pred_kps, gt_kps in zip(all_predictions, all_ground_truths):\n",
                                     "            for alpha in self.alphas:\n",
                                     "                correct, distances = self.compute_pck(pred_kps, gt_kps, alpha)\n",
                                     "                all_correct[alpha].extend(correct)\n",
                                     "                if alpha == self.alphas[0]:  # Solo una volta\n",
                                     "                    all_distances.extend(distances)\n",
                                     "\n",
                                     "        # Compute average PCK for each alpha\n",
                                     "        avg_pck = {f\u0027PCK@{alpha}\u0027: np.mean(all_correct[alpha])\n",
                                     "                   for alpha in self.alphas}\n",
                                     "\n",
                                     "        per_sample_pck = {f\u0027PCK@{alpha}\u0027: all_correct[alpha]\n",
                                     "                          for alpha in self.alphas}\n",
                                     "\n",
                                     "        return avg_pck, per_sample_pck, np.array(all_distances)\n",
                                     "\n",
                                     "\n",
                                     "def evaluate_on_dataset(dataset, feature_extractor, matcher, evaluator,\n",
                                     "                       max_samples=None, save_visualizations=False):\n",
                                     "    \"\"\"\n",
                                     "    Evaluate correspondence on entire dataset.\n",
                                     "\n",
                                     "    Args:\n",
                                     "        dataset: SPairDataset instance\n",
                                     "        feature_extractor: UnifiedFeatureExtractor instance\n",
                                     "        matcher: CorrespondenceMatcher instance\n",
                                     "        evaluator: PCKEvaluator instance\n",
                                     "        max_samples: Maximum samples to evaluate (None = all)\n",
                                     "        save_visualizations: Whether to save sample visualizations\n",
                                     "\n",
                                     "    Returns:\n",
                                     "        results: Dictionary with evaluation metrics\n",
                                     "    \"\"\"\n",
                                     "    print(f\"Evaluating on {len(dataset)} samples...\")\n",
                                     "\n",
                                     "    all_predictions = []\n",
                                     "    all_ground_truths = []\n",
                                     "    all_confidences = []\n",
                                     "\n",
                                     "    num_samples = min(max_samples, len(dataset)) if max_samples else len(dataset)\n",
                                     "\n",
                                     "    for idx in tqdm(range(num_samples), desc=\"Evaluating\"):\n",
                                     "        sample = dataset[idx]\n",
                                     "\n",
                                     "        src_img = sample[\u0027src_img\u0027]\n",
                                     "        tgt_img = sample[\u0027tgt_img\u0027]\n",
                                     "        src_kps = sample[\u0027src_kps\u0027]\n",
                                     "        tgt_kps = sample[\u0027tgt_kps\u0027]\n",
                                     "\n",
                                     "        # Get valid keypoints\n",
                                     "        valid_mask = (src_kps[:, 0] \u003e= 0) \u0026 (src_kps[:, 1] \u003e= 0)\n",
                                     "        valid_src_kps = src_kps[valid_mask]\n",
                                     "        valid_tgt_kps = tgt_kps[valid_mask]\n",
                                     "\n",
                                     "        if len(valid_src_kps) == 0:\n",
                                     "            continue\n",
                                     "\n",
                                     "        # Extract features usando il feature extractor\n",
                                     "        src_feats, _ = feature_extractor.extract(src_img)\n",
                                     "        tgt_feats, _ = feature_extractor.extract(tgt_img)\n",
                                     "\n",
                                     "        # Remove batch dimension\n",
                                     "        src_feats = src_feats[0]  # (H, W, D)\n",
                                     "        tgt_feats = tgt_feats[0]  # (H, W, D)\n",
                                     "\n",
                                     "        H, W, D = tgt_feats.shape\n",
                                     "        patch_size = feature_extractor.patch_size\n",
                                     "\n",
                                     "        # Map keypoints to feature grid\n",
                                     "        def kps_to_feat(kps):\n",
                                     "            k = kps.cpu().numpy() if isinstance(kps, torch.Tensor) else kps.copy()\n",
                                     "            k[:, 0] = np.clip(np.round(k[:, 0] / patch_size), 0, W - 1)\n",
                                     "            k[:, 1] = np.clip(np.round(k[:, 1] / patch_size), 0, H - 1)\n",
                                     "            return k.astype(int)\n",
                                     "\n",
                                     "        src_feat_idx = kps_to_feat(valid_src_kps)\n",
                                     "\n",
                                     "        # Gather source descriptors at keypoints\n",
                                     "        src_desc = src_feats[src_feat_idx[:, 1], src_feat_idx[:, 0], :]  # (K, D)\n",
                                     "\n",
                                     "        # Match\n",
                                     "        pred_feat_coords = matcher.match(src_desc[None, None, ...], tgt_feats[None, ...])\n",
                                     "        pred_feat_coords = pred_feat_coords[:len(valid_src_kps)]  # (K, 2)\n",
                                     "\n",
                                     "        # Map feature coords back to image pixels (center of patch)\n",
                                     "        pred_kps = np.stack([\n",
                                     "            pred_feat_coords[:, 0] * patch_size + patch_size // 2,\n",
                                     "            pred_feat_coords[:, 1] * patch_size + patch_size // 2\n",
                                     "        ], axis=1).astype(float)\n",
                                     "\n",
                                     "        # Compute confidence scores based on similarity\n",
                                     "        src_flat = src_desc.view(-1, D).cpu()\n",
                                     "        tgt_flat = tgt_feats.view(-1, D).cpu()\n",
                                     "        sim = torch.matmul(src_flat, tgt_flat.T)\n",
                                     "        confidences = sim.max(dim=1)[0].numpy()  # Max similarity per ogni keypoint\n",
                                     "\n",
                                     "        all_predictions.append(pred_kps)\n",
                                     "\n",
                                     "        # Convert valid_tgt_kps to numpy array before appending\n",
                                     "        if isinstance(valid_tgt_kps, torch.Tensor):\n",
                                     "            all_ground_truths.append(valid_tgt_kps.cpu().numpy())\n",
                                     "        else:\n",
                                     "            all_ground_truths.append(valid_tgt_kps)\n",
                                     "\n",
                                     "        all_confidences.append(confidences)\n",
                                     "\n",
                                     "        # Save visualization for first few samples\n",
                                     "        if save_visualizations and idx \u003c 5:\n",
                                     "            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
                                     "\n",
                                     "            axes[0].imshow(denorm_show(src_img))\n",
                                     "            valid_src_kps_np = valid_src_kps.cpu().numpy() if isinstance(valid_src_kps, torch.Tensor) else valid_src_kps\n",
                                     "            axes[0].scatter(valid_src_kps_np[:, 0], valid_src_kps_np[:, 1],\n",
                                     "                           c=\u0027red\u0027, s=100, marker=\u0027x\u0027, linewidths=3)\n",
                                     "            axes[0].set_title(f\u0027Source (Sample {idx})\u0027, fontsize=12, fontweight=\u0027bold\u0027)\n",
                                     "            axes[0].axis(\u0027off\u0027)\n",
                                     "\n",
                                     "            axes[1].imshow(denorm_show(tgt_img))\n",
                                     "            valid_tgt_kps_np = valid_tgt_kps.cpu().numpy() if isinstance(valid_tgt_kps, torch.Tensor) else valid_tgt_kps\n",
                                     "            axes[1].scatter(valid_tgt_kps_np[:, 0], valid_tgt_kps_np[:, 1],\n",
                                     "                           c=\u0027lime\u0027, s=100, marker=\u0027o\u0027, alpha=0.6, linewidths=2,\n",
                                     "                           edgecolors=\u0027darkgreen\u0027, label=\u0027GT\u0027)\n",
                                     "            axes[1].scatter(pred_kps[:, 0], pred_kps[:, 1],\n",
                                     "                           c=\u0027red\u0027, s=80, marker=\u0027x\u0027, linewidths=2, label=\u0027Pred\u0027)\n",
                                     "            axes[1].set_title(f\u0027Target (Sample {idx})\u0027, fontsize=12, fontweight=\u0027bold\u0027)\n",
                                     "            axes[1].axis(\u0027off\u0027)\n",
                                     "            axes[1].legend()\n",
                                     "\n",
                                     "            plt.tight_layout()\n",
                                     "            plt.savefig(os.path.join(OUTPUT_ROOT, f\u0027match_sample_{idx}.png\u0027),\n",
                                     "                       dpi=150, bbox_inches=\u0027tight\u0027)\n",
                                     "            plt.close()\n",
                                     "\n",
                                     "    # Evaluate\n",
                                     "    print(\"\\nComputing PCK metrics...\")\n",
                                     "    avg_pck, per_sample_pck, all_distances = evaluator.evaluate_dataset(\n",
                                     "        all_predictions, all_ground_truths\n",
                                     "    )\n",
                                     "\n",
                                     "    # Compute additional statistics\n",
                                     "    all_confidences_flat = np.concatenate(all_confidences)\n",
                                     "\n",
                                     "    results = {\n",
                                     "        \u0027avg_pck\u0027: avg_pck,\n",
                                     "        \u0027num_samples\u0027: num_samples,\n",
                                     "        \u0027num_keypoints\u0027: len(all_distances),\n",
                                     "        \u0027avg_confidence\u0027: float(all_confidences_flat.mean()),\n",
                                     "        \u0027distance_stats\u0027: {\n",
                                     "            \u0027mean\u0027: float(all_distances.mean()),\n",
                                     "            \u0027std\u0027: float(all_distances.std()),\n",
                                     "            \u0027median\u0027: float(np.median(all_distances)),\n",
                                     "            \u0027min\u0027: float(all_distances.min()),\n",
                                     "            \u0027max\u0027: float(all_distances.max())\n",
                                     "        }\n",
                                     "    }\n",
                                     "\n",
                                     "    return results\n"
                                 ],
                      "metadata":  {
                                       "id":  "1nhLv__DfUI6"
                                   },
                      "id":  "1nhLv__DfUI6",
                      "execution_count":  14,
                      "outputs":  [

                                  ]
                  },
                  {
                      "cell_type":  "markdown",
                      "source":  [
                                     "## EVALUATION ON 50 SAMPLES"
                                 ],
                      "metadata":  {
                                       "id":  "KJgnWBYgQjXO"
                                   },
                      "id":  "KJgnWBYgQjXO"
                  },
                  {
                      "cell_type":  "code",
                      "source":  [
                                     "# Create evaluator\n",
                                     "evaluator = PCKEvaluator(alphas=[0.05, 0.10, 0.15])\n",
                                     "\n",
                                     "# Create dataset\n",
                                     "print(\"Wait, loading pairs (takes some minutes)...\")\n",
                                     "SPAIR_ROOT = os.path.join(DATA_ROOT, \u0027SPair-71k\u0027)\n",
                                     "dataset = SPairDataset(SPAIR_ROOT, split=\u0027test\u0027, subset=50)\n",
                                     "\n",
                                     "# Create matcher\n",
                                     "matcher = CorrespondenceMatcher(\n",
                                     "    mutual_nn=False,\n",
                                     "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
                                     "    soft_window=SOFT_WINDOW,\n",
                                     "    soft_tau=SOFT_TAU\n",
                                     ")\n",
                                     "\n",
                                     "# Run evaluation on small subset\n",
                                     "print(\"Running evaluation on 50 samples...\")\n",
                                     "results = evaluate_on_dataset(\n",
                                     "    dataset=dataset,\n",
                                     "    feature_extractor=feature_extractor,\n",
                                     "    matcher=matcher,\n",
                                     "    evaluator=evaluator,\n",
                                     "    max_samples=50,\n",
                                     "    save_visualizations=True\n",
                                     ")\n",
                                     "\n",
                                     "print(\"\\n\" + \"=\"*60)\n",
                                     "print(\"EVALUATION RESULTS (50 samples)\")\n",
                                     "print(\"=\"*60)\n",
                                     "print(f\"\\nPCK Metrics:\")\n",
                                     "for key, value in results[\u0027avg_pck\u0027].items():\n",
                                     "    print(f\"  {key}: {value:.4f} ({value*100:.2f}%)\")\n",
                                     "\n",
                                     "print(f\"\\nDataset Statistics:\")\n",
                                     "print(f\"  Samples evaluated: {results[\u0027num_samples\u0027]}\")\n",
                                     "print(f\"  Total keypoints: {results[\u0027num_keypoints\u0027]}\")\n",
                                     "print(f\"  Average confidence: {results[\u0027avg_confidence\u0027]:.4f}\")\n",
                                     "\n",
                                     "print(f\"\\nDistance Statistics:\")\n",
                                     "for key, value in results[\u0027distance_stats\u0027].items():\n",
                                     "    print(f\"  {key}: {value:.4f}\")\n",
                                     "\n",
                                     "# Save results\n",
                                     "results_path = os.path.join(OUTPUT_ROOT, \u0027evaluation_results_subset.json\u0027)\n",
                                     "with open(results_path, \u0027w\u0027) as f:\n",
                                     "    json.dump(results, f, indent=2)\n",
                                     "print(f\"\\nâœ“ Results saved to {results_path}\")"
                                 ],
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/",
                                                     "height":  553,
                                                     "referenced_widgets":  [
                                                                                "71e69cbaf210466c85805734c40ad0eb",
                                                                                "d17577abd2004e7e9ae045cd5b0fe0f9",
                                                                                "278b579937e240b090ca72d37978fc3b",
                                                                                "8b4715a06dec4395ba07e1ca61a8f113",
                                                                                "2dc08ac13b484ddeb9656dc92ed3eff7",
                                                                                "77f57549dbe947fbab1d2d3d76ceb0dc",
                                                                                "14c6eb88e29b4fdf8fe1c66caec2fda4",
                                                                                "5624311922e24b218703db80736853cb",
                                                                                "22497899ca6b43fbadb140431b336e78",
                                                                                "42eab1100e2b4f82bd0157756c192171",
                                                                                "24877ce20fc24cd1b6d28729c97cf0c0"
                                                                            ]
                                                 },
                                       "id":  "PKpOs3jUQeBs",
                                       "outputId":  "e4802ceb-ae8e-4217-957c-35e12b050f67"
                                   },
                      "id":  "PKpOs3jUQeBs",
                      "execution_count":  16,
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "Wait, loading pairs...\n",
                                                       "SPair-71k test dataset: 50 pairs loaded\n",
                                                       "Running evaluation on 50 samples...\n",
                                                       "Evaluating on 50 samples...\n"
                                                   ]
                                      },
                                      {
                                          "output_type":  "display_data",
                                          "data":  {
                                                       "text/plain":  [
                                                                          "Evaluating:   0%|          | 0/50 [00:00\u003c?, ?it/s]"
                                                                      ],
                                                       "application/vnd.jupyter.widget-view+json":  {
                                                                                                        "version_major":  2,
                                                                                                        "version_minor":  0,
                                                                                                        "model_id":  "71e69cbaf210466c85805734c40ad0eb"
                                                                                                    }
                                                   },
                                          "metadata":  {

                                                       }
                                      },
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "\n",
                                                       "Computing PCK metrics...\n",
                                                       "\n",
                                                       "============================================================\n",
                                                       "EVALUATION RESULTS (50 samples)\n",
                                                       "============================================================\n",
                                                       "\n",
                                                       "PCK Metrics:\n",
                                                       "  PCK@0.05: 0.0272 (2.72%)\n",
                                                       "  PCK@0.1: 0.0845 (8.45%)\n",
                                                       "  PCK@0.15: 0.1553 (15.53%)\n",
                                                       "\n",
                                                       "Dataset Statistics:\n",
                                                       "  Samples evaluated: 50\n",
                                                       "  Total keypoints: 367\n",
                                                       "  Average confidence: 0.9756\n",
                                                       "\n",
                                                       "Distance Statistics:\n",
                                                       "  mean: 0.3264\n",
                                                       "  std: 0.1612\n",
                                                       "  median: 0.3292\n",
                                                       "  min: 0.0061\n",
                                                       "  max: 0.8162\n",
                                                       "\n",
                                                       "âœ“ Results saved to /content/drive/MyDrive/AMLProject/outputs/evaluation_results_subset.json\n"
                                                   ]
                                      }
                                  ]
                  },
                  {
                      "cell_type":  "markdown",
                      "source":  [
                                     "##EVALUATION ON 600 SAMPLES"
                                 ],
                      "metadata":  {
                                       "id":  "piucxOkbQpSV"
                                   },
                      "id":  "piucxOkbQpSV"
                  },
                  {
                      "cell_type":  "code",
                      "source":  [
                                     "# ðŸš€ EVALUATION: Evaluation on 600 pairs\n",
                                     "print(\"=\"*70)\n",
                                     "print(\"ðŸ”¥ RUNNING EVALUATION ON 600 VALIDATION PAIRS\")\n",
                                     "print(\"=\"*70)\n",
                                     "\n",
                                     "# Create evaluator\n",
                                     "evaluator = PCKEvaluator(alphas=[0.05, 0.10, 0.15])\n",
                                     "\n",
                                     "# Create dataset with 600 pairs\n",
                                     "print(\"Wait, loading pairs (takes some minutes)...\")\n",
                                     "SPAIR_ROOT = os.path.join(DATA_ROOT, \u0027SPair-71k\u0027)\n",
                                     "eval_dataset = SPairDataset(SPAIR_ROOT, split=\u0027test\u0027, subset=600)\n",
                                     "\n",
                                     "print(f\"\\nðŸ“Š Dataset size: {len(eval_dataset)} pairs\")\n",
                                     "print(f\"â±ï¸  Estimated time: ~{len(eval_dataset)*0.5/60:.1f} minutes\\n\")\n",
                                     "\n",
                                     "# Create matcher\n",
                                     "matcher = CorrespondenceMatcher(\n",
                                     "    mutual_nn=False,\n",
                                     "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
                                     "    soft_window=SOFT_WINDOW,\n",
                                     "    soft_tau=SOFT_TAU\n",
                                     ")\n",
                                     "\n",
                                     "# Run evaluation\n",
                                     "print(\"Starting evaluation...\")\n",
                                     "results = evaluate_on_dataset(\n",
                                     "    dataset=eval_dataset,\n",
                                     "    feature_extractor=feature_extractor,\n",
                                     "    matcher=matcher,\n",
                                     "    evaluator=evaluator,\n",
                                     "    max_samples=None,  # All samples from subset\n",
                                     "    save_visualizations=True  # Save first 5 visualizations\n",
                                     ")\n",
                                     "\n",
                                     "# Display results\n",
                                     "print(\"\\n\" + \"=\"*70)\n",
                                     "print(\"ðŸ“ˆ EVALUATION RESULTS (600 pairs)\")\n",
                                     "print(\"=\"*70)\n",
                                     "\n",
                                     "print(f\"\\nðŸŽ¯ PCK Metrics:\")\n",
                                     "for key, value in results[\u0027avg_pck\u0027].items():\n",
                                     "    print(f\"   {key}: {value:.4f} ({value*100:.2f}%)\")\n",
                                     "\n",
                                     "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
                                     "print(f\"   Samples evaluated: {results[\u0027num_samples\u0027]}\")\n",
                                     "print(f\"   Total keypoints: {results[\u0027num_keypoints\u0027]}\")\n",
                                     "print(f\"   Average confidence: {results[\u0027avg_confidence\u0027]:.4f}\")\n",
                                     "\n",
                                     "print(f\"\\nðŸ“ Distance Statistics (pixels):\")\n",
                                     "for key, value in results[\u0027distance_stats\u0027].items():\n",
                                     "    print(f\"   {key}: {value:.2f}\")\n",
                                     "\n",
                                     "# Save results\n",
                                     "results_path = os.path.join(OUTPUT_ROOT, \u0027evaluation_results_600.json\u0027)\n",
                                     "with open(results_path, \u0027w\u0027) as f:\n",
                                     "    json.dump(results, f, indent=2)\n",
                                     "\n",
                                     "print(f\"\\nðŸ’¾ Results saved to: {results_path}\")\n",
                                     "print(\"=\"*70)"
                                 ],
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/",
                                                     "height":  674,
                                                     "referenced_widgets":  [
                                                                                "aef8721d7fad408ab1561d69fbc27fa8",
                                                                                "8328f9ec63564207be81036a54b50a77",
                                                                                "1a7ae4e09cfd49e2bcfd74f6799899d0",
                                                                                "2aa5d654f17549cab12a6ee0562c76ce",
                                                                                "5bc9727588c945068d3dd6a24869bc04",
                                                                                "c341dba73149445b81436c9bded0d01c",
                                                                                "761ca6dfad47407da109a35e0ed10f86",
                                                                                "a899ff9d22f94574ae62f95f3823b7f8",
                                                                                "9716b48642104f61813741cf188fa113",
                                                                                "7820e5708a52434d8a661fa6a0bb184c",
                                                                                "4ebda1d4d29f454aafda63e90a6809c5"
                                                                            ]
                                                 },
                                       "id":  "lZRYxR9k-SNN",
                                       "outputId":  "3922d235-8a4e-4bf3-e9a0-94eddd8de4af"
                                   },
                      "id":  "lZRYxR9k-SNN",
                      "execution_count":  null,
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "======================================================================\n",
                                                       "ðŸ”¥ RUNNING EVALUATION ON 600 VALIDATION PAIRS\n",
                                                       "======================================================================\n",
                                                       "SPair-71k val dataset: 600 pairs loaded\n",
                                                       "\n",
                                                       "ðŸ“Š Dataset size: 600 pairs\n",
                                                       "â±ï¸  Estimated time: ~5.0 minutes\n",
                                                       "\n",
                                                       "Starting evaluation...\n",
                                                       "Evaluating on 600 samples...\n"
                                                   ]
                                      },
                                      {
                                          "output_type":  "display_data",
                                          "data":  {
                                                       "text/plain":  [
                                                                          "Evaluating:   0%|          | 0/600 [00:00\u003c?, ?it/s]"
                                                                      ],
                                                       "application/vnd.jupyter.widget-view+json":  {
                                                                                                        "version_major":  2,
                                                                                                        "version_minor":  0,
                                                                                                        "model_id":  "aef8721d7fad408ab1561d69fbc27fa8"
                                                                                                    }
                                                   },
                                          "metadata":  {

                                                       }
                                      },
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "\n",
                                                       "Computing PCK metrics...\n",
                                                       "\n",
                                                       "======================================================================\n",
                                                       "ðŸ“ˆ EVALUATION RESULTS (600 pairs)\n",
                                                       "======================================================================\n",
                                                       "\n",
                                                       "ðŸŽ¯ PCK Metrics:\n",
                                                       "   PCK@0.05: 0.0311 (3.11%)\n",
                                                       "   PCK@0.1: 0.1045 (10.45%)\n",
                                                       "   PCK@0.15: 0.1925 (19.25%)\n",
                                                       "\n",
                                                       "ðŸ“Š Dataset Statistics:\n",
                                                       "   Samples evaluated: 600\n",
                                                       "   Total keypoints: 5438\n",
                                                       "   Average confidence: 0.9761\n",
                                                       "\n",
                                                       "ðŸ“ Distance Statistics (pixels):\n",
                                                       "   mean: 0.31\n",
                                                       "   std: 0.16\n",
                                                       "   median: 0.30\n",
                                                       "   min: 0.00\n",
                                                       "   max: 0.81\n",
                                                       "\n",
                                                       "ðŸ’¾ Results saved to: /content/drive/MyDrive/AMLProject/outputs/evaluation_results_600.json\n",
                                                       "======================================================================\n"
                                                   ]
                                      }
                                  ]
                  },
                  {
                      "cell_type":  "markdown",
                      "source":  [
                                     "##EVALUATION ON WHOLE DATASET"
                                 ],
                      "metadata":  {
                                       "id":  "-YoxAVA7QuqM"
                                   },
                      "id":  "-YoxAVA7QuqM"
                  },
                  {
                      "cell_type":  "code",
                      "source":  [
                                     "# ðŸš€ FULL EVALUATION: Evaluation on whole dataset\n",
                                     "print(\"=\"*70)\n",
                                     "print(\"ðŸ”¥ RUNNING FULL EVALUATION ON ENTIRE VALIDATION SET\")\n",
                                     "print(\"=\"*70)\n",
                                     "\n",
                                     "# Create evaluator\n",
                                     "evaluator = PCKEvaluator(alphas=[0.05, 0.10, 0.15])\n",
                                     "\n",
                                     "# Create complete dataset  (no subset)\n",
                                     "print(\"Wait, loading pairs (takes some minutes)...\")\n",
                                     "SPAIR_ROOT = os.path.join(DATA_ROOT, \u0027SPair-71k\u0027)\n",
                                     "full_dataset = SPairDataset(SPAIR_ROOT, split=\u0027test\u0027, subset=None)\n",
                                     "\n",
                                     "print(f\"\\nðŸ“Š Dataset size: {len(full_dataset)} pairs\")\n",
                                     "print(f\"â±ï¸  Estimated time: ~{len(full_dataset)*0.5/60:.1f} minutes\\n\")\n",
                                     "\n",
                                     "# Create matcher\n",
                                     "matcher = CorrespondenceMatcher(\n",
                                     "    mutual_nn=False,\n",
                                     "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
                                     "    soft_window=SOFT_WINDOW,\n",
                                     "    soft_tau=SOFT_TAU\n",
                                     ")\n",
                                     "\n",
                                     "# Run evaluation on full dataset\n",
                                     "print(\"Starting full evaluation...\")\n",
                                     "full_results = evaluate_on_dataset(\n",
                                     "    dataset=full_dataset,\n",
                                     "    feature_extractor=feature_extractor,\n",
                                     "    matcher=matcher,\n",
                                     "    evaluator=evaluator,\n",
                                     "    max_samples=None,  # All samples\n",
                                     "    save_visualizations=True  # Save first 5 visualizations\n",
                                     ")\n",
                                     "\n",
                                     "# Display results\n",
                                     "print(\"\\n\" + \"=\"*70)\n",
                                     "print(\"ðŸ“ˆ FULL EVALUATION RESULTS\")\n",
                                     "print(\"=\"*70)\n",
                                     "\n",
                                     "print(f\"\\nðŸŽ¯ PCK Metrics:\")\n",
                                     "for key, value in full_results[\u0027avg_pck\u0027].items():\n",
                                     "    print(f\"   {key}: {value:.4f} ({value*100:.2f}%)\")\n",
                                     "\n",
                                     "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
                                     "print(f\"   Samples evaluated: {full_results[\u0027num_samples\u0027]}\")\n",
                                     "print(f\"   Total keypoints: {full_results[\u0027num_keypoints\u0027]}\")\n",
                                     "print(f\"   Average confidence: {full_results[\u0027avg_confidence\u0027]:.4f}\")\n",
                                     "\n",
                                     "print(f\"\\nðŸ“ Distance Statistics (pixels):\")\n",
                                     "for key, value in full_results[\u0027distance_stats\u0027].items():\n",
                                     "    print(f\"   {key}: {value:.4f}\")\n",
                                     "\n",
                                     "# Save results\n",
                                     "results_path = os.path.join(OUTPUT_ROOT, \u0027evaluation_results_full.json\u0027)\n",
                                     "with open(results_path, \u0027w\u0027) as f:\n",
                                     "    json.dump(full_results, f, indent=2)\n",
                                     "\n",
                                     "print(f\"\\nðŸ’¾ Results saved to: {results_path}\")\n",
                                     "print(\"=\"*70)"
                                 ],
                      "metadata":  {
                                       "colab":  {
                                                     "base_uri":  "https://localhost:8080/",
                                                     "height":  674,
                                                     "referenced_widgets":  [
                                                                                "a9bc61c1e53f4777a1e91067656dbf6a",
                                                                                "e5dde25129274ee0b0b35d2a8a162d80",
                                                                                "f4f3a1b0331c4d3abeb453ff801518a3",
                                                                                "148299fb47644aaf85e5eef05efd2a97",
                                                                                "bce67cb260214522a87c8cc91051a519",
                                                                                "b9818a546b0c4d789e92d3403d7637d5",
                                                                                "329302b8a6e0420ba7e6e36770383d6b",
                                                                                "4baf7580f3364f70b972815ad7462116",
                                                                                "30c0e7735169427394406c098e5ab72d",
                                                                                "091e86533988474d8bed47d7ea7e761d",
                                                                                "afaf02d16222486a888ccde397fd355f"
                                                                            ]
                                                 },
                                       "id":  "IOBlOQFR7CQr",
                                       "outputId":  "295660e9-7a4d-46b4-faca-322217963fb8"
                                   },
                      "id":  "IOBlOQFR7CQr",
                      "execution_count":  null,
                      "outputs":  [
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "======================================================================\n",
                                                       "ðŸ”¥ RUNNING FULL EVALUATION ON ENTIRE VALIDATION SET\n",
                                                       "======================================================================\n",
                                                       "SPair-71k test dataset: 12234 pairs loaded\n",
                                                       "\n",
                                                       "ðŸ“Š Dataset size: 12234 pairs\n",
                                                       "â±ï¸  Estimated time: ~102.0 minutes\n",
                                                       "\n",
                                                       "Starting full evaluation...\n",
                                                       "Evaluating on 12234 samples...\n"
                                                   ]
                                      },
                                      {
                                          "output_type":  "display_data",
                                          "data":  {
                                                       "text/plain":  [
                                                                          "Evaluating:   0%|          | 0/12234 [00:00\u003c?, ?it/s]"
                                                                      ],
                                                       "application/vnd.jupyter.widget-view+json":  {
                                                                                                        "version_major":  2,
                                                                                                        "version_minor":  0,
                                                                                                        "model_id":  "a9bc61c1e53f4777a1e91067656dbf6a"
                                                                                                    }
                                                   },
                                          "metadata":  {

                                                       }
                                      },
                                      {
                                          "output_type":  "stream",
                                          "name":  "stdout",
                                          "text":  [
                                                       "\n",
                                                       "Computing PCK metrics...\n",
                                                       "\n",
                                                       "======================================================================\n",
                                                       "ðŸ“ˆ FULL EVALUATION RESULTS\n",
                                                       "======================================================================\n",
                                                       "\n",
                                                       "ðŸŽ¯ PCK Metrics:\n",
                                                       "   PCK@0.05: 0.0264 (2.64%)\n",
                                                       "   PCK@0.1: 0.0883 (8.83%)\n",
                                                       "   PCK@0.15: 0.1772 (17.72%)\n",
                                                       "\n",
                                                       "ðŸ“Š Dataset Statistics:\n",
                                                       "   Samples evaluated: 12234\n",
                                                       "   Total keypoints: 88328\n",
                                                       "   Average confidence: 0.9743\n",
                                                       "\n",
                                                       "ðŸ“ Distance Statistics (pixels):\n",
                                                       "   mean: 0.3131\n",
                                                       "   std: 0.1611\n",
                                                       "   median: 0.2998\n",
                                                       "   min: 0.0008\n",
                                                       "   max: 0.8951\n",
                                                       "\n",
                                                       "ðŸ’¾ Results saved to: /content/drive/MyDrive/AMLProject/outputs/evaluation_results_full.json\n",
                                                       "======================================================================\n"
                                                   ]
                                      }
                                  ]
                  }
              ],
    "metadata":  {
                     "kernelspec":  {
                                        "display_name":  "Python 3",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.11.11"
                                       },
                     "colab":  {
                                   "provenance":  [

                                                  ],
                                   "gpuType":  "T4"
                               },
                     "accelerator":  "GPU"
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
