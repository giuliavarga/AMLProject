{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd13a90c",
      "metadata": {
        "id": "cd13a90c"
      },
      "source": [
        "# DINOv3 Dense Correspondence Pipeline\n",
        "\n",
        "This notebook implements a complete pipeline for evaluating DINOv3 on the **SPair-71k** dense correspondence task, using Google Colab. The pipeline includes:\n",
        "\n",
        "1. **Model Loading**: Automatic detection and loading of DINOv3 weights\n",
        "2. **Feature Extraction**: Dense feature extraction from image pairs using vision transformer backbones\n",
        "3. **Correspondence Matching**: Computing correspondences between image pairs through feature similarity\n",
        "4. **Evaluation**: Computing PCK (Percentage of Correct Keypoints) metrics on the SPair-71k validation/test sets\n",
        "5. **Optional Finetuning**: Light finetuning of the last transformer blocks on SPair-71k training data\n",
        "6. **Optional Soft-Argmax**: Sub-pixel coordinate refinement using window-based soft-argmax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ALL IMPORTS:\n",
        "from google.colab import drive\n",
        "import os, sys, shutil, importlib\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import json\n",
        "from torchvision import transforms\n",
        "import requests\n",
        "import tarfile\n",
        "from tqdm import tqdm as tqdm_requests\n",
        "import random\n",
        "import subprocess\n",
        "import timm\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n"
      ],
      "metadata": {
        "id": "_VJoIsz5Q97E"
      },
      "id": "_VJoIsz5Q97E",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define Project and Data Roots (must be consistent with previous setup)\n",
        "PROJECT_ROOT = '/content/AMLProject'\n",
        "DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'\n",
        "\n",
        "# Create necessary directories if they don't exist\n",
        "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints')\n",
        "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'dinov3') # Adjust for DINOv3 outputs\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
        "\n",
        "for directory in [CHECKPOINT_DIR, OUTPUT_DIR, MODEL_DIR, DATA_ROOT]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data root: {DATA_ROOT}\")\n",
        "print(f\"Output directory for DINOv3: {OUTPUT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl2Jdk_idnk5",
        "outputId": "925f1969-a142-4357-af56-d1455ddf6fea"
      },
      "id": "jl2Jdk_idnk5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Project root: /content/AMLProject\n",
            "Data root: /content/drive/MyDrive/AMLProject/data\n",
            "Output directory for DINOv3: /content/AMLProject/outputs/dinov3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuring project path ---\n",
        "PROJECT_ROOT = \"/content/drive/MyDrive/AMLProject\"\n",
        "repo_path = os.path.join(PROJECT_ROOT, \"SD4Match\")\n",
        "\n",
        "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
        "\n",
        "# --- Downlaoding SD4Match if missing ---\n",
        "if not os.path.exists(repo_path):\n",
        "    print(\"ðŸ“¦ Downloading SD4Match repository from ActiveVisionLab...\")\n",
        "\n",
        "    zip_url = \"https://github.com/ActiveVisionLab/SD4Match/archive/refs/heads/main.zip\"\n",
        "    zip_path = os.path.join(PROJECT_ROOT, \"SD4Match.zip\")\n",
        "\n",
        "    # download\n",
        "\n",
        "    r = requests.get(zip_url, stream=True)\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    # zip extraction\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(PROJECT_ROOT)\n",
        "\n",
        "    # move and rename\n",
        "    extracted = os.path.join(PROJECT_ROOT, \"SD4Match-main\")\n",
        "    if os.path.exists(repo_path):\n",
        "        shutil.rmtree(repo_path)\n",
        "    os.rename(extracted, repo_path)\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    print(\"âœ“ Repository SD4Match ready at\", repo_path)\n",
        "\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)\n",
        "\n",
        "try:\n",
        "    module = importlib.import_module(\"dataset.spair\")\n",
        "    SPairDataset = getattr(module, \"SPairDataset\")\n",
        "    print(\"âœ“ SPairDataset imported successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"âŒ Import Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc9O5wiady-Z",
        "outputId": "48dfe437-8933-4053-8f43-31a8e05b65e1"
      },
      "id": "lc9O5wiady-Z",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ SPairDataset imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c25544",
      "metadata": {
        "id": "50c25544"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "Window soft-argmax for sub-pixel refinement and finetuning utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b683a63c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b683a63c",
        "outputId": "5d39ebd1-b4dc-4565-9ea6-012a4b7f4099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Utility functions loaded\n"
          ]
        }
      ],
      "source": [
        "def window_soft_argmax(similarity, H, W, window=7, tau=0.05):\n",
        "    \"\"\"\n",
        "    Window soft-argmax for sub-pixel coordinate prediction.\n",
        "\n",
        "    Args:\n",
        "        similarity: [N, H*W] or [N, H, W] similarity scores\n",
        "        H, W: Grid dimensions\n",
        "        window: Window size around peak (odd number)\n",
        "        tau: Temperature for softmax (lower = sharper)\n",
        "\n",
        "    Returns:\n",
        "        [N, 2] tensor with (y, x) coordinates in patch space\n",
        "    \"\"\"\n",
        "    if similarity.dim() == 2:\n",
        "        N = similarity.size(0)\n",
        "        sim2d = similarity.view(N, H, W)\n",
        "    elif similarity.dim() == 3:\n",
        "        N = similarity.size(0)\n",
        "        sim2d = similarity\n",
        "    else:\n",
        "        raise ValueError(\"similarity must be [N,H*W] or [N,H,W]\")\n",
        "\n",
        "    r = window // 2\n",
        "    preds = []\n",
        "\n",
        "    for i in range(N):\n",
        "        s = sim2d[i]  # [H, W]\n",
        "\n",
        "        # Find peak with argmax\n",
        "        idx = torch.argmax(s)\n",
        "        y0 = (idx // W).item()\n",
        "        x0 = (idx % W).item()\n",
        "\n",
        "        # Extract window around peak\n",
        "        y1, y2 = max(y0 - r, 0), min(y0 + r + 1, H)\n",
        "        x1, x2 = max(x0 - r, 0), min(x0 + r + 1, W)\n",
        "\n",
        "        sub = s[y1:y2, x1:x2]\n",
        "\n",
        "        # Create coordinate grids\n",
        "        yy, xx = torch.meshgrid(\n",
        "            torch.arange(y1, y2, device=s.device, dtype=torch.float32),\n",
        "            torch.arange(x1, x2, device=s.device, dtype=torch.float32),\n",
        "            indexing='ij'\n",
        "        )\n",
        "\n",
        "        # Soft-argmax within window\n",
        "        wts = torch.softmax(sub.flatten() / tau, dim=0).view_as(sub)\n",
        "        y_hat = (wts * yy).sum()\n",
        "        x_hat = (wts * xx).sum()\n",
        "\n",
        "        preds.append(torch.stack([y_hat, x_hat]))\n",
        "\n",
        "    return torch.stack(preds, dim=0)  # [N, 2]\n",
        "\n",
        "\n",
        "def unfreeze_last_k_blocks(model, k, blocks_attr='blocks'):\n",
        "    \"\"\"\n",
        "    Unfreeze the last k transformer blocks of a model.\n",
        "\n",
        "    Args:\n",
        "        model: The backbone model\n",
        "        k: Number of last blocks to unfreeze\n",
        "        blocks_attr: Attribute name for blocks (default 'blocks')\n",
        "\n",
        "    Returns:\n",
        "        List of trainable parameters\n",
        "    \"\"\"\n",
        "    # Freeze all parameters\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Unfreeze last k blocks\n",
        "    blocks = getattr(model, blocks_attr)\n",
        "    for block in blocks[-k:]:\n",
        "        for p in block.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    # Return trainable parameters\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    print(f\"Unfroze last {k} blocks: {len(trainable_params)} trainable parameters\")\n",
        "\n",
        "    return trainable_params\n",
        "\n",
        "\n",
        "def compute_keypoint_loss(sim2d, H, W, gt_xy_px, patch_size, use_soft=True, window=7, tau=0.05):\n",
        "    \"\"\"\n",
        "    Compute loss from similarity map to ground truth keypoint.\n",
        "\n",
        "    Args:\n",
        "        sim2d: [H, W] similarity map\n",
        "        H, W: Grid dimensions\n",
        "        gt_xy_px: [2] ground truth coordinates in pixels (y, x)\n",
        "        patch_size: Patch size for coordinate conversion\n",
        "        use_soft: Use soft-argmax (True) or argmax (False)\n",
        "        window, tau: Soft-argmax parameters\n",
        "\n",
        "    Returns:\n",
        "        Scalar loss\n",
        "    \"\"\"\n",
        "    if use_soft:\n",
        "        pred_xy_patch = window_soft_argmax(sim2d[None], H, W, window, tau)[0]\n",
        "    else:\n",
        "        idx = sim2d.argmax()\n",
        "        pred_xy_patch = torch.stack([idx // W, idx % W]).float()\n",
        "\n",
        "    pred_xy_px = (pred_xy_patch + 0.5) * patch_size\n",
        "\n",
        "    return F.smooth_l1_loss(pred_xy_px, gt_xy_px)\n",
        "\n",
        "\n",
        "print(\"âœ“ Utility functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "495c312d",
      "metadata": {
        "id": "495c312d"
      },
      "source": [
        "## SPair-71k Dataloader:\n",
        "Complete dataloader for SPair-71k with keypoint annotations for finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "669244e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "669244e2",
        "outputId": "89e9443f-cd17-424b-a902-f6c99e84e718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ SPair-71k dataloader ready\n"
          ]
        }
      ],
      "source": [
        "class SPairDataset(Dataset):\n",
        "    \"\"\"SPair-71k dataset with keypoint annotations for correspondence learning.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, split='trn', category=None, image_size=224, subset=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.category = category\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.pairs = self._load_pairs()\n",
        "        if subset is not None:\n",
        "            self.pairs = self.pairs[:subset]\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        print(f\"SPair-71k {split} dataset: {len(self.pairs)} pairs loaded\")\n",
        "\n",
        "    def _load_pairs(self):\n",
        "        pairs = []\n",
        "        # SPair-71k: JSON files are directly in PairAnnotation/{split}/ directory\n",
        "        layout_dir = os.path.join(self.root_dir, 'PairAnnotation', self.split)\n",
        "\n",
        "        if not os.path.exists(layout_dir):\n",
        "            print(f\"Warning: PairAnnotation directory not found: {layout_dir}\")\n",
        "            return pairs\n",
        "\n",
        "        # Read all JSON files directly from the split directory\n",
        "        for fname in os.listdir(layout_dir):\n",
        "            if not fname.endswith('.json'):\n",
        "                continue\n",
        "\n",
        "            json_path = os.path.join(layout_dir, fname)\n",
        "            try:\n",
        "                with open(json_path, 'r') as f:\n",
        "                    pair_data = json.load(f)\n",
        "\n",
        "                cat = pair_data.get('category', 'unknown')\n",
        "\n",
        "                # Filter by category if specified\n",
        "                if self.category and cat != self.category:\n",
        "                    continue\n",
        "\n",
        "                pair = {\n",
        "                    'category': cat,\n",
        "                    'src_img': pair_data['src_imname'],\n",
        "                    'tgt_img': pair_data['trg_imname'],\n",
        "                    'src_kps': np.array(pair_data['src_kps']).reshape(-1, 2),\n",
        "                    'tgt_kps': np.array(pair_data['trg_kps']).reshape(-1, 2),\n",
        "                    'src_bbox': pair_data.get('src_bndbox', None),\n",
        "                    'tgt_bbox': pair_data.get('trg_bndbox', None),\n",
        "                }\n",
        "                pairs.append(pair)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "\n",
        "        src_img_path = os.path.join(self.root_dir, 'JPEGImages',\n",
        "                                    pair['category'], pair['src_img'])\n",
        "        tgt_img_path = os.path.join(self.root_dir, 'JPEGImages',\n",
        "                                    pair['category'], pair['tgt_img'])\n",
        "\n",
        "        src_img_pil = Image.open(src_img_path).convert('RGB')\n",
        "        tgt_img_pil = Image.open(tgt_img_path).convert('RGB')\n",
        "\n",
        "        src_w, src_h = src_img_pil.size\n",
        "        tgt_w, tgt_h = tgt_img_pil.size\n",
        "\n",
        "        src_kps = pair['src_kps'].copy().astype(float)\n",
        "        tgt_kps = pair['tgt_kps'].copy().astype(float)\n",
        "\n",
        "        src_kps[:, 0] *= self.image_size / src_w\n",
        "        src_kps[:, 1] *= self.image_size / src_h\n",
        "        tgt_kps[:, 0] *= self.image_size / tgt_w\n",
        "        tgt_kps[:, 1] *= self.image_size / tgt_h\n",
        "\n",
        "        src_img = self.transform(src_img_pil)\n",
        "        tgt_img = self.transform(tgt_img_pil)\n",
        "\n",
        "        if pair['src_bbox'] is not None:\n",
        "            src_bbox = np.array(pair['src_bbox'], dtype=float)\n",
        "            src_bbox[0::2] *= self.image_size / src_w\n",
        "            src_bbox[1::2] *= self.image_size / src_h\n",
        "            src_bbox_wh = np.array([src_bbox[2] - src_bbox[0], src_bbox[3] - src_bbox[1]])\n",
        "        else:\n",
        "            src_bbox_wh = np.array([self.image_size, self.image_size])\n",
        "\n",
        "        if pair['tgt_bbox'] is not None:\n",
        "            tgt_bbox = np.array(pair['tgt_bbox'], dtype=float)\n",
        "            tgt_bbox[0::2] *= self.image_size / tgt_w\n",
        "            tgt_bbox[1::2] *= self.image_size / tgt_h\n",
        "            tgt_bbox_wh = np.array([tgt_bbox[2] - tgt_bbox[0], tgt_bbox[3] - tgt_bbox[1]])\n",
        "        else:\n",
        "            tgt_bbox_wh = np.array([self.image_size, self.image_size])\n",
        "\n",
        "        return {\n",
        "            'src_img': src_img,\n",
        "            'tgt_img': tgt_img,\n",
        "            'src_kps': torch.from_numpy(src_kps).float(),\n",
        "            'tgt_kps': torch.from_numpy(tgt_kps).float(),\n",
        "            'src_bbox_wh': torch.from_numpy(src_bbox_wh).float(),\n",
        "            'tgt_bbox_wh': torch.from_numpy(tgt_bbox_wh).float(),\n",
        "            'category': pair['category'],\n",
        "            'pair_id': idx\n",
        "        }\n",
        "\n",
        "\n",
        "def create_spair_dataloaders(root_dir, batch_size=1, num_workers=2,\n",
        "                             train_subset=None, val_subset=None):\n",
        "    train_dataset = SPairDataset(root_dir, split='trn', subset=train_subset)\n",
        "    val_dataset = SPairDataset(root_dir, split='val', subset=val_subset)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                             num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                           num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "print(\"âœ“ SPair-71k dataloader ready\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download SPair-71k dataset (if not already present)\n",
        "\n",
        "data_path = os.path.join(DATA_ROOT, 'SPair-71k')\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print(\"Downloading SPair-71k dataset...\")\n",
        "    url = \"http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
        "    tar_path = os.path.join(DATA_ROOT, 'SPair-71k.tar.gz')\n",
        "\n",
        "    # Download with progress bar\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "    with open(tar_path, 'wb') as f, tqdm_requests(\n",
        "        desc='Downloading',\n",
        "        total=total_size,\n",
        "        unit='B',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as pbar:\n",
        "        for data in response.iter_content(chunk_size=1024):\n",
        "            size = f.write(data)\n",
        "            pbar.update(size)\n",
        "\n",
        "    print(\"\\nExtracting...\")\n",
        "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "        tar.extractall(DATA_ROOT)\n",
        "\n",
        "    # Cleanup\n",
        "    os.remove(tar_path)\n",
        "    print(\"âœ“ Extraction complete\")\n",
        "else:\n",
        "    print(f\"âœ“ SPair-71k dataset already exists at {data_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrVK5uXoGWhS",
        "outputId": "6581b9e1-ed0e-4045-9aef-21d1afc49f3c"
      },
      "id": "ZrVK5uXoGWhS",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ SPair-71k dataset already exists at /content/drive/MyDrive/AMLProject/data/SPair-71k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e9fd00",
      "metadata": {
        "id": "37e9fd00"
      },
      "source": [
        "## Configuration Flags\n",
        "\n",
        "Set these flags to control the pipeline behavior:\n",
        "- `ENABLE_FINETUNING`: Enable light finetuning of last transformer blocks\n",
        "- `USE_SOFT_ARGMAX`: Use window soft-argmax instead of argmax for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "33a55962",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33a55962",
        "outputId": "70bbaa61-0e67-4a46-be71-8a7499dfdacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  ENABLE_FINETUNING = False\n",
            "  USE_SOFT_ARGMAX = False\n"
          ]
        }
      ],
      "source": [
        "# ========== CONFIGURATION FLAGS ==========\n",
        "# Set these flags to control behavior\n",
        "ENABLE_FINETUNING = False  # Set True to enable light finetuning of last layers\n",
        "USE_SOFT_ARGMAX = False    # Set True to use window soft-argmax instead of argmax\n",
        "\n",
        "# Finetuning hyperparameters (only used if ENABLE_FINETUNING=True)\n",
        "FINETUNE_K_LAYERS = 2      # Number of last transformer blocks to unfreeze {1, 2, 4}\n",
        "FINETUNE_LR = 1e-5         # Learning rate\n",
        "FINETUNE_WD = 1e-4         # Weight decay\n",
        "FINETUNE_EPOCHS = 3        # Number of training epochs\n",
        "FINETUNE_BATCH_SIZE = 1    # Batch size for training\n",
        "FINETUNE_TRAIN_SUBSET = None  # None for full training set, or int for subset\n",
        "\n",
        "# Soft-argmax hyperparameters (only used if USE_SOFT_ARGMAX=True)\n",
        "SOFT_WINDOW = 7            # Window size around peak (odd number: 5, 7, 9)\n",
        "SOFT_TAU = 0.05            # Softmax temperature (lower = sharper)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  ENABLE_FINETUNING = {ENABLE_FINETUNING}\")\n",
        "print(f\"  USE_SOFT_ARGMAX = {USE_SOFT_ARGMAX}\")\n",
        "if ENABLE_FINETUNING:\n",
        "    print(f\"  Finetuning: k={FINETUNE_K_LAYERS}, lr={FINETUNE_LR}, epochs={FINETUNE_EPOCHS}\")\n",
        "if USE_SOFT_ARGMAX:\n",
        "    print(f\"  Soft-argmax: window={SOFT_WINDOW}, tau={SOFT_TAU}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bb67222d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb67222d",
        "outputId": "ce574649-e75c-438b-a077-b079b7c62fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment: colab\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "PROJECT_ROOT: /content/drive/MyDrive/AMLProject\n",
            "DATA_ROOT: /content/drive/MyDrive/AMLProject/data\n",
            "MODEL_ROOT: /content/drive/MyDrive/AMLProject/models\n",
            "OUTPUT_ROOT: /content/drive/MyDrive/AMLProject/outputs\n",
            "Seed set to 42\n"
          ]
        }
      ],
      "source": [
        "# Extract and Normalize Common Utility Functions\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"Seed set to {seed}\")\n",
        "\n",
        "def detect_env():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return 'colab'\n",
        "    except Exception:\n",
        "        return 'local'\n",
        "\n",
        "ENV = detect_env()\n",
        "print('Environment:', ENV)\n",
        "\n",
        "def get_paths():\n",
        "    if ENV == 'colab':\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        root = '/content/drive/MyDrive/AMLProject'\n",
        "        data_root = os.path.join(root, 'data')\n",
        "        model_root = os.path.join(root, 'models')\n",
        "        out_root = os.path.join(root, 'outputs')\n",
        "    else:\n",
        "        root = os.getcwd()\n",
        "        data_root = os.path.join(root, 'data')\n",
        "        model_root = os.path.join(root, 'models')\n",
        "        out_root = os.path.join(root, 'outputs')\n",
        "    os.makedirs(data_root, exist_ok=True)\n",
        "    os.makedirs(model_root, exist_ok=True)\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "    return root, data_root, model_root, out_root\n",
        "\n",
        "PROJECT_ROOT, DATA_ROOT, MODEL_ROOT, OUTPUT_ROOT = get_paths()\n",
        "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
        "print('DATA_ROOT:', DATA_ROOT)\n",
        "print('MODEL_ROOT:', MODEL_ROOT)\n",
        "print('OUTPUT_ROOT:', OUTPUT_ROOT)\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fa80e1b",
        "outputId": "3d0a85a2-413b-4237-8816-2e4ffa2f38b8"
      },
      "source": [
        "# Clone DINOv3 repository if it doesn't exist\n",
        "\n",
        "# Assuming PROJECT_ROOT is defined from previous cells, e.g., /content/drive/MyDrive/AMLProject\n",
        "dinov3_repo_path = os.path.join(PROJECT_ROOT, 'dinov3') # Or os.path.join(MODEL_ROOT, 'dinov3')\n",
        "\n",
        "if not os.path.exists(dinov3_repo_path):\n",
        "    print(f\"ðŸ“¦ Cloning DINOv3 repository to {dinov3_repo_path}...\")\n",
        "    # Make sure the parent directory exists\n",
        "    os.makedirs(dinov3_repo_path, exist_ok=True)\n",
        "    # Use a subprocess to clone, as !git clone might not work reliably with paths containing spaces\n",
        "    try:\n",
        "        subprocess.run(['git', 'clone', 'https://github.com/facebookresearch/dinov3.git', dinov3_repo_path], check=True)\n",
        "        print(\"âœ“ Repository DINOv3 cloned successfully!\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"âŒ Error cloning DINOv3 repository: {e}\")\n",
        "        print(\"Please check your internet connection or the repository URL.\")\n",
        "else:\n",
        "    print(f\"âœ“ Repository DINOv3 already exists at {dinov3_repo_path}\")\n",
        "\n",
        "# Add the cloned repo to sys.path if not already there\n",
        "if dinov3_repo_path not in sys.path:\n",
        "    sys.path.insert(0, dinov3_repo_path)\n",
        "    print(f\"Added {dinov3_repo_path} to sys.path\")\n"
      ],
      "id": "8fa80e1b",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Repository DINOv3 already exists at /content/drive/MyDrive/AMLProject/dinov3\n",
            "Added /content/drive/MyDrive/AMLProject/dinov3 to sys.path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0b001c33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b001c33",
        "outputId": "1e1234bc-5fb9-41e0-e22d-bda5810f562b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "DINOv3 repo: /content/drive/MyDrive/AMLProject/dinov3\n",
            "DINOv3 weight: /content/drive/MyDrive/AMLProject/models/dinov3_vitb16.pth\n",
            "Loaded DINOv3 weights. Missing=0 Unexpected=37\n",
            "âœ“ Model type: DINOv3 ViT-B/16 (official)\n",
            "Feature extractor ready. Mock mode: False image_size: 224 patch: 16\n"
          ]
        }
      ],
      "source": [
        "# Model: Auto-detect weights, fallback, and mock outputs\n",
        "\n",
        "# Fallback paths if previous cell not run\n",
        "if 'PROJECT_ROOT' not in globals() or 'MODEL_ROOT' not in globals():\n",
        "    PROJECT_ROOT = os.getcwd()\n",
        "    MODEL_ROOT = os.path.join(PROJECT_ROOT, 'models')\n",
        "    OUTPUT_ROOT = os.path.join(PROJECT_ROOT, 'outputs')\n",
        "    os.makedirs(MODEL_ROOT, exist_ok=True)\n",
        "    os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
        "    print('Initialized default paths (previous cell not run).')\n",
        "\n",
        "# Device detection (CUDA, MPS, CPU)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "# Auto-detect official DINOv3 repo and weights\n",
        "def find_dinov3_repo_and_weights():\n",
        "    candidates_repo = [\n",
        "        Path(MODEL_ROOT) / 'dinov3',\n",
        "        Path(PROJECT_ROOT) / 'models' / 'dinov3',\n",
        "        Path(PROJECT_ROOT) / 'dinov3'\n",
        "    ]\n",
        "    repo_path = next((str(p) for p in candidates_repo if p.exists()), None)\n",
        "    if repo_path and repo_path not in sys.path:\n",
        "        sys.path.append(repo_path)\n",
        "\n",
        "    candidates_weights = [\n",
        "        Path(MODEL_ROOT) / 'dinov3_vitb16.pth',\n",
        "        Path(MODEL_ROOT) / 'dinov3' / 'dinov3_vitb16.pth',\n",
        "        Path(PROJECT_ROOT) / 'models' / 'dinov3_vitb16.pth',\n",
        "        Path(PROJECT_ROOT) / 'models' / 'dinov3' / 'dinov3_vitb16.pth',\n",
        "        Path(PROJECT_ROOT) / 'checkpoints' / 'dinov3' / 'dinov3_vitb16.pth',\n",
        "        Path(PROJECT_ROOT) / 'weights' / 'dinov3_vitb16.pth'\n",
        "    ]\n",
        "    weight_path = next((str(p) for p in candidates_weights if p.exists()), None)\n",
        "    return repo_path, weight_path\n",
        "\n",
        "# Re-initialize dinov3_model and model_type before searching to clear previous state\n",
        "dinov3_model = None\n",
        "model_type = None\n",
        "\n",
        "repo_path, weight_path = find_dinov3_repo_and_weights()\n",
        "print('DINOv3 repo:', repo_path)\n",
        "print('DINOv3 weight:', weight_path)\n",
        "\n",
        "# Try official DINOv3 if both repo and weights are present\n",
        "try:\n",
        "    if repo_path:\n",
        "        # Ensure the dinov3 library can be imported\n",
        "        try:\n",
        "            from dinov3.models.vision_transformer import vit_base\n",
        "        except ImportError:\n",
        "            print(f\"Warning: DINOv3 repository found at {repo_path} but module 'dinov3.models.vision_transformer' could not be imported. Please ensure the repository is correctly set up.\")\n",
        "            vit_base = None\n",
        "\n",
        "        if vit_base:\n",
        "            dinov3_model = vit_base(patch_size=16)\n",
        "            if weight_path:\n",
        "                ckpt = torch.load(weight_path, map_location='cpu')\n",
        "                missing, unexpected = dinov3_model.load_state_dict(ckpt, strict=False)\n",
        "                print(f'Loaded DINOv3 weights. Missing={len(missing)} Unexpected={len(unexpected)}')\n",
        "                model_type = 'DINOv3 ViT-B/16 (official)'\n",
        "            else:\n",
        "                print('Weights not found; using mock outputs with uninitialized model.')\n",
        "                model_type = 'DINOv3 ViT-B/16 (no weights, mock)'\n",
        "            dinov3_model.to(device).eval()\n",
        "    if not dinov3_model:\n",
        "        raise Exception('Official DINOv3 model or weights not found/loadable.')\n",
        "except Exception as e:\n",
        "    print('âŒ ERROR: Official DINOv3 model could not be loaded.')\n",
        "    print(f'   Details: {e}')\n",
        "    print('\\nðŸ“‹ Required setup:')\n",
        "    print('   1. Clone DINOv3 repository to models/dinov3/')\n",
        "    print('   2. Download DINOv3 weights (dinov3_vitb16.pth) to models/ or checkpoints/')\n",
        "    print('\\nExiting - DINOv3 is required for this notebook.')\n",
        "    raise RuntimeError('DINOv3 model not available. Please follow the setup instructions above.')\n",
        "\n",
        "print('âœ“ Model type:', model_type)\n",
        "\n",
        "def _model_image_size(model):\n",
        "    cfg = getattr(model, 'default_cfg', {})\n",
        "    size = cfg.get('input_size', (3, 224, 224))\n",
        "    if isinstance(size, (list, tuple)) and len(size) == 3:\n",
        "        return size[1]\n",
        "    return 224\n",
        "\n",
        "def _model_patch_size(model):\n",
        "    ps = getattr(getattr(model, 'patch_embed', None), 'patch_size', (14, 14))\n",
        "    if isinstance(ps, (list, tuple)) and len(ps) >= 1:\n",
        "        return ps[0]\n",
        "    return 14\n",
        "\n",
        "# Unified feature extractor with optional mock when weights missing\n",
        "class UnifiedFeatureExtractor:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.image_size = _model_image_size(model)\n",
        "        self.patch_size = _model_patch_size(model)\n",
        "        self.feat_dim = getattr(getattr(model, 'num_features', None), 'real', None) or getattr(model, 'num_features', 768)\n",
        "        import torchvision.transforms as transforms\n",
        "        from PIL import Image\n",
        "        self.Image = Image\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
        "            transforms.CenterCrop(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        # Corrected logic for use_mock: activate mock if the model itself is None\n",
        "        self.use_mock = (self.model is None) or (model_type is not None and 'mock' in model_type)\n",
        "\n",
        "    def preprocess(self, img):\n",
        "        if isinstance(img, torch.Tensor): # Check if input is already a Tensor\n",
        "            # Assume it's already preprocessed by the dataset (normalized, resized, ToTensor)\n",
        "            # Just ensure it has a batch dimension and is on the correct device\n",
        "            if img.dim() == 3: # (C, H, W)\n",
        "                x = img.unsqueeze(0).to(self.device)\n",
        "            elif img.dim() == 4: # (B, C, H, W)\n",
        "                x = img.to(self.device)\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected tensor dimension for image input: {img.dim()}\")\n",
        "            return x # Return directly if it's already a Tensor\n",
        "        elif isinstance(img, self.Image.Image):\n",
        "            pil = img\n",
        "        else: # Assume numpy array if not PIL or Tensor\n",
        "            pil = self.Image.fromarray(img)\n",
        "        return self.transforms(pil).unsqueeze(0).to(self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract(self, img, normalize=True):\n",
        "        x = self.preprocess(img)\n",
        "        if self.use_mock:\n",
        "            grid = (self.image_size // self.patch_size)\n",
        "            torch.manual_seed(42)\n",
        "            feats = torch.randn(1, grid, grid, self.feat_dim)\n",
        "        else:\n",
        "            out = self.model.forward_features(x)\n",
        "            if isinstance(out, dict) and 'x_norm_patchtokens' in out:\n",
        "                tokens = out['x_norm_patchtokens']\n",
        "            elif isinstance(out, dict) and 'x_norm_clstoken' in out and 'x_norm_patchtokens' in out:\n",
        "                tokens = out['x_norm_patchtokens']\n",
        "            else:\n",
        "                tokens = out[:, 1:, :] if out.dim() == 3 else out\n",
        "            grid = int(tokens.shape[1] ** 0.5)\n",
        "            feats = tokens.view(1, grid, grid, self.feat_dim)\n",
        "        if normalize:\n",
        "            feats = F.normalize(feats, p=2, dim=-1)\n",
        "        info = {\n",
        "            'feature_size': (feats.shape[2], feats.shape[1]),\n",
        "            'processed_size': (self.image_size, self.image_size),\n",
        "            'patch_size': self.patch_size,\n",
        "            'feat_dim': self.feat_dim\n",
        "        }\n",
        "        return feats.cpu(), info\n",
        "\n",
        "feature_extractor = UnifiedFeatureExtractor(dinov3_model, device)\n",
        "print('Feature extractor ready. Mock mode:', feature_extractor.use_mock, 'image_size:', feature_extractor.image_size, 'patch:', feature_extractor.patch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7270c573",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7270c573",
        "outputId": "aa23f44a-1c7f-4d13-9048-c5dc2ce01ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finetuning disabled. Using pretrained weights only.\n"
          ]
        }
      ],
      "source": [
        "if ENABLE_FINETUNING:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"LIGHT FINETUNING ENABLED\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Check if we have real model (not mock)\n",
        "    if dinov3_model is None or feature_extractor.use_mock:\n",
        "        print(\"\\nâš ï¸  Cannot finetune: model is in mock mode or not loaded\")\n",
        "        print(\"   Please ensure DINOv3 weights are available\")\n",
        "    else:\n",
        "        # Path to SPair-71k\n",
        "        SPAIR_ROOT = os.path.join(DATA_ROOT if 'DATA_ROOT' in globals() else os.path.join(PROJECT_ROOT, 'data'), 'SPair-71k')\n",
        "\n",
        "        if not os.path.exists(SPAIR_ROOT):\n",
        "            print(f\"\\nâš ï¸  SPair-71k not found at: {SPAIR_ROOT}\")\n",
        "            print(\"   Download from: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
        "        else:\n",
        "            print(f\"\\nLoading SPair-71k from: {SPAIR_ROOT}\")\n",
        "            train_loader, val_loader = create_spair_dataloaders(\n",
        "                SPAIR_ROOT,\n",
        "                batch_size=FINETUNE_BATCH_SIZE,\n",
        "                num_workers=2,\n",
        "                train_subset=FINETUNE_TRAIN_SUBSET,\n",
        "                val_subset=500\n",
        "            )\n",
        "\n",
        "            # Unfreeze last k blocks\n",
        "            trainable_params = unfreeze_last_k_blocks(dinov3_model, FINETUNE_K_LAYERS, blocks_attr='blocks')\n",
        "\n",
        "            # Setup optimizer\n",
        "            optimizer = torch.optim.AdamW(trainable_params, lr=FINETUNE_LR, weight_decay=FINETUNE_WD)\n",
        "\n",
        "            print(f\"\\nFinetuning configuration:\")\n",
        "            print(f\"  Model: {model_type}\")\n",
        "            print(f\"  k={FINETUNE_K_LAYERS} layers\")\n",
        "            print(f\"  lr={FINETUNE_LR}, wd={FINETUNE_WD}\")\n",
        "            print(f\"  epochs={FINETUNE_EPOCHS}\")\n",
        "            print(f\"  train samples: {len(train_loader.dataset)}\")\n",
        "            print(f\"  val samples: {len(val_loader.dataset)}\")\n",
        "\n",
        "            patch_size = feature_extractor.patch_size  # 14 or 16\n",
        "\n",
        "            # Training\n",
        "            dinov3_model.train()\n",
        "            best_val_loss = float('inf')\n",
        "\n",
        "            for epoch in range(FINETUNE_EPOCHS):\n",
        "                epoch_loss = 0.0\n",
        "                num_batches = 0\n",
        "\n",
        "                for batch_idx, batch in enumerate(train_loader):\n",
        "                    src_img = batch['src_img'].to(device)\n",
        "                    tgt_img = batch['tgt_img'].to(device)\n",
        "                    src_kps = batch['src_kps'].to(device)\n",
        "                    tgt_kps = batch['tgt_kps'].to(device)\n",
        "\n",
        "                    # Extract features\n",
        "                    src_out = dinov3_model.get_intermediate_layers(src_img, n=1, return_class_token=False)[0]\n",
        "                    tgt_out = dinov3_model.get_intermediate_layers(tgt_img, n=1, return_class_token=False)[0]\n",
        "\n",
        "                    B = src_img.size(0)\n",
        "                    num_patches = src_out.size(1)\n",
        "                    grid_size = int(np.sqrt(num_patches))\n",
        "\n",
        "                    batch_loss = 0.0\n",
        "                    num_kps = 0\n",
        "\n",
        "                    for b in range(B):\n",
        "                        src_f = F.normalize(src_out[b], dim=-1)\n",
        "                        tgt_f = F.normalize(tgt_out[b], dim=-1)\n",
        "\n",
        "                        for kp_idx in range(src_kps[b].size(0)):\n",
        "                            src_x = int(src_kps[b][kp_idx, 0].item() / patch_size)\n",
        "                            src_y = int(src_kps[b][kp_idx, 1].item() / patch_size)\n",
        "                            src_x = max(0, min(grid_size - 1, src_x))\n",
        "                            src_y = max(0, min(grid_size - 1, src_y))\n",
        "\n",
        "                            src_patch_idx = src_y * grid_size + src_x\n",
        "                            sim = torch.matmul(tgt_f, src_f[src_patch_idx])\n",
        "                            sim_2d = sim.view(grid_size, grid_size)\n",
        "\n",
        "                            gt_xy = tgt_kps[b][kp_idx]\n",
        "                            gt_yx = torch.stack([gt_xy[1], gt_xy[0]])\n",
        "\n",
        "                            loss = compute_keypoint_loss(\n",
        "                                sim_2d, grid_size, grid_size, gt_yx, patch_size,\n",
        "                                use_soft=True, window=SOFT_WINDOW, tau=SOFT_TAU\n",
        "                            )\n",
        "                            batch_loss += loss\n",
        "                            num_kps += 1\n",
        "\n",
        "                    if num_kps > 0:\n",
        "                        batch_loss = batch_loss / num_kps\n",
        "                        optimizer.zero_grad()\n",
        "                        batch_loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        epoch_loss += batch_loss.item()\n",
        "                        num_batches += 1\n",
        "\n",
        "                    if (batch_idx + 1) % 50 == 0:\n",
        "                        print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {epoch_loss/max(1,num_batches):.4f}\")\n",
        "\n",
        "                avg_train_loss = epoch_loss / max(1, num_batches)\n",
        "\n",
        "                # Validation\n",
        "                dinov3_model.eval()\n",
        "                val_loss = 0.0\n",
        "                val_batches = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_loader:\n",
        "                        src_img = batch['src_img'].to(device)\n",
        "                        tgt_img = batch['tgt_img'].to(device)\n",
        "                        src_kps = batch['src_kps'].to(device)\n",
        "                        tgt_kps = batch['tgt_kps'].to(device)\n",
        "\n",
        "                        src_out = dinov3_model.get_intermediate_layers(src_img, n=1, return_class_token=False)[0]\n",
        "                        tgt_out = dinov3_model.get_intermediate_layers(tgt_img, n=1, return_class_token=False)[0]\n",
        "\n",
        "                        num_patches = src_out.size(1)\n",
        "                        grid_size = int(np.sqrt(num_patches))\n",
        "\n",
        "                        batch_val_loss = 0.0\n",
        "                        num_kps = 0\n",
        "\n",
        "                        for b in range(src_img.size(0)):\n",
        "                            src_f = F.normalize(src_out[b], dim=-1)\n",
        "                            tgt_f = F.normalize(tgt_out[b], dim=-1)\n",
        "\n",
        "                            for kp_idx in range(src_kps[b].size(0)):\n",
        "                                src_x = int(src_kps[b][kp_idx, 0].item() / patch_size)\n",
        "                                src_y = int(src_kps[b][kp_idx, 1].item() / patch_size)\n",
        "                                src_x = max(0, min(grid_size - 1, src_x))\n",
        "                                src_y = max(0, min(grid_size - 1, src_y))\n",
        "\n",
        "                                src_patch_idx = src_y * grid_size + src_x\n",
        "                                sim = torch.matmul(tgt_f, src_f[src_patch_idx])\n",
        "                                sim_2d = sim.view(grid_size, grid_size)\n",
        "\n",
        "                                gt_xy = tgt_kps[b][kp_idx]\n",
        "                                gt_yx = torch.stack([gt_xy[1], gt_xy[0]])\n",
        "\n",
        "                                loss = compute_keypoint_loss(\n",
        "                                    sim_2d, grid_size, grid_size, gt_yx, patch_size,\n",
        "                                    use_soft=True, window=SOFT_WINDOW, tau=SOFT_TAU\n",
        "                                )\n",
        "                                batch_val_loss += loss\n",
        "                                num_kps += 1\n",
        "\n",
        "                        if num_kps > 0:\n",
        "                            val_loss += (batch_val_loss / num_kps).item()\n",
        "                            val_batches += 1\n",
        "\n",
        "                avg_val_loss = val_loss / max(1, val_batches)\n",
        "\n",
        "                print(f\"\\nEpoch {epoch+1}/{FINETUNE_EPOCHS}:\")\n",
        "                print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                # Save best\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    ckpt_path = os.path.join(OUTPUT_ROOT, f'dinov3_finetuned_k{FINETUNE_K_LAYERS}_best.pth')\n",
        "                    torch.save({\n",
        "                        'model_state_dict': dinov3_model.state_dict(),\n",
        "                        'config': {'k': FINETUNE_K_LAYERS, 'lr': FINETUNE_LR}\n",
        "                    }, ckpt_path)\n",
        "                    print(f\"  âœ“ Best model saved\")\n",
        "\n",
        "                dinov3_model.train()\n",
        "\n",
        "            dinov3_model.eval()\n",
        "            print(f\"\\nâœ… Finetuning completed! Best val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Finetuning disabled. Using pretrained weights only.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "998c62c6",
      "metadata": {
        "id": "998c62c6"
      },
      "source": [
        "## Light Finetuning (Optional)\n",
        "\n",
        "If `ENABLE_FINETUNING=True`, this section finetunes the last k transformer blocks on SPair-71k with keypoint supervision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "168a5a39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "168a5a39",
        "outputId": "d0fd84de-b1c3-4e7e-88ed-14b44099e4a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matcher (soft-argmax=disabled), PCK, and visualization ready.\n"
          ]
        }
      ],
      "source": [
        "# 4) Correspondence matching, PCK, and visualization (works with mock)\n",
        "\n",
        "class CorrespondenceMatcher:\n",
        "    def __init__(self, mutual_nn=False, use_soft_argmax=False, soft_window=7, soft_tau=0.05):\n",
        "        self.mutual_nn = mutual_nn\n",
        "        self.use_soft_argmax = use_soft_argmax\n",
        "        self.soft_window = soft_window\n",
        "        self.soft_tau = soft_tau\n",
        "\n",
        "    def match(self, src_feats, tgt_feats):\n",
        "        H, W, D = tgt_feats.shape[1], tgt_feats.shape[2], tgt_feats.shape[3]\n",
        "        src_flat = src_feats.view(-1, D)\n",
        "        tgt_flat = tgt_feats.view(-1, D)\n",
        "\n",
        "        if self.use_soft_argmax:\n",
        "            # Compute similarity on device\n",
        "            sim = torch.matmul(src_flat, tgt_flat.T)  # [N, H*W]\n",
        "\n",
        "            # Use window soft-argmax\n",
        "            pred_coords = window_soft_argmax(\n",
        "                sim, H, W,\n",
        "                window=self.soft_window,\n",
        "                tau=self.soft_tau\n",
        "            )  # [N, 2] in (y, x) patch coordinates\n",
        "\n",
        "            x = pred_coords[:, 1].cpu().numpy()\n",
        "            y = pred_coords[:, 0].cpu().numpy()\n",
        "        else:\n",
        "            # Original argmax approach\n",
        "            sim = src_flat.numpy() @ tgt_flat.numpy().T\n",
        "            best = np.argmax(sim, axis=1)\n",
        "            y = best // W\n",
        "            x = best % W\n",
        "\n",
        "        return np.stack([x, y], axis=1)\n",
        "\n",
        "def pck(pred_kps, gt_kps, alpha=0.1, img_wh=(224,224)):\n",
        "    if len(pred_kps)==0 or len(gt_kps)==0:\n",
        "        return 0.0\n",
        "    d = np.linalg.norm(pred_kps - gt_kps, axis=1)\n",
        "    norm = np.sqrt(img_wh[0]**2 + img_wh[1]**2)\n",
        "    thr = alpha*norm\n",
        "    return float((d<=thr).mean())\n",
        "\n",
        "def visualize(src_img, tgt_img, src_kps, pred_kps):\n",
        "    fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
        "    ax[0].imshow(src_img)\n",
        "    ax[0].scatter(src_kps[:,0], src_kps[:,1], c='r', s=40)\n",
        "    ax[0].set_title('Source')\n",
        "    ax[0].axis('off')\n",
        "    ax[1].imshow(tgt_img)\n",
        "    ax[1].scatter(pred_kps[:,0], pred_kps[:,1], c='b', s=40)\n",
        "    ax[1].set_title('Target (Pred)')\n",
        "    ax[1].axis('off')\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "print(f'Matcher (soft-argmax={'enabled' if USE_SOFT_ARGMAX else 'disabled'}), PCK, and visualization ready.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "19838d23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "19838d23",
        "outputId": "5f1a332d-d789-4013-ccc9-f5ee2abfb652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCK@0.1: 0.2500 (mock data)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ End-to-end sanity check completed.\n"
          ]
        }
      ],
      "source": [
        "# 5) End-to-end sanity check with mock data\n",
        "\n",
        "# Create synthetic images and keypoints\n",
        "src = Image.new('RGB', (224,224), color=(200,200,220))\n",
        "tgt = Image.new('RGB', (224,224), color=(210,190,200))\n",
        "src_kps = np.array([[30,30],[60,120],[150,80],[200,200]], dtype=float)\n",
        "gt_kps  = np.array([[40,40],[70,130],[160,90],[210,210]], dtype=float)  # fake GT\n",
        "\n",
        "# Extract features (mock if weights missing)\n",
        "src_feats, _ = feature_extractor.extract(src)  # (1,H,W,D)\n",
        "tgt_feats, _ = feature_extractor.extract(tgt)\n",
        "src_feats = src_feats[0]\n",
        "tgt_feats = tgt_feats[0]\n",
        "H, W, D = src_feats.shape\n",
        "\n",
        "# Map keypoints to feature grid\n",
        "patch = feature_extractor.patch_size\n",
        "def kps_to_feat(kps):\n",
        "    k = kps.copy()\n",
        "    k[:,0] = np.clip(np.round(k[:,0] / patch), 0, W-1)\n",
        "    k[:,1] = np.clip(np.round(k[:,1] / patch), 0, H-1)\n",
        "    return k.astype(int)\n",
        "\n",
        "src_feat_idx = kps_to_feat(src_kps)\n",
        "tgt_feat_idx_gt = kps_to_feat(gt_kps)\n",
        "\n",
        "# Gather source descriptors at keypoints\n",
        "src_desc = src_feats[src_feat_idx[:,1], src_feat_idx[:,0], :]  # (K,D)\n",
        "\n",
        "# Match only for keypoints with configuration\n",
        "matcher = CorrespondenceMatcher(\n",
        "    mutual_nn=False,\n",
        "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
        "    soft_window=SOFT_WINDOW if USE_SOFT_ARGMAX else 7,\n",
        "    soft_tau=SOFT_TAU if USE_SOFT_ARGMAX else 0.05\n",
        ")\n",
        "pred_feat_coords = matcher.match(src_desc[None, None, ...], tgt_feats[None, ...])\n",
        "pred_feat_coords = pred_feat_coords[:len(src_kps)]  # (K,2)\n",
        "\n",
        "# Map feature coords back to image pixels (center of patch)\n",
        "pred_kps = np.stack([pred_feat_coords[:,0]*patch + patch//2,\n",
        "                      pred_feat_coords[:,1]*patch + patch//2], axis=1).astype(float)\n",
        "\n",
        "# Compute PCK\n",
        "alpha_val = 0.1\n",
        "pck_val = pck(pred_kps, gt_kps, alpha=alpha_val, img_wh=(224,224))\n",
        "print(f'PCK@{alpha_val}: {pck_val:.4f} (mock data)')\n",
        "\n",
        "# Visualize\n",
        "fig = visualize(np.array(src), np.array(tgt), src_kps, pred_kps)\n",
        "plt.show()\n",
        "print('âœ“ End-to-end sanity check completed.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to denormalize images for visualization\n",
        "def denorm_show(img_tensor):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    img = img_tensor * std + mean\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "    return img.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "\n",
        "class PCKEvaluator:\n",
        "    \"\"\"Evaluator for PCK metrics at different alpha thresholds.\"\"\"\n",
        "\n",
        "    def __init__(self, alphas=[0.05, 0.10, 0.15]):\n",
        "        self.alphas = alphas\n",
        "\n",
        "    def compute_pck(self, pred_kps, gt_kps, alpha, img_size=224):\n",
        "        \"\"\"Compute PCK for given alpha threshold.\"\"\"\n",
        "        distances = np.linalg.norm(pred_kps - gt_kps, axis=1)\n",
        "        # Compute normalization factor (same as DINOv2)\n",
        "        norm_factor = np.sqrt(img_size**2 + img_size**2)\n",
        "\n",
        "        # Normalize distances (same as DINOv2)\n",
        "        normalized_distances = distances / (norm_factor + 1e-8)\n",
        "\n",
        "        # Compare with alpha threshold\n",
        "        threshold = alpha\n",
        "        return (normalized_distances <= threshold).astype(float), normalized_distances\n",
        "\n",
        "    def evaluate_dataset(self, all_predictions, all_ground_truths):\n",
        "        \"\"\"Evaluate PCK across entire dataset.\"\"\"\n",
        "        all_correct = {alpha: [] for alpha in self.alphas}\n",
        "        all_distances = []\n",
        "\n",
        "        for pred_kps, gt_kps in zip(all_predictions, all_ground_truths):\n",
        "            for alpha in self.alphas:\n",
        "                correct, distances = self.compute_pck(pred_kps, gt_kps, alpha)\n",
        "                all_correct[alpha].extend(correct)\n",
        "                if alpha == self.alphas[0]:  # Solo una volta\n",
        "                    all_distances.extend(distances)\n",
        "\n",
        "        # Compute average PCK for each alpha\n",
        "        avg_pck = {f'PCK@{alpha}': np.mean(all_correct[alpha])\n",
        "                   for alpha in self.alphas}\n",
        "\n",
        "        per_sample_pck = {f'PCK@{alpha}': all_correct[alpha]\n",
        "                          for alpha in self.alphas}\n",
        "\n",
        "        return avg_pck, per_sample_pck, np.array(all_distances)\n",
        "\n",
        "\n",
        "def evaluate_on_dataset(dataset, feature_extractor, matcher, evaluator,\n",
        "                       max_samples=None, save_visualizations=False):\n",
        "    \"\"\"\n",
        "    Evaluate correspondence on entire dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: SPairDataset instance\n",
        "        feature_extractor: UnifiedFeatureExtractor instance\n",
        "        matcher: CorrespondenceMatcher instance\n",
        "        evaluator: PCKEvaluator instance\n",
        "        max_samples: Maximum samples to evaluate (None = all)\n",
        "        save_visualizations: Whether to save sample visualizations\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    print(f\"Evaluating on {len(dataset)} samples...\")\n",
        "\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "    all_confidences = []\n",
        "\n",
        "    num_samples = min(max_samples, len(dataset)) if max_samples else len(dataset)\n",
        "\n",
        "    for idx in tqdm(range(num_samples), desc=\"Evaluating\"):\n",
        "        sample = dataset[idx]\n",
        "\n",
        "        src_img = sample['src_img']\n",
        "        tgt_img = sample['tgt_img']\n",
        "        src_kps = sample['src_kps']\n",
        "        tgt_kps = sample['tgt_kps']\n",
        "\n",
        "        # Get valid keypoints\n",
        "        valid_mask = (src_kps[:, 0] >= 0) & (src_kps[:, 1] >= 0)\n",
        "        valid_src_kps = src_kps[valid_mask]\n",
        "        valid_tgt_kps = tgt_kps[valid_mask]\n",
        "\n",
        "        if len(valid_src_kps) == 0:\n",
        "            continue\n",
        "\n",
        "        # Extract features usando il feature extractor\n",
        "        src_feats, _ = feature_extractor.extract(src_img)\n",
        "        tgt_feats, _ = feature_extractor.extract(tgt_img)\n",
        "\n",
        "        # Remove batch dimension\n",
        "        src_feats = src_feats[0]  # (H, W, D)\n",
        "        tgt_feats = tgt_feats[0]  # (H, W, D)\n",
        "\n",
        "        H, W, D = tgt_feats.shape\n",
        "        patch_size = feature_extractor.patch_size\n",
        "\n",
        "        # Map keypoints to feature grid\n",
        "        def kps_to_feat(kps):\n",
        "            k = kps.cpu().numpy() if isinstance(kps, torch.Tensor) else kps.copy()\n",
        "            k[:, 0] = np.clip(np.round(k[:, 0] / patch_size), 0, W - 1)\n",
        "            k[:, 1] = np.clip(np.round(k[:, 1] / patch_size), 0, H - 1)\n",
        "            return k.astype(int)\n",
        "\n",
        "        src_feat_idx = kps_to_feat(valid_src_kps)\n",
        "\n",
        "        # Gather source descriptors at keypoints\n",
        "        src_desc = src_feats[src_feat_idx[:, 1], src_feat_idx[:, 0], :]  # (K, D)\n",
        "\n",
        "        # Match\n",
        "        pred_feat_coords = matcher.match(src_desc[None, None, ...], tgt_feats[None, ...])\n",
        "        pred_feat_coords = pred_feat_coords[:len(valid_src_kps)]  # (K, 2)\n",
        "\n",
        "        # Map feature coords back to image pixels (center of patch)\n",
        "        pred_kps = np.stack([\n",
        "            pred_feat_coords[:, 0] * patch_size + patch_size // 2,\n",
        "            pred_feat_coords[:, 1] * patch_size + patch_size // 2\n",
        "        ], axis=1).astype(float)\n",
        "\n",
        "        # Compute confidence scores based on similarity\n",
        "        src_flat = src_desc.view(-1, D).cpu()\n",
        "        tgt_flat = tgt_feats.view(-1, D).cpu()\n",
        "        sim = torch.matmul(src_flat, tgt_flat.T)\n",
        "        confidences = sim.max(dim=1)[0].numpy()  # Max similarity per ogni keypoint\n",
        "\n",
        "        all_predictions.append(pred_kps)\n",
        "\n",
        "        # Convert valid_tgt_kps to numpy array before appending\n",
        "        if isinstance(valid_tgt_kps, torch.Tensor):\n",
        "            all_ground_truths.append(valid_tgt_kps.cpu().numpy())\n",
        "        else:\n",
        "            all_ground_truths.append(valid_tgt_kps)\n",
        "\n",
        "        all_confidences.append(confidences)\n",
        "\n",
        "        # Save visualization for first few samples\n",
        "        if save_visualizations and idx < 5:\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
        "\n",
        "            axes[0].imshow(denorm_show(src_img))\n",
        "            valid_src_kps_np = valid_src_kps.cpu().numpy() if isinstance(valid_src_kps, torch.Tensor) else valid_src_kps\n",
        "            axes[0].scatter(valid_src_kps_np[:, 0], valid_src_kps_np[:, 1],\n",
        "                           c='red', s=100, marker='x', linewidths=3)\n",
        "            axes[0].set_title(f'Source (Sample {idx})', fontsize=12, fontweight='bold')\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            axes[1].imshow(denorm_show(tgt_img))\n",
        "            valid_tgt_kps_np = valid_tgt_kps.cpu().numpy() if isinstance(valid_tgt_kps, torch.Tensor) else valid_tgt_kps\n",
        "            axes[1].scatter(valid_tgt_kps_np[:, 0], valid_tgt_kps_np[:, 1],\n",
        "                           c='lime', s=100, marker='o', alpha=0.6, linewidths=2,\n",
        "                           edgecolors='darkgreen', label='GT')\n",
        "            axes[1].scatter(pred_kps[:, 0], pred_kps[:, 1],\n",
        "                           c='red', s=80, marker='x', linewidths=2, label='Pred')\n",
        "            axes[1].set_title(f'Target (Sample {idx})', fontsize=12, fontweight='bold')\n",
        "            axes[1].axis('off')\n",
        "            axes[1].legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(OUTPUT_ROOT, f'match_sample_{idx}.png'),\n",
        "                       dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\nComputing PCK metrics...\")\n",
        "    avg_pck, per_sample_pck, all_distances = evaluator.evaluate_dataset(\n",
        "        all_predictions, all_ground_truths\n",
        "    )\n",
        "\n",
        "    # Compute additional statistics\n",
        "    all_confidences_flat = np.concatenate(all_confidences)\n",
        "\n",
        "    results = {\n",
        "        'avg_pck': avg_pck,\n",
        "        'num_samples': num_samples,\n",
        "        'num_keypoints': len(all_distances),\n",
        "        'avg_confidence': float(all_confidences_flat.mean()),\n",
        "        'distance_stats': {\n",
        "            'mean': float(all_distances.mean()),\n",
        "            'std': float(all_distances.std()),\n",
        "            'median': float(np.median(all_distances)),\n",
        "            'min': float(all_distances.min()),\n",
        "            'max': float(all_distances.max())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "1nhLv__DfUI6"
      },
      "id": "1nhLv__DfUI6",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION ON 50 SAMPLES"
      ],
      "metadata": {
        "id": "KJgnWBYgQjXO"
      },
      "id": "KJgnWBYgQjXO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create evaluator\n",
        "evaluator = PCKEvaluator(alphas=[0.05, 0.10, 0.15])\n",
        "\n",
        "# Create dataset\n",
        "print(\"Wait, loading pairs (takes some minutes)...\")\n",
        "SPAIR_ROOT = os.path.join(DATA_ROOT, 'SPair-71k')\n",
        "dataset = SPairDataset(SPAIR_ROOT, split='test', subset=50)\n",
        "\n",
        "# Create matcher\n",
        "matcher = CorrespondenceMatcher(\n",
        "    mutual_nn=False,\n",
        "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
        "    soft_window=SOFT_WINDOW,\n",
        "    soft_tau=SOFT_TAU\n",
        ")\n",
        "\n",
        "# Run evaluation on small subset\n",
        "print(\"Running evaluation on 50 samples...\")\n",
        "results = evaluate_on_dataset(\n",
        "    dataset=dataset,\n",
        "    feature_extractor=feature_extractor,\n",
        "    matcher=matcher,\n",
        "    evaluator=evaluator,\n",
        "    max_samples=50,\n",
        "    save_visualizations=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION RESULTS (50 samples)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nPCK Metrics:\")\n",
        "for key, value in results['avg_pck'].items():\n",
        "    print(f\"  {key}: {value:.4f} ({value*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"  Samples evaluated: {results['num_samples']}\")\n",
        "print(f\"  Total keypoints: {results['num_keypoints']}\")\n",
        "print(f\"  Average confidence: {results['avg_confidence']:.4f}\")\n",
        "\n",
        "print(f\"\\nDistance Statistics:\")\n",
        "for key, value in results['distance_stats'].items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results_path = os.path.join(OUTPUT_ROOT, 'evaluation_results_subset.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "print(f\"\\nâœ“ Results saved to {results_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553,
          "referenced_widgets": [
            "71e69cbaf210466c85805734c40ad0eb",
            "d17577abd2004e7e9ae045cd5b0fe0f9",
            "278b579937e240b090ca72d37978fc3b",
            "8b4715a06dec4395ba07e1ca61a8f113",
            "2dc08ac13b484ddeb9656dc92ed3eff7",
            "77f57549dbe947fbab1d2d3d76ceb0dc",
            "14c6eb88e29b4fdf8fe1c66caec2fda4",
            "5624311922e24b218703db80736853cb",
            "22497899ca6b43fbadb140431b336e78",
            "42eab1100e2b4f82bd0157756c192171",
            "24877ce20fc24cd1b6d28729c97cf0c0"
          ]
        },
        "id": "PKpOs3jUQeBs",
        "outputId": "e4802ceb-ae8e-4217-957c-35e12b050f67"
      },
      "id": "PKpOs3jUQeBs",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wait, loading pairs...\n",
            "SPair-71k test dataset: 50 pairs loaded\n",
            "Running evaluation on 50 samples...\n",
            "Evaluating on 50 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71e69cbaf210466c85805734c40ad0eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing PCK metrics...\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS (50 samples)\n",
            "============================================================\n",
            "\n",
            "PCK Metrics:\n",
            "  PCK@0.05: 0.0272 (2.72%)\n",
            "  PCK@0.1: 0.0845 (8.45%)\n",
            "  PCK@0.15: 0.1553 (15.53%)\n",
            "\n",
            "Dataset Statistics:\n",
            "  Samples evaluated: 50\n",
            "  Total keypoints: 367\n",
            "  Average confidence: 0.9756\n",
            "\n",
            "Distance Statistics:\n",
            "  mean: 0.3264\n",
            "  std: 0.1612\n",
            "  median: 0.3292\n",
            "  min: 0.0061\n",
            "  max: 0.8162\n",
            "\n",
            "âœ“ Results saved to /content/drive/MyDrive/AMLProject/outputs/evaluation_results_subset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EVALUATION ON 600 SAMPLES"
      ],
      "metadata": {
        "id": "piucxOkbQpSV"
      },
      "id": "piucxOkbQpSV"
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ EVALUATION: Evaluation on 600 pairs\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ”¥ RUNNING EVALUATION ON 600 VALIDATION PAIRS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create evaluator\n",
        "evaluator = PCKEvaluator(alphas=[0.05, 0.10, 0.15])\n",
        "\n",
        "# Create dataset with 600 pairs\n",
        "print(\"Wait, loading pairs (takes some minutes)...\")\n",
        "SPAIR_ROOT = os.path.join(DATA_ROOT, 'SPair-71k')\n",
        "eval_dataset = SPairDataset(SPAIR_ROOT, split='test', subset=600)\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset size: {len(eval_dataset)} pairs\")\n",
        "print(f\"â±ï¸  Estimated time: ~{len(eval_dataset)*0.5/60:.1f} minutes\\n\")\n",
        "\n",
        "# Create matcher\n",
        "matcher = CorrespondenceMatcher(\n",
        "    mutual_nn=False,\n",
        "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
        "    soft_window=SOFT_WINDOW,\n",
        "    soft_tau=SOFT_TAU\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "print(\"Starting evaluation...\")\n",
        "results = evaluate_on_dataset(\n",
        "    dataset=eval_dataset,\n",
        "    feature_extractor=feature_extractor,\n",
        "    matcher=matcher,\n",
        "    evaluator=evaluator,\n",
        "    max_samples=None,  # All samples from subset\n",
        "    save_visualizations=True  # Save first 5 visualizations\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“ˆ EVALUATION RESULTS (600 pairs)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nðŸŽ¯ PCK Metrics:\")\n",
        "for key, value in results['avg_pck'].items():\n",
        "    print(f\"   {key}: {value:.4f} ({value*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
        "print(f\"   Samples evaluated: {results['num_samples']}\")\n",
        "print(f\"   Total keypoints: {results['num_keypoints']}\")\n",
        "print(f\"   Average confidence: {results['avg_confidence']:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ“ Distance Statistics (pixels):\")\n",
        "for key, value in results['distance_stats'].items():\n",
        "    print(f\"   {key}: {value:.2f}\")\n",
        "\n",
        "# Save results\n",
        "results_path = os.path.join(OUTPUT_ROOT, 'evaluation_results_600.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\nðŸ’¾ Results saved to: {results_path}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674,
          "referenced_widgets": [
            "aef8721d7fad408ab1561d69fbc27fa8",
            "8328f9ec63564207be81036a54b50a77",
            "1a7ae4e09cfd49e2bcfd74f6799899d0",
            "2aa5d654f17549cab12a6ee0562c76ce",
            "5bc9727588c945068d3dd6a24869bc04",
            "c341dba73149445b81436c9bded0d01c",
            "761ca6dfad47407da109a35e0ed10f86",
            "a899ff9d22f94574ae62f95f3823b7f8",
            "9716b48642104f61813741cf188fa113",
            "7820e5708a52434d8a661fa6a0bb184c",
            "4ebda1d4d29f454aafda63e90a6809c5"
          ]
        },
        "id": "lZRYxR9k-SNN",
        "outputId": "3922d235-8a4e-4bf3-e9a0-94eddd8de4af"
      },
      "id": "lZRYxR9k-SNN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸ”¥ RUNNING EVALUATION ON 600 VALIDATION PAIRS\n",
            "======================================================================\n",
            "SPair-71k val dataset: 600 pairs loaded\n",
            "\n",
            "ðŸ“Š Dataset size: 600 pairs\n",
            "â±ï¸  Estimated time: ~5.0 minutes\n",
            "\n",
            "Starting evaluation...\n",
            "Evaluating on 600 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/600 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aef8721d7fad408ab1561d69fbc27fa8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing PCK metrics...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“ˆ EVALUATION RESULTS (600 pairs)\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ PCK Metrics:\n",
            "   PCK@0.05: 0.0311 (3.11%)\n",
            "   PCK@0.1: 0.1045 (10.45%)\n",
            "   PCK@0.15: 0.1925 (19.25%)\n",
            "\n",
            "ðŸ“Š Dataset Statistics:\n",
            "   Samples evaluated: 600\n",
            "   Total keypoints: 5438\n",
            "   Average confidence: 0.9761\n",
            "\n",
            "ðŸ“ Distance Statistics (pixels):\n",
            "   mean: 0.31\n",
            "   std: 0.16\n",
            "   median: 0.30\n",
            "   min: 0.00\n",
            "   max: 0.81\n",
            "\n",
            "ðŸ’¾ Results saved to: /content/drive/MyDrive/AMLProject/outputs/evaluation_results_600.json\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EVALUATION ON WHOLE DATASET"
      ],
      "metadata": {
        "id": "-YoxAVA7QuqM"
      },
      "id": "-YoxAVA7QuqM"
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ FULL EVALUATION: Evaluation on whole dataset\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ”¥ RUNNING FULL EVALUATION ON ENTIRE VALIDATION SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create evaluator\n",
        "evaluator = PCKEvaluator(alphas=[0.05, 0.10, 0.15])\n",
        "\n",
        "# Create complete dataset  (no subset)\n",
        "print(\"Wait, loading pairs (takes some minutes)...\")\n",
        "SPAIR_ROOT = os.path.join(DATA_ROOT, 'SPair-71k')\n",
        "full_dataset = SPairDataset(SPAIR_ROOT, split='test', subset=None)\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset size: {len(full_dataset)} pairs\")\n",
        "print(f\"â±ï¸  Estimated time: ~{len(full_dataset)*0.5/60:.1f} minutes\\n\")\n",
        "\n",
        "# Create matcher\n",
        "matcher = CorrespondenceMatcher(\n",
        "    mutual_nn=False,\n",
        "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
        "    soft_window=SOFT_WINDOW,\n",
        "    soft_tau=SOFT_TAU\n",
        ")\n",
        "\n",
        "# Run evaluation on full dataset\n",
        "print(\"Starting full evaluation...\")\n",
        "full_results = evaluate_on_dataset(\n",
        "    dataset=full_dataset,\n",
        "    feature_extractor=feature_extractor,\n",
        "    matcher=matcher,\n",
        "    evaluator=evaluator,\n",
        "    max_samples=None,  # All samples\n",
        "    save_visualizations=True  # Save first 5 visualizations\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“ˆ FULL EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nðŸŽ¯ PCK Metrics:\")\n",
        "for key, value in full_results['avg_pck'].items():\n",
        "    print(f\"   {key}: {value:.4f} ({value*100:.2f}%)\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
        "print(f\"   Samples evaluated: {full_results['num_samples']}\")\n",
        "print(f\"   Total keypoints: {full_results['num_keypoints']}\")\n",
        "print(f\"   Average confidence: {full_results['avg_confidence']:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ“ Distance Statistics (pixels):\")\n",
        "for key, value in full_results['distance_stats'].items():\n",
        "    print(f\"   {key}: {value:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results_path = os.path.join(OUTPUT_ROOT, 'evaluation_results_full.json')\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(full_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nðŸ’¾ Results saved to: {results_path}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674,
          "referenced_widgets": [
            "a9bc61c1e53f4777a1e91067656dbf6a",
            "e5dde25129274ee0b0b35d2a8a162d80",
            "f4f3a1b0331c4d3abeb453ff801518a3",
            "148299fb47644aaf85e5eef05efd2a97",
            "bce67cb260214522a87c8cc91051a519",
            "b9818a546b0c4d789e92d3403d7637d5",
            "329302b8a6e0420ba7e6e36770383d6b",
            "4baf7580f3364f70b972815ad7462116",
            "30c0e7735169427394406c098e5ab72d",
            "091e86533988474d8bed47d7ea7e761d",
            "afaf02d16222486a888ccde397fd355f"
          ]
        },
        "id": "IOBlOQFR7CQr",
        "outputId": "295660e9-7a4d-46b4-faca-322217963fb8"
      },
      "id": "IOBlOQFR7CQr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸ”¥ RUNNING FULL EVALUATION ON ENTIRE VALIDATION SET\n",
            "======================================================================\n",
            "SPair-71k test dataset: 12234 pairs loaded\n",
            "\n",
            "ðŸ“Š Dataset size: 12234 pairs\n",
            "â±ï¸  Estimated time: ~102.0 minutes\n",
            "\n",
            "Starting full evaluation...\n",
            "Evaluating on 12234 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/12234 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9bc61c1e53f4777a1e91067656dbf6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing PCK metrics...\n",
            "\n",
            "======================================================================\n",
            "ðŸ“ˆ FULL EVALUATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "ðŸŽ¯ PCK Metrics:\n",
            "   PCK@0.05: 0.0264 (2.64%)\n",
            "   PCK@0.1: 0.0883 (8.83%)\n",
            "   PCK@0.15: 0.1772 (17.72%)\n",
            "\n",
            "ðŸ“Š Dataset Statistics:\n",
            "   Samples evaluated: 12234\n",
            "   Total keypoints: 88328\n",
            "   Average confidence: 0.9743\n",
            "\n",
            "ðŸ“ Distance Statistics (pixels):\n",
            "   mean: 0.3131\n",
            "   std: 0.1611\n",
            "   median: 0.2998\n",
            "   min: 0.0008\n",
            "   max: 0.8951\n",
            "\n",
            "ðŸ’¾ Results saved to: /content/drive/MyDrive/AMLProject/outputs/evaluation_results_full.json\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aef8721d7fad408ab1561d69fbc27fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8328f9ec63564207be81036a54b50a77",
              "IPY_MODEL_1a7ae4e09cfd49e2bcfd74f6799899d0",
              "IPY_MODEL_2aa5d654f17549cab12a6ee0562c76ce"
            ],
            "layout": "IPY_MODEL_5bc9727588c945068d3dd6a24869bc04"
          }
        },
        "8328f9ec63564207be81036a54b50a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c341dba73149445b81436c9bded0d01c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_761ca6dfad47407da109a35e0ed10f86",
            "value": "Evaluating:â€‡100%"
          }
        },
        "1a7ae4e09cfd49e2bcfd74f6799899d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a899ff9d22f94574ae62f95f3823b7f8",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9716b48642104f61813741cf188fa113",
            "value": 600
          }
        },
        "2aa5d654f17549cab12a6ee0562c76ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7820e5708a52434d8a661fa6a0bb184c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4ebda1d4d29f454aafda63e90a6809c5",
            "value": "â€‡600/600â€‡[18:10&lt;00:00,â€‡â€‡1.64s/it]"
          }
        },
        "5bc9727588c945068d3dd6a24869bc04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c341dba73149445b81436c9bded0d01c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "761ca6dfad47407da109a35e0ed10f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a899ff9d22f94574ae62f95f3823b7f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9716b48642104f61813741cf188fa113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7820e5708a52434d8a661fa6a0bb184c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ebda1d4d29f454aafda63e90a6809c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9bc61c1e53f4777a1e91067656dbf6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5dde25129274ee0b0b35d2a8a162d80",
              "IPY_MODEL_f4f3a1b0331c4d3abeb453ff801518a3",
              "IPY_MODEL_148299fb47644aaf85e5eef05efd2a97"
            ],
            "layout": "IPY_MODEL_bce67cb260214522a87c8cc91051a519"
          }
        },
        "e5dde25129274ee0b0b35d2a8a162d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9818a546b0c4d789e92d3403d7637d5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_329302b8a6e0420ba7e6e36770383d6b",
            "value": "Evaluating:â€‡100%"
          }
        },
        "f4f3a1b0331c4d3abeb453ff801518a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4baf7580f3364f70b972815ad7462116",
            "max": 12234,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30c0e7735169427394406c098e5ab72d",
            "value": 12234
          }
        },
        "148299fb47644aaf85e5eef05efd2a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_091e86533988474d8bed47d7ea7e761d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_afaf02d16222486a888ccde397fd355f",
            "value": "â€‡12234/12234â€‡[15:00&lt;00:00,â€‡16.78it/s]"
          }
        },
        "bce67cb260214522a87c8cc91051a519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9818a546b0c4d789e92d3403d7637d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "329302b8a6e0420ba7e6e36770383d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4baf7580f3364f70b972815ad7462116": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c0e7735169427394406c098e5ab72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "091e86533988474d8bed47d7ea7e761d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afaf02d16222486a888ccde397fd355f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71e69cbaf210466c85805734c40ad0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d17577abd2004e7e9ae045cd5b0fe0f9",
              "IPY_MODEL_278b579937e240b090ca72d37978fc3b",
              "IPY_MODEL_8b4715a06dec4395ba07e1ca61a8f113"
            ],
            "layout": "IPY_MODEL_2dc08ac13b484ddeb9656dc92ed3eff7"
          }
        },
        "d17577abd2004e7e9ae045cd5b0fe0f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77f57549dbe947fbab1d2d3d76ceb0dc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_14c6eb88e29b4fdf8fe1c66caec2fda4",
            "value": "Evaluating:â€‡100%"
          }
        },
        "278b579937e240b090ca72d37978fc3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5624311922e24b218703db80736853cb",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22497899ca6b43fbadb140431b336e78",
            "value": 50
          }
        },
        "8b4715a06dec4395ba07e1ca61a8f113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42eab1100e2b4f82bd0157756c192171",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_24877ce20fc24cd1b6d28729c97cf0c0",
            "value": "â€‡50/50â€‡[00:06&lt;00:00,â€‡16.63it/s]"
          }
        },
        "2dc08ac13b484ddeb9656dc92ed3eff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f57549dbe947fbab1d2d3d76ceb0dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c6eb88e29b4fdf8fe1c66caec2fda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5624311922e24b218703db80736853cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22497899ca6b43fbadb140431b336e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42eab1100e2b4f82bd0157756c192171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24877ce20fc24cd1b6d28729c97cf0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}