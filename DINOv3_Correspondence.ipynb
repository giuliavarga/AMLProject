{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd13a90c",
   "metadata": {},
   "source": [
    "# Merged AML_Project_Dinov3 and DINOv3_Correspondence\n",
    "This notebook programmatically merges your colleague's `AML_Project_Dinov3.ipynb` with your `DINOv3_Correspondence.ipynb`, unifying environment setup, model loading, dataset handling, correspondence matching, evaluation, and tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e05afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded notebooks:\n",
      " - AML_Project_Dinov3.ipynb cells = 9\n",
      " - DINOv3_Correspondence.ipynb cells = 23\n",
      "\n",
      "--- Structure A ---\n",
      "#1 code [python] :: # Mount Google Drive so the SPair-71k dataset can persist be\n",
      "#2 code [python] :: import os import shutil  # Force delete the broken repositor\n",
      "#3 code [python] :: import os import shutil  # Re-define the project path (just \n",
      "#4 code [python] :: import sys import os import importlib import matplotlib.pypl\n",
      "#5 code [python] :: import sys import os import importlib import matplotlib.pypl\n",
      "#6 code [python] :: # === clone_dinov3_repo === %cd /content/drive/MyDrive/AML_P\n",
      "#7 code [python] :: # === download_dinov3_weights === # Carica i pesi DINOv3 dal\n",
      "#8 code [python] :: # === setup_dinov3_model === import torch import sys import \n",
      "#9 code [python] :: # === integrate_dinov3_with_spair === import torch import to\n",
      "\n",
      "--- Structure B ---\n",
      "#1 markdown [markdown] :: ## Section 1: Environment Setup\n",
      "#2 code [python] :: # Detect environment and configure paths import sys import o\n",
      "#3 code [python] :: # Install dependencies import subprocess  print(\"Installing \n",
      "#4 code [python] :: # Import libraries import torch import torch.nn as nn import\n",
      "#5 markdown [markdown] :: ## Section 2: Load DINOv3 Model  ### DINOv3 Setup Options  *\n",
      "#6 code [python] :: # Clone DINOv3 repository (if using official implementation)\n",
      "#7 code [python] :: # Load DINOv3 model with fallback strategies print(\"Loading \n",
      "#8 markdown [markdown] :: ## Section 3: Dense Feature Extraction  DINOv3 feature extra\n",
      "#9 code [python] :: class DINOv3FeatureExtractor:     \"\"\"     Extract dense spat\n",
      "#10 code [python] :: # Test feature extraction print(\"Testing feature extraction.\n",
      "#11 markdown [markdown] :: ## Section 3 (continued): Correspondence Matching\n",
      "#12 code [python] :: class CorrespondenceMatcher:     \"\"\"Match keypoints using de\n",
      "#13 markdown [markdown] :: ## Section 3 (continued): Evaluation Metrics\n",
      "#14 code [python] :: class PCKEvaluator:     \"\"\"PCK (Percentage of Correct Keypoi\n",
      "#15 markdown [markdown] :: ## Dataset Loaders\n",
      "#16 code [python] :: # Dataset setup def setup_datasets(data_root):     \"\"\"Setup \n",
      "#17 code [python] :: # SPair-71k dataset loader from torch.utils.data import Data\n",
      "#18 markdown [markdown] :: ## Visualization and Evaluation Pipeline\n",
      "#19 code [python] :: def visualize_correspondences(src_img, tgt_img, src_kps, pre\n",
      "#20 code [python] :: def evaluate_on_dataset(dataset, feature_extractor, matcher,\n",
      "#21 markdown [markdown] :: ## Run Evaluation  Uncomment to run evaluation on your datas\n",
      "#22 code [python] :: # Load and evaluate # spair_test = SPairDataset( #     root_\n",
      "#23 markdown [markdown] :: ## Summary  ### DINOv3 Implementation Complete ✓  **Implemen\n",
      "\n",
      "Example diff of first code cells (if present):\n",
      "--- \n",
      "+++ \n",
      "@@ -1,15 +1,36 @@\n",
      "-# Mount Google Drive so the SPair-71k dataset can persist between sessions\n",
      "-from google.colab import drive\n",
      "+# Detect environment and configure paths\n",
      "+import sys\n",
      " import os\n",
      "-drive.mount('/content/drive')\n",
      "+from pathlib import Path\n",
      " \n",
      "-# Create a project folder to keep things organized\n",
      "-project_path = '/content/drive/MyDrive/AML_Project'\n",
      "+# Detect Google Colab\n",
      "+try:\n",
      "+    import google.colab\n",
      "+    IN_COLAB = True\n",
      "+    print(\"✓ Running on Google Colab\")\n",
      "+except:\n",
      "+    IN_COLAB = False\n",
      "+    print(\"✓ Running locally\")\n",
      " \n",
      "-if not os.path.exists(project_path):\n",
      "-    os.makedirs(project_path)\n",
      "-    print(f\"Created project directory at {project_path}\")\n",
      "+# Set up paths\n",
      "+if IN_COLAB:\n",
      "+    from google.colab import drive\n",
      "+    drive.mount('/content/drive')\n",
      "+    PROJECT_ROOT = '/content/AMLProject'\n",
      "+    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'\n",
      " else:\n",
      "-    print(f\"Project directory already exists at {project_path}\")\n",
      "+    PROJECT_ROOT = os.getcwd()\n",
      "+    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
      " \n",
      "-%cd {project_path}\n",
      "+# Create necessary directories\n",
      "+CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints', 'dinov3')\n",
      "+OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'dinov3')\n"
     ]
    }
   ],
   "source": [
    "# 1) Load and Inspect Notebooks\n",
    "import os, json, difflib, nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(os.getcwd())\n",
    "A = ROOT / 'AML_Project_Dinov3.ipynb'\n",
    "B = ROOT / 'DINOv3_Correspondence.ipynb'\n",
    "\n",
    "def load_nb(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return nbformat.read(f, as_version=4)\n",
    "\n",
    "nbA = load_nb(A)\n",
    "nbB = load_nb(B)\n",
    "print('Loaded notebooks:')\n",
    "print(' -', A.name, 'cells =', len(nbA['cells']))\n",
    "print(' -', B.name, 'cells =', len(nbB['cells']))\n",
    "\n",
    "def summarize(nb):\n",
    "    lines = []\n",
    "    for i, c in enumerate(nb['cells']):\n",
    "        t = c['cell_type']\n",
    "        lang = c.get('metadata', {}).get('language', 'python' if t=='code' else 'markdown')\n",
    "        head = ''.join(c.get('source',''))[:60].replace('\\n',' ')\n",
    "        lines.append(f\"#{i+1} {t} [{lang}] :: {head}\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print('\\n--- Structure A ---')\n",
    "print(summarize(nbA))\n",
    "print('\\n--- Structure B ---')\n",
    "print(summarize(nbB))\n",
    "\n",
    "def diff_cells(srcA, srcB):\n",
    "    return difflib.unified_diff(srcA.splitlines(), srcB.splitlines(), lineterm='')\n",
    "\n",
    "print('\\nExample diff of first code cells (if present):')\n",
    "idxA = next((i for i,c in enumerate(nbA['cells']) if c['cell_type']=='code'), None)\n",
    "idxB = next((i for i,c in enumerate(nbB['cells']) if c['cell_type']=='code'), None)\n",
    "if idxA is not None and idxB is not None:\n",
    "    srcA = ''.join(nbA['cells'][idxA]['source'])\n",
    "    srcB = ''.join(nbB['cells'][idxB]['source'])\n",
    "    for line in list(diff_cells(srcA, srcB))[:40]:\n",
    "        print(line)\n",
    "else:\n",
    "    print('No code cells to diff.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def window_soft_argmax(similarity, H, W, window=7, tau=0.05):\n",
    "    \"\"\"\n",
    "    Window soft-argmax for sub-pixel coordinate prediction.\n",
    "    \n",
    "    Args:\n",
    "        similarity: [N, H*W] or [N, H, W] similarity scores\n",
    "        H, W: Grid dimensions\n",
    "        window: Window size around peak (odd number)\n",
    "        tau: Temperature for softmax (lower = sharper)\n",
    "    \n",
    "    Returns:\n",
    "        [N, 2] tensor with (y, x) coordinates in patch space\n",
    "    \"\"\"\n",
    "    if similarity.dim() == 2:\n",
    "        N = similarity.size(0)\n",
    "        sim2d = similarity.view(N, H, W)\n",
    "    elif similarity.dim() == 3:\n",
    "        N = similarity.size(0)\n",
    "        sim2d = similarity\n",
    "    else:\n",
    "        raise ValueError(\"similarity must be [N,H*W] or [N,H,W]\")\n",
    "    \n",
    "    r = window // 2\n",
    "    preds = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        s = sim2d[i]  # [H, W]\n",
    "        \n",
    "        # Find peak with argmax\n",
    "        idx = torch.argmax(s)\n",
    "        y0 = (idx // W).item()\n",
    "        x0 = (idx % W).item()\n",
    "        \n",
    "        # Extract window around peak\n",
    "        y1, y2 = max(y0 - r, 0), min(y0 + r + 1, H)\n",
    "        x1, x2 = max(x0 - r, 0), min(x0 + r + 1, W)\n",
    "        \n",
    "        sub = s[y1:y2, x1:x2]\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        yy, xx = torch.meshgrid(\n",
    "            torch.arange(y1, y2, device=s.device, dtype=torch.float32),\n",
    "            torch.arange(x1, x2, device=s.device, dtype=torch.float32),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        \n",
    "        # Soft-argmax within window\n",
    "        wts = torch.softmax(sub.flatten() / tau, dim=0).view_as(sub)\n",
    "        y_hat = (wts * yy).sum()\n",
    "        x_hat = (wts * xx).sum()\n",
    "        \n",
    "        preds.append(torch.stack([y_hat, x_hat]))\n",
    "    \n",
    "    return torch.stack(preds, dim=0)  # [N, 2]\n",
    "\n",
    "\n",
    "def unfreeze_last_k_blocks(model, k, blocks_attr='blocks'):\n",
    "    \"\"\"\n",
    "    Unfreeze the last k transformer blocks of a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The backbone model\n",
    "        k: Number of last blocks to unfreeze\n",
    "        blocks_attr: Attribute name for blocks (default 'blocks')\n",
    "    \n",
    "    Returns:\n",
    "        List of trainable parameters\n",
    "    \"\"\"\n",
    "    # Freeze all parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Unfreeze last k blocks\n",
    "    blocks = getattr(model, blocks_attr)\n",
    "    for block in blocks[-k:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "    \n",
    "    # Return trainable parameters\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f\"Unfroze last {k} blocks: {len(trainable_params)} trainable parameters\")\n",
    "    \n",
    "    return trainable_params\n",
    "\n",
    "\n",
    "def compute_keypoint_loss(sim2d, H, W, gt_xy_px, patch_size, use_soft=True, window=7, tau=0.05):\n",
    "    \"\"\"\n",
    "    Compute loss from similarity map to ground truth keypoint.\n",
    "    \n",
    "    Args:\n",
    "        sim2d: [H, W] similarity map\n",
    "        H, W: Grid dimensions\n",
    "        gt_xy_px: [2] ground truth coordinates in pixels (y, x)\n",
    "        patch_size: Patch size for coordinate conversion\n",
    "        use_soft: Use soft-argmax (True) or argmax (False)\n",
    "        window, tau: Soft-argmax parameters\n",
    "    \n",
    "    Returns:\n",
    "        Scalar loss\n",
    "    \"\"\"\n",
    "    if use_soft:\n",
    "        pred_xy_patch = window_soft_argmax(sim2d[None], H, W, window, tau)[0]\n",
    "    else:\n",
    "        idx = sim2d.argmax()\n",
    "        pred_xy_patch = torch.stack([idx // W, idx % W]).float()\n",
    "    \n",
    "    pred_xy_px = (pred_xy_patch + 0.5) * patch_size\n",
    "    \n",
    "    return F.smooth_l1_loss(pred_xy_px, gt_xy_px)\n",
    "\n",
    "\n",
    "print(\"✓ Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669244e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "class SPairDataset(Dataset):\n",
    "    \"\"\"SPair-71k dataset with keypoint annotations for correspondence learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, split='trn', category=None, image_size=224, subset=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.category = category\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.pairs = self._load_pairs()\n",
    "        if subset is not None:\n",
    "            self.pairs = self.pairs[:subset]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(f\"SPair-71k {split} dataset: {len(self.pairs)} pairs loaded\")\n",
    "    \n",
    "    def _load_pairs(self):\n",
    "        pairs = []\n",
    "        layout_dir = os.path.join(self.root_dir, 'Layout', self.split)\n",
    "        \n",
    "        if not os.path.exists(layout_dir):\n",
    "            print(f\"Warning: Layout directory not found: {layout_dir}\")\n",
    "            return pairs\n",
    "        \n",
    "        if self.category:\n",
    "            categories = [self.category]\n",
    "        else:\n",
    "            categories = [d for d in os.listdir(layout_dir) \n",
    "                         if os.path.isdir(os.path.join(layout_dir, d))]\n",
    "        \n",
    "        for cat in categories:\n",
    "            cat_dir = os.path.join(layout_dir, cat)\n",
    "            if not os.path.exists(cat_dir):\n",
    "                continue\n",
    "            \n",
    "            for fname in os.listdir(cat_dir):\n",
    "                if not fname.endswith('.json'):\n",
    "                    continue\n",
    "                \n",
    "                json_path = os.path.join(cat_dir, fname)\n",
    "                try:\n",
    "                    with open(json_path, 'r') as f:\n",
    "                        pair_data = json.load(f)\n",
    "                    \n",
    "                    pair = {\n",
    "                        'category': cat,\n",
    "                        'src_img': pair_data['src_imname'],\n",
    "                        'tgt_img': pair_data['trg_imname'],\n",
    "                        'src_kps': np.array(pair_data['src_kps']).reshape(-1, 2),\n",
    "                        'tgt_kps': np.array(pair_data['trg_kps']).reshape(-1, 2),\n",
    "                        'src_bbox': pair_data.get('src_bndbox', None),\n",
    "                        'tgt_bbox': pair_data.get('trg_bndbox', None),\n",
    "                    }\n",
    "                    pairs.append(pair)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        src_img_path = os.path.join(self.root_dir, 'JPEGImages', \n",
    "                                    pair['category'], pair['src_img'])\n",
    "        tgt_img_path = os.path.join(self.root_dir, 'JPEGImages',\n",
    "                                    pair['category'], pair['tgt_img'])\n",
    "        \n",
    "        src_img_pil = Image.open(src_img_path).convert('RGB')\n",
    "        tgt_img_pil = Image.open(tgt_img_path).convert('RGB')\n",
    "        \n",
    "        src_w, src_h = src_img_pil.size\n",
    "        tgt_w, tgt_h = tgt_img_pil.size\n",
    "        \n",
    "        src_kps = pair['src_kps'].copy().astype(float)\n",
    "        tgt_kps = pair['tgt_kps'].copy().astype(float)\n",
    "        \n",
    "        src_kps[:, 0] *= self.image_size / src_w\n",
    "        src_kps[:, 1] *= self.image_size / src_h\n",
    "        tgt_kps[:, 0] *= self.image_size / tgt_w\n",
    "        tgt_kps[:, 1] *= self.image_size / tgt_h\n",
    "        \n",
    "        src_img = self.transform(src_img_pil)\n",
    "        tgt_img = self.transform(tgt_img_pil)\n",
    "        \n",
    "        if pair['src_bbox'] is not None:\n",
    "            src_bbox = np.array(pair['src_bbox'])\n",
    "            src_bbox[0::2] *= self.image_size / src_w\n",
    "            src_bbox[1::2] *= self.image_size / src_h\n",
    "            src_bbox_wh = np.array([src_bbox[2] - src_bbox[0], src_bbox[3] - src_bbox[1]])\n",
    "        else:\n",
    "            src_bbox_wh = np.array([self.image_size, self.image_size])\n",
    "        \n",
    "        if pair['tgt_bbox'] is not None:\n",
    "            tgt_bbox = np.array(pair['tgt_bbox'])\n",
    "            tgt_bbox[0::2] *= self.image_size / tgt_w\n",
    "            tgt_bbox[1::2] *= self.image_size / tgt_h\n",
    "            tgt_bbox_wh = np.array([tgt_bbox[2] - tgt_bbox[0], tgt_bbox[3] - tgt_bbox[1]])\n",
    "        else:\n",
    "            tgt_bbox_wh = np.array([self.image_size, self.image_size])\n",
    "        \n",
    "        return {\n",
    "            'src_img': src_img,\n",
    "            'tgt_img': tgt_img,\n",
    "            'src_kps': torch.from_numpy(src_kps).float(),\n",
    "            'tgt_kps': torch.from_numpy(tgt_kps).float(),\n",
    "            'src_bbox_wh': torch.from_numpy(src_bbox_wh).float(),\n",
    "            'tgt_bbox_wh': torch.from_numpy(tgt_bbox_wh).float(),\n",
    "            'category': pair['category'],\n",
    "            'pair_id': idx\n",
    "        }\n",
    "\n",
    "\n",
    "def create_spair_dataloaders(root_dir, batch_size=1, num_workers=2, \n",
    "                             train_subset=None, val_subset=None):\n",
    "    train_dataset = SPairDataset(root_dir, split='trn', subset=train_subset)\n",
    "    val_dataset = SPairDataset(root_dir, split='val', subset=val_subset)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                           num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "print(\"✓ SPair-71k dataloader ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c312d",
   "metadata": {},
   "source": [
    "## SPair-71k Dataloader\n",
    "\n",
    "Complete dataloader for SPair-71k with keypoint annotations for finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c25544",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Window soft-argmax for sub-pixel refinement and finetuning utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a55962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION FLAGS ==========\n",
    "# Set these flags to control behavior\n",
    "ENABLE_FINETUNING = False  # Set True to enable light finetuning of last layers\n",
    "USE_SOFT_ARGMAX = False    # Set True to use window soft-argmax instead of argmax\n",
    "\n",
    "# Finetuning hyperparameters (only used if ENABLE_FINETUNING=True)\n",
    "FINETUNE_K_LAYERS = 2      # Number of last transformer blocks to unfreeze {1, 2, 4}\n",
    "FINETUNE_LR = 1e-5         # Learning rate\n",
    "FINETUNE_WD = 1e-4         # Weight decay\n",
    "FINETUNE_EPOCHS = 3        # Number of training epochs\n",
    "FINETUNE_BATCH_SIZE = 1    # Batch size for training\n",
    "FINETUNE_TRAIN_SUBSET = None  # None for full training set, or int for subset\n",
    "\n",
    "# Soft-argmax hyperparameters (only used if USE_SOFT_ARGMAX=True)\n",
    "SOFT_WINDOW = 7            # Window size around peak (odd number: 5, 7, 9)\n",
    "SOFT_TAU = 0.05            # Softmax temperature (lower = sharper)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  ENABLE_FINETUNING = {ENABLE_FINETUNING}\")\n",
    "print(f\"  USE_SOFT_ARGMAX = {USE_SOFT_ARGMAX}\")\n",
    "if ENABLE_FINETUNING:\n",
    "    print(f\"  Finetuning: k={FINETUNE_K_LAYERS}, lr={FINETUNE_LR}, epochs={FINETUNE_EPOCHS}\")\n",
    "if USE_SOFT_ARGMAX:\n",
    "    print(f\"  Soft-argmax: window={SOFT_WINDOW}, tau={SOFT_TAU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9fd00",
   "metadata": {},
   "source": [
    "## Configuration Flags\n",
    "\n",
    "Set these flags to control the pipeline behavior:\n",
    "- `ENABLE_FINETUNING`: Enable light finetuning of last transformer blocks\n",
    "- `USE_SOFT_ARGMAX`: Use window soft-argmax instead of argmax for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Extract and Normalize Common Utility Functions\n",
    "import os, sys, random, numpy as np, torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "def detect_env():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except Exception:\n",
    "        return 'local'\n",
    "\n",
    "ENV = detect_env()\n",
    "print('Environment:', ENV)\n",
    "\n",
    "def get_paths():\n",
    "    if ENV == 'colab':\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        root = '/content/drive/MyDrive/AMLProject'\n",
    "        data_root = os.path.join(root, 'data')\n",
    "        model_root = os.path.join(root, 'models')\n",
    "        out_root = os.path.join(root, 'outputs')\n",
    "    else:\n",
    "        root = os.getcwd()\n",
    "        data_root = os.path.join(root, 'data')\n",
    "        model_root = os.path.join(root, 'models')\n",
    "        out_root = os.path.join(root, 'outputs')\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "    os.makedirs(model_root, exist_ok=True)\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    return root, data_root, model_root, out_root\n",
    "\n",
    "PROJECT_ROOT, DATA_ROOT, MODEL_ROOT, OUTPUT_ROOT = get_paths()\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('DATA_ROOT:', DATA_ROOT)\n",
    "print('MODEL_ROOT:', MODEL_ROOT)\n",
    "print('OUTPUT_ROOT:', OUTPUT_ROOT)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b001c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "DINOv3 repo: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/models/dinov3\n",
      "DINOv3 weight: None\n",
      "Official DINOv3 unavailable: No module named 'dinov3'\n",
      "Loaded timm DINOv2 fallback.\n",
      "Model type: timm DINOv2 ViT-B/14\n",
      "Feature extractor ready. Mock mode: False image_size: 518 patch: 14\n"
     ]
    }
   ],
   "source": [
    "# 3) Model: Auto-detect weights, fallback, and mock outputs\n",
    "import os, sys, torch, timm\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# Fallback paths if previous cell not run\n",
    "if 'PROJECT_ROOT' not in globals() or 'MODEL_ROOT' not in globals():\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    MODEL_ROOT = os.path.join(PROJECT_ROOT, 'models')\n",
    "    OUTPUT_ROOT = os.path.join(PROJECT_ROOT, 'outputs')\n",
    "    os.makedirs(MODEL_ROOT, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "    print('Initialized default paths (previous cell not run).')\n",
    "\n",
    "# Device detection (CUDA, MPS, CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Auto-detect official DINOv3 repo and weights\n",
    "def find_dinov3_repo_and_weights():\n",
    "    candidates_repo = [\n",
    "        Path(MODEL_ROOT) / 'dinov3',\n",
    "        Path(PROJECT_ROOT) / 'models' / 'dinov3',\n",
    "        Path(PROJECT_ROOT) / 'dinov3'\n",
    "    ]\n",
    "    repo_path = next((str(p) for p in candidates_repo if p.exists()), None)\n",
    "    if repo_path and repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "    \n",
    "    candidates_weights = [\n",
    "        Path(MODEL_ROOT) / 'dinov3_vitb16.pth',\n",
    "        Path(MODEL_ROOT) / 'dinov3' / 'dinov3_vitb16.pth',\n",
    "        Path(PROJECT_ROOT) / 'models' / 'dinov3_vitb16.pth',\n",
    "        Path(PROJECT_ROOT) / 'models' / 'dinov3' / 'dinov3_vitb16.pth',\n",
    "        Path(PROJECT_ROOT) / 'checkpoints' / 'dinov3' / 'dinov3_vitb16.pth',\n",
    "        Path(PROJECT_ROOT) / 'weights' / 'dinov3_vitb16.pth'\n",
    "    ]\n",
    "    weight_path = next((str(p) for p in candidates_weights if p.exists()), None)\n",
    "    return repo_path, weight_path\n",
    "\n",
    "repo_path, weight_path = find_dinov3_repo_and_weights()\n",
    "print('DINOv3 repo:', repo_path)\n",
    "print('DINOv3 weight:', weight_path)\n",
    "\n",
    "dinov3_model = None\n",
    "model_type = None\n",
    "\n",
    "# Try official DINOv3 if both repo and weights are present\n",
    "try:\n",
    "    if repo_path:\n",
    "        from dinov3.models.vision_transformer import vit_base\n",
    "        dinov3_model = vit_base(patch_size=16)\n",
    "        if weight_path:\n",
    "            ckpt = torch.load(weight_path, map_location='cpu')\n",
    "            missing, unexpected = dinov3_model.load_state_dict(ckpt, strict=False)\n",
    "            print(f'Loaded DINOv3 weights. Missing={len(missing)} Unexpected={len(unexpected)}')\n",
    "            model_type = 'DINOv3 ViT-B/16 (official)'\n",
    "        else:\n",
    "            print('Weights not found; using mock outputs with uninitialized model.')\n",
    "            model_type = 'DINOv3 ViT-B/16 (no weights, mock)'\n",
    "        dinov3_model.to(device).eval()\n",
    "except Exception as e:\n",
    "    print('Official DINOv3 unavailable:', e)\n",
    "\n",
    "# Fallback: timm DINOv2 variant\n",
    "if dinov3_model is None:\n",
    "    try:\n",
    "        dinov3_model = timm.create_model('vit_base_patch14_dinov2.lvd142m', pretrained=True, num_classes=0)\n",
    "        dinov3_model.to(device).eval()\n",
    "        model_type = 'timm DINOv2 ViT-B/14'\n",
    "        print('Loaded timm DINOv2 fallback.')\n",
    "    except Exception as e:\n",
    "        print('Failed timm fallback:', e)\n",
    "        raise\n",
    "\n",
    "print('Model type:', model_type)\n",
    "\n",
    "def _model_image_size(model):\n",
    "    cfg = getattr(model, 'default_cfg', {})\n",
    "    size = cfg.get('input_size', (3, 224, 224))\n",
    "    if isinstance(size, (list, tuple)) and len(size) == 3:\n",
    "        return size[1]\n",
    "    return 224\n",
    "\n",
    "def _model_patch_size(model):\n",
    "    ps = getattr(getattr(model, 'patch_embed', None), 'patch_size', (14, 14))\n",
    "    if isinstance(ps, (list, tuple)) and len(ps) >= 1:\n",
    "        return ps[0]\n",
    "    return 14\n",
    "\n",
    "# Unified feature extraction with optional mock when weights missing\n",
    "class UnifiedFeatureExtractor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.image_size = _model_image_size(model)\n",
    "        self.patch_size = _model_patch_size(model)\n",
    "        self.feat_dim = getattr(getattr(model, 'num_features', None), 'real', None) or getattr(model, 'num_features', 768)\n",
    "        import torchvision.transforms as transforms\n",
    "        from PIL import Image\n",
    "        self.Image = Image\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(self.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.use_mock = (model_type is not None and 'mock' in model_type)\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        if isinstance(img, self.Image.Image):\n",
    "            pil = img\n",
    "        else:\n",
    "            pil = self.Image.fromarray(img)\n",
    "        return self.transforms(pil).unsqueeze(0).to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract(self, img, normalize=True):\n",
    "        x = self.preprocess(img)\n",
    "        if self.use_mock:\n",
    "            grid = (self.image_size // self.patch_size)\n",
    "            torch.manual_seed(42)\n",
    "            feats = torch.randn(grid*grid, self.feat_dim)\n",
    "            feats = feats.view(1, grid, grid, self.feat_dim)\n",
    "        else:\n",
    "            out = self.model.forward_features(x)\n",
    "            if isinstance(out, dict) and 'x_norm_patchtokens' in out:\n",
    "                tokens = out['x_norm_patchtokens']\n",
    "            elif isinstance(out, dict) and 'x_norm_clstoken' in out and 'x_norm_patchtokens' in out:\n",
    "                tokens = out['x_norm_patchtokens']\n",
    "            else:\n",
    "                tokens = out[:, 1:, :] if out.dim() == 3 else out\n",
    "            grid = int(tokens.shape[1] ** 0.5)\n",
    "            feats = tokens.view(1, grid, grid, self.feat_dim)\n",
    "        if normalize:\n",
    "            feats = F.normalize(feats, p=2, dim=-1)\n",
    "        info = {\n",
    "            'feature_size': (feats.shape[2], feats.shape[1]),\n",
    "            'processed_size': (self.image_size, self.image_size),\n",
    "            'patch_size': self.patch_size,\n",
    "            'feat_dim': self.feat_dim\n",
    "        }\n",
    "        return feats.cpu(), info\n",
    "\n",
    "feature_extractor = UnifiedFeatureExtractor(dinov3_model, device)\n",
    "print('Feature extractor ready. Mock mode:', feature_extractor.use_mock, 'image_size:', feature_extractor.image_size, 'patch:', feature_extractor.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_FINETUNING:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LIGHT FINETUNING ENABLED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if we have real model (not mock)\n",
    "    if dinov3_model is None or feature_extractor.use_mock:\n",
    "        print(\"\\n⚠️  Cannot finetune: model is in mock mode or not loaded\")\n",
    "        print(\"   Please ensure DINOv3 weights are available\")\n",
    "    else:\n",
    "        # Path to SPair-71k\n",
    "        SPAIR_ROOT = os.path.join(DATA_ROOT if 'DATA_ROOT' in globals() else os.path.join(PROJECT_ROOT, 'data'), 'SPair-71k')\n",
    "        \n",
    "        if not os.path.exists(SPAIR_ROOT):\n",
    "            print(f\"\\n⚠️  SPair-71k not found at: {SPAIR_ROOT}\")\n",
    "            print(\"   Download from: http://cvlab.postech.ac.kr/research/SPair-71k/\")\n",
    "        else:\n",
    "            print(f\"\\nLoading SPair-71k from: {SPAIR_ROOT}\")\n",
    "            train_loader, val_loader = create_spair_dataloaders(\n",
    "                SPAIR_ROOT,\n",
    "                batch_size=FINETUNE_BATCH_SIZE,\n",
    "                num_workers=2,\n",
    "                train_subset=FINETUNE_TRAIN_SUBSET,\n",
    "                val_subset=500\n",
    "            )\n",
    "            \n",
    "            # Unfreeze last k blocks\n",
    "            trainable_params = unfreeze_last_k_blocks(dinov3_model, FINETUNE_K_LAYERS, blocks_attr='blocks')\n",
    "            \n",
    "            # Setup optimizer\n",
    "            optimizer = torch.optim.AdamW(trainable_params, lr=FINETUNE_LR, weight_decay=FINETUNE_WD)\n",
    "            \n",
    "            print(f\"\\nFinetuning configuration:\")\n",
    "            print(f\"  Model: {model_type}\")\n",
    "            print(f\"  k={FINETUNE_K_LAYERS} layers\")\n",
    "            print(f\"  lr={FINETUNE_LR}, wd={FINETUNE_WD}\")\n",
    "            print(f\"  epochs={FINETUNE_EPOCHS}\")\n",
    "            print(f\"  train samples: {len(train_loader.dataset)}\")\n",
    "            print(f\"  val samples: {len(val_loader.dataset)}\")\n",
    "            \n",
    "            patch_size = feature_extractor.patch_size  # 14 or 16\n",
    "            \n",
    "            # Training\n",
    "            dinov3_model.train()\n",
    "            best_val_loss = float('inf')\n",
    "            \n",
    "            for epoch in range(FINETUNE_EPOCHS):\n",
    "                epoch_loss = 0.0\n",
    "                num_batches = 0\n",
    "                \n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    src_img = batch['src_img'].to(device)\n",
    "                    tgt_img = batch['tgt_img'].to(device)\n",
    "                    src_kps = batch['src_kps'].to(device)\n",
    "                    tgt_kps = batch['tgt_kps'].to(device)\n",
    "                    \n",
    "                    # Extract features\n",
    "                    src_out = dinov3_model.get_intermediate_layers(src_img, n=1, return_class_token=False)[0]\n",
    "                    tgt_out = dinov3_model.get_intermediate_layers(tgt_img, n=1, return_class_token=False)[0]\n",
    "                    \n",
    "                    B = src_img.size(0)\n",
    "                    num_patches = src_out.size(1)\n",
    "                    grid_size = int(np.sqrt(num_patches))\n",
    "                    \n",
    "                    batch_loss = 0.0\n",
    "                    num_kps = 0\n",
    "                    \n",
    "                    for b in range(B):\n",
    "                        src_f = F.normalize(src_out[b], dim=-1)\n",
    "                        tgt_f = F.normalize(tgt_out[b], dim=-1)\n",
    "                        \n",
    "                        for kp_idx in range(src_kps[b].size(0)):\n",
    "                            src_x = int(src_kps[b][kp_idx, 0].item() / patch_size)\n",
    "                            src_y = int(src_kps[b][kp_idx, 1].item() / patch_size)\n",
    "                            src_x = max(0, min(grid_size - 1, src_x))\n",
    "                            src_y = max(0, min(grid_size - 1, src_y))\n",
    "                            \n",
    "                            src_patch_idx = src_y * grid_size + src_x\n",
    "                            sim = torch.matmul(tgt_f, src_f[src_patch_idx])\n",
    "                            sim_2d = sim.view(grid_size, grid_size)\n",
    "                            \n",
    "                            gt_xy = tgt_kps[b][kp_idx]\n",
    "                            gt_yx = torch.stack([gt_xy[1], gt_xy[0]])\n",
    "                            \n",
    "                            loss = compute_keypoint_loss(\n",
    "                                sim_2d, grid_size, grid_size, gt_yx, patch_size,\n",
    "                                use_soft=True, window=SOFT_WINDOW, tau=SOFT_TAU\n",
    "                            )\n",
    "                            batch_loss += loss\n",
    "                            num_kps += 1\n",
    "                    \n",
    "                    if num_kps > 0:\n",
    "                        batch_loss = batch_loss / num_kps\n",
    "                        optimizer.zero_grad()\n",
    "                        batch_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        epoch_loss += batch_loss.item()\n",
    "                        num_batches += 1\n",
    "                    \n",
    "                    if (batch_idx + 1) % 50 == 0:\n",
    "                        print(f\"  Epoch {epoch+1}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {epoch_loss/max(1,num_batches):.4f}\")\n",
    "                \n",
    "                avg_train_loss = epoch_loss / max(1, num_batches)\n",
    "                \n",
    "                # Validation\n",
    "                dinov3_model.eval()\n",
    "                val_loss = 0.0\n",
    "                val_batches = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch in val_loader:\n",
    "                        src_img = batch['src_img'].to(device)\n",
    "                        tgt_img = batch['tgt_img'].to(device)\n",
    "                        src_kps = batch['src_kps'].to(device)\n",
    "                        tgt_kps = batch['tgt_kps'].to(device)\n",
    "                        \n",
    "                        src_out = dinov3_model.get_intermediate_layers(src_img, n=1, return_class_token=False)[0]\n",
    "                        tgt_out = dinov3_model.get_intermediate_layers(tgt_img, n=1, return_class_token=False)[0]\n",
    "                        \n",
    "                        num_patches = src_out.size(1)\n",
    "                        grid_size = int(np.sqrt(num_patches))\n",
    "                        \n",
    "                        batch_val_loss = 0.0\n",
    "                        num_kps = 0\n",
    "                        \n",
    "                        for b in range(src_img.size(0)):\n",
    "                            src_f = F.normalize(src_out[b], dim=-1)\n",
    "                            tgt_f = F.normalize(tgt_out[b], dim=-1)\n",
    "                            \n",
    "                            for kp_idx in range(src_kps[b].size(0)):\n",
    "                                src_x = int(src_kps[b][kp_idx, 0].item() / patch_size)\n",
    "                                src_y = int(src_kps[b][kp_idx, 1].item() / patch_size)\n",
    "                                src_x = max(0, min(grid_size - 1, src_x))\n",
    "                                src_y = max(0, min(grid_size - 1, src_y))\n",
    "                                \n",
    "                                src_patch_idx = src_y * grid_size + src_x\n",
    "                                sim = torch.matmul(tgt_f, src_f[src_patch_idx])\n",
    "                                sim_2d = sim.view(grid_size, grid_size)\n",
    "                                \n",
    "                                gt_xy = tgt_kps[b][kp_idx]\n",
    "                                gt_yx = torch.stack([gt_xy[1], gt_xy[0]])\n",
    "                                \n",
    "                                loss = compute_keypoint_loss(\n",
    "                                    sim_2d, grid_size, grid_size, gt_yx, patch_size,\n",
    "                                    use_soft=True, window=SOFT_WINDOW, tau=SOFT_TAU\n",
    "                                )\n",
    "                                batch_val_loss += loss\n",
    "                                num_kps += 1\n",
    "                        \n",
    "                        if num_kps > 0:\n",
    "                            val_loss += (batch_val_loss / num_kps).item()\n",
    "                            val_batches += 1\n",
    "                \n",
    "                avg_val_loss = val_loss / max(1, val_batches)\n",
    "                \n",
    "                print(f\"\\nEpoch {epoch+1}/{FINETUNE_EPOCHS}:\")\n",
    "                print(f\"  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "                \n",
    "                # Save best\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    ckpt_path = os.path.join(OUTPUT_ROOT, f'dinov3_finetuned_k{FINETUNE_K_LAYERS}_best.pth')\n",
    "                    torch.save({\n",
    "                        'model_state_dict': dinov3_model.state_dict(),\n",
    "                        'config': {'k': FINETUNE_K_LAYERS, 'lr': FINETUNE_LR}\n",
    "                    }, ckpt_path)\n",
    "                    print(f\"  ✓ Best model saved\")\n",
    "                \n",
    "                dinov3_model.train()\n",
    "            \n",
    "            dinov3_model.eval()\n",
    "            print(f\"\\n✅ Finetuning completed! Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Finetuning disabled. Using pretrained weights only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c62c6",
   "metadata": {},
   "source": [
    "## Light Finetuning (Optional)\n",
    "\n",
    "If `ENABLE_FINETUNING=True`, this section finetunes the last k transformer blocks on SPair-71k with keypoint supervision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matcher, PCK, and visualization ready.\n"
     ]
    }
   ],
   "source": [
    "# 4) Correspondence matching, PCK, and visualization (works with mock)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CorrespondenceMatcher:\n",
    "    def __init__(self, mutual_nn=False, use_soft_argmax=False, soft_window=7, soft_tau=0.05):\n",
    "        self.mutual_nn = mutual_nn\n",
    "        self.use_soft_argmax = use_soft_argmax\n",
    "        self.soft_window = soft_window\n",
    "        self.soft_tau = soft_tau\n",
    "\n",
    "    def match(self, src_feats, tgt_feats):\n",
    "        H, W, D = tgt_feats.shape[1], tgt_feats.shape[2], tgt_feats.shape[3]\n",
    "        src_flat = src_feats.view(-1, D)\n",
    "        tgt_flat = tgt_feats.view(-1, D)\n",
    "        \n",
    "        if self.use_soft_argmax:\n",
    "            # Compute similarity on device\n",
    "            sim = torch.matmul(src_flat, tgt_flat.T)  # [N, H*W]\n",
    "            \n",
    "            # Use window soft-argmax\n",
    "            pred_coords = window_soft_argmax(\n",
    "                sim, H, W, \n",
    "                window=self.soft_window, \n",
    "                tau=self.soft_tau\n",
    "            )  # [N, 2] in (y, x) patch coordinates\n",
    "            \n",
    "            x = pred_coords[:, 1].cpu().numpy()\n",
    "            y = pred_coords[:, 0].cpu().numpy()\n",
    "        else:\n",
    "            # Original argmax approach\n",
    "            sim = src_flat.numpy() @ tgt_flat.numpy().T\n",
    "            best = np.argmax(sim, axis=1)\n",
    "            y = best // W\n",
    "            x = best % W\n",
    "        \n",
    "        return np.stack([x, y], axis=1)\n",
    "\n",
    "def pck(pred_kps, gt_kps, alpha=0.1, img_wh=(224,224)):\n",
    "    if len(pred_kps)==0 or len(gt_kps)==0:\n",
    "        return 0.0\n",
    "    d = np.linalg.norm(pred_kps - gt_kps, axis=1)\n",
    "    norm = np.sqrt(img_wh[0]**2 + img_wh[1]**2)\n",
    "    thr = alpha*norm\n",
    "    return float((d<=thr).mean())\n",
    "\n",
    "def visualize(src_img, tgt_img, src_kps, pred_kps):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "    ax[0].imshow(src_img)\n",
    "    ax[0].scatter(src_kps[:,0], src_kps[:,1], c='r', s=40)\n",
    "    ax[0].set_title('Source')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(tgt_img)\n",
    "    ax[1].scatter(pred_kps[:,0], pred_kps[:,1], c='b', s=40)\n",
    "    ax[1].set_title('Target (Pred)')\n",
    "    ax[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "print(f'Matcher (soft-argmax={'enabled' if USE_SOFT_ARGMAX else 'disabled'}), PCK, and visualization ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19838d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOCK PCK@0.10: 100.00%\n",
      "✓ Sanity check complete (mock).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9IAAAH6CAYAAADr83SsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHzRJREFUeJzt3XtwXVXZ+PF1QkKRm9JAhVJaUVFERVEERQEFFCgCY1FUBFFQUHBkFNEBL+CrctURVARxHBT9Q2BkpEgB5SI6XlBHxZ8D3qfl0mIl4MAglpSzf/MsJiFJT2Of0jZpzuczk7d0751k59R3dr5nr71Wq2mapgAAAACrpGfVDgMAAACENAAAACS5Iw0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpCGtey2224rb3rTm8rs2bPLtGnTyjOf+czyqle9qpx88sleewAYR6vVWqWPH//4x5PqdbzjjjvKGWecURYuXJj6vP/7v/8rO+20U2m328Pbxv6sT3/608trX/vacu2115Z1IX6O+L5DBgcHy3Oe85xy/vnnr5PvD5OVkIa1KC5ye+yxR3nooYfKueeeW374wx+WCy64oLz61a8ul19+udceAMbxi1/8YtTH3Llzy9Oe9rQVtr/sZS+bdCH96U9/OhXSixcvrr8rREz39Iz+Ff3Nb35z/Tl/9rOflQsvvLDcd9995eCDD15nMT1SX19f+dSnPlXPc2BgYJ1/f5gseif6BGAqiwvi9ttvX2644YbS2/vk/7u97W1vq/vWpXgHOd5RHnkeADCZvfKVrxz196222qpG5tjtq+s///lP2XjjjctkEG+0P+MZzyjz5s1bYV+MZhv6meMN+hjZ9tznPrfeFT7ooIPW+XX/7W9/e/nwhz9cvva1r5XTTjttjX99WB+4Iw1rUbxTu+WWW3a8iI18tzmGcEVY77jjjnX494wZM8o73/nOcs8994z6nGc961nlXe961wpfK4Z4xceQGOIWF89vf/vbdQj5tttuW7/u3/72t7r/+uuvL/vuu28dHha/QLzgBS8oZ5111qiv+Zvf/KYccsghZfr06WWjjTYqu+yyS7niiivWyOsCAGtK3KHda6+96rVzk002KS9+8YvrNTVCcqS4Tr7oRS8qP/nJT2qMxvXvmGOOqfvieht3fTfbbLMas+94xzvKr3/963ot/eY3v5m6Psbxb3nLW+p/v+51rxsekj3264z02GOPlW984xvliCOOWOFudCcxtDreVFi0aNEqXfdvvPHGet3ffPPN688dI+NuuummFb5u3OF+6UtfWj83bgR8/vOf7/j9N9xww/LWt761XHLJJaVpmv95vjAVCWlYi+Id43hG+oMf/GD9c+xFfcj73//+8rGPfay8/vWvL/Pnzy+f+cxnauzGhf7+++9f7e9/6qmnlrvuuqtcfPHF5Zprrqm/ZMSFOobGRbwPbY/zGxntt9xyS73I/vvf/67HXH311fXCGhfN8X4RAIB17e9//3sN0IjIH/zgB+XYY48t5513Xjn++ONXOHbJkiXlyCOPrMcvWLCgnHDCCeWRRx6pwRvXvnPOOadGcdwBjmveWKtyfYw7xGeeeeZw5A8NP1/ZneMQvyPEm+9xHqviwQcfrMdHTP+v6/53vvOd8oY3vKFG9Le+9a3688WbAPvvv/+omI7/PvTQQ+ubCd/97nfraxjHXnrppR3PId6YiJD/4x//uErnDFNOA6w1999/f/Oa17wm3qqtH319fc0ee+zRnHXWWc3DDz9cj7nzzjvrvhNOOGHU59522211+2mnnTa8bc6cOc3RRx+9wvfZe++968eQW265pX7uXnvtNeq4+J6bb755Pad2u73S895xxx2bXXbZpRkcHBy1/Y1vfGOzzTbbNI8//vhqvBoA8NTENXCTTTZZ6f64PsW167LLLms22GCD5oEHHhjeF9fJuDbedNNNoz7nwgsvrNuvu+66UduPP/74uv3SSy9NXx+vvPLK+rlxPV4V55xzTj3+vvvuW2Hf0O8I8T0fe+yx+nvDgQceWLfHuY933X/kkUea6dOnNwcffPAKr9NLXvKSZrfddhvetvvuuzczZ85sHn300eFtDz30UP38Tsnw17/+tW6/6KKLVulnhKnGHWlYi/r7+8tPf/rTOjzs7LPPru/0/uUvf6nvGMfQs7jbHO9uh7FDtnfbbbc65LrT0KtVddhhh436+89//vM68Vm8Az9yBs6RYhjYn/70pzqsLSxfvnz4I+5kx7v5f/7zn1f7nABgTfrd735Xh1rHNXeDDTaok2HF41GPP/54veaOtMUWW5R99tln1LZbb7213oU94IADVngOeF1dH2Oisbgux+NgnXz1q1+tP1cMqY7fDeJ6HpN9xfX8f133H3jggXL00UePOt8YlRY/b/x+Enfk4yP+O57PjuHqQ+J1iUnNOom73eHee+9drZ8Z1ndmHYJ1YNddd60fIYZ3xzDuL37xi/UZrhhqFbbZZpsVPm/mzJnDzz+tjrFf81//+lf9c9asWSv9nH/+85/1z4985CP1o5OnMtwcANaUGMa85557luc///l1sq6YSyRC8Fe/+lU58cQTy6OPPjrq+E7X2hgiHUO5xxq7bW1eH+M8I5TjjYBODj/88HLKKafU2I64jWekOx079ucbOud4/ntlIrTj60Zcb7311ivs77QtDAX32NcYuoWQhnUsLpSnn356Del4rmjond54J3ts4MY71CPfnY6L1rJlyzpeuDu9iz32rvPQs1RjJzEbaejrxF3zTjOHhviFBQAm2ve///16N/Wqq64qc+bMGd7++9//vuPxnUZjxZ3sCO+xYompdXV9jK8dE47FzxITpo0V1++hN+THM/bnGzrnL3/5yyud6TzeMBia4Xvszxw6bRsK8JHfA7qNkIa1KOK407vfd9555/Ad56EhZjEZyCte8YrhY2KIVRz38Y9/fHhbvNP+hz/8YdTXimFrMZRsVS5kMXlZzNQdk5DEElydfqGIXwJ22GGHcvvttw9PlgIAk9HQdSxmmR4SjxV//etfX+Wvsffee9dJta677rpy4IEHDm+PCbdW9/o4dD6rerc2Vu0Ymjht5513LmtKTIwWs5DHutYf+MAHVnpcDBmPR8riDYmYZGzobvPDDz9cJy3r5B//+Ef9c6eddlpj5wvrEyENa1HMiBl3meOuc1wkY9hUvEv+hS98oWy66ablpJNOqhfm4447rr5bHEtexEV84cKF5ZOf/GTZbrvtyoc+9KHhr3fUUUfV2Ubjmah4DiqGfcfw8LGzdq5MfM/43u95z3vKfvvtV9773vfWd6Ljua/4xeArX/lKPS7WhYzziPOPZ7djGY145znC/re//W258sor19prBgCrKla7iAiM55k/+tGPlv/+97/loosuqrNar6p4fjhGicX19bOf/Wxdnzmi+oYbbqj7Ry5HtarXx1hmK8TyUDEUO8I0lpOKu9+dDC1h+ctf/nKNhnRc9+P3i/gZ4zxjiHc82xyPesV1P/6M1yvEiiHx3HS8prGEVjxjHrOYxx3yobvPI8W5xvDyWHoMutJEz3YGU9nll1/eHHHEEc0OO+zQbLrppnXW7tmzZzdHHXVUc8cdd4yaPTNm7Hze855Xj9lyyy2bI488srn77rtHfb2Yafvcc89tnv3sZzcbbbRRs+uuuzY333zzSmftjllDO1mwYEE9PmY+3XjjjZuddtqpfv+Rbr/99ubwww9vZsyYUc9p6623bvbZZ5/m4osvXuOvEwCs7qzd11xzTZ2BOq6L2267bXPKKafUGbjHzpod170XvvCFHb/uXXfd1cybN69eqzfbbLPmsMMOq9fK+BpXX331al0fzz///Gb77bevs4ePnf27kz333LOZO3fuCtvjc0888cRxP/d/XfdvvfXW5qCDDqozcMc5x+sUfx97/Pz585udd9652XDDDevvK2effXZz+umnd5y1O8537Gzg0E1a8X8mOuYBAGAyieHbn/jEJ+qEZuNN0rmmfO9736vrUcdos7jTPZnFEPQY5h537eMONnQjIQ0AQFcberQpHsOKibduvvnm8qUvfamG7WWXXbZOziHubcVcJi9/+cuHz2eyeve7310nLv3Rj3400acCE8Yz0gAAdLWNN964Picdc5TE6hizZ8+uS1XGHel1OXFaTJI2f/78OqfKyGezJ5NYhzqW34rZy6GbuSMNAAAACZPzrS4AAACYpIQ0AAAAJAhpAAAASBDSAAAAsDZm7b7ttr9nvi4AMMbuuz9nnbwm/++W27z2APAUvPh1u4+73x1pAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAICQBgAAgLXDHWkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAEBCb+bgbjNt8d2l//r5pe+B+8vg9C3LwAGHlGUzt5vo0wIAAGACCekOWssHy5zzzigzrr6ilJ6e0vS0SqvdlFmXnF+WHnp4WXTKGaXp7Vv3/1oAAABMOCHdQY3o+VeUVmlKaT9eWu0n98X2sPDUz62zfyQAAAAmD89IjzHt3rvqnehW03R8wWJ77I9h3wAAAHQfIT1G/w3X1OHc479qPfXZaQAAALqPkB4jJhaLZ6LHE/vjOAAAALqPkB4jZueOicXG02q363EAAAB0HyE9xsD+B5fSHjG7WCftpi6FBQAAQPcR0mMs23Z2XeKqaXUe3h3bY7/1pAEAALqT5a86iHWiw+h1pNv1TvTSQ55YRxoAAIDuJKQ7aHr76jrRS45+X52dOyYWG+zfqg77dicaAACguwnpcUQ0Lz7mxHX3rwEAsIbcs2RaWXBjfxl4sK/0bzFY5u43UGZts8zrC7AGCGkAgClkcHmrnHnBnHLVtTNKqyeeUmtKu90qF146q8w7aGk57aRFpa93/BVKABifkAYAmEJqRC+YUZrSKk1M8dJ+cgLV2B5OP3nhBJ4hwPrPrN0AAFPEPYun1TvRTbOS1UeaVt0fw74BWH1CGgBgilhwU38dzj2e2B/PTgOw+oQ0AMAUEROLxTPR44n9cRwAq09IAwBMETE798hnojuJ/XEcAKtPSAMATBFz9x2oE4yNJ/bHUlgArD4hDQAwRcyauawucdVqdR7eHdtjv/WkAZ4ay18BAEwhsU50GLuOdNyJnjf3iXWkAXhqhDQAwBTS19vUdaKPPWJJnZ07Jhbrnz5Yh327Ew2wZghpAIApKKL5uKMWT/RpAExJnpEGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQ0Js5GAAAYCq5Z8m0suDG/jLwYF/p32KwzN1voMzaZtlEnxaTnJAGAAC6zuDyVjnzgjnlqmtnlFZPKT09TWm3W+XCS2eVeQctLaedtKj09TYTfZpMUkIaAADoOjWiF8woTWmVpl1qRA+J7eH0kxdO4BkymXlGGgAA6Cr3LJ5W70Q3zZPxPFJsj/0x7Bs6EdIAAEBXWXBTfx3OPZ7YH89OQydCGgAA6CoxsVg8Ez2e2B/HQSdCGgAA6CoxO/fIZ6I7if1xHHQipAEAgK4yd9+BOsHYeGJ/LIUFnQhpAACgq8yauawucdVqdR7eHdtjv/WkWRnLXwEAAF0n1okOY9eRjjvR8+Y+sY40rIyQBgAAuk5fb1PXiT72iCV1du6YWKx/+mAd9u1ONP+LkAYAALpWRPNxRy2e6NNgPeMZaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABJ6MwcDTDXTFt9d+q+fX/oeuL8MTt+yDBxwSFk2c7uJPi0AACYxIQ10pdbywTLnvDPKjKuvKKWnpzQ9rdJqN2XWJeeXpYceXhadckZpevsm+jQBAJiEhDTQlWpEz7+itEpTSvvx0mo/uS+2h4Wnfm7iThAAgEnLM9JA15l27131TnSraTruj+2xP4Z9AwDAWEIa6Dr9N1xTh3OPq6enPjsNAABjCWmg68TEYvFM9HhifxwHAABjCWmg68Ts3DGx2Hha7XY9DgAAxhLSQNcZ2P/gUtojZhfrpN3UpbAAAGAsIQ10nWXbzq5LXDWtzsO7Y3vst540AACdWP4K6EqxTnQYvY50u96JXnrIE+tIAwBAJ0Ia6EpNb19dJ3rJ0e+rs3PHxGKD/VvVYd/uRAMAMB4hDXS1iObFx5w40acBAMB6xDPSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASOjNHEz3mLb47tJ//fzS98D9ZXD6lmXggEPKspnbTfRpAQAATDghzSit5YNlznlnlBlXX1FKT09pelql1W7KrEvOL0sPPbwsOuWM0vT2edUAAICuJaQZpUb0/CtKqzSltB8vrfaT+2J7WHjq57xqAABA1/KMNMOm3XtXvRPdapqOr0psj/0x7BsAAKBbCWmG9d9wTR3OPf7/Ynrqs9MAAADdSkgzLCYWi2eixxP74zgAAIBuJaQZFrNzx8Ri42m12/U4AACAbiWkGTaw/8GltEfMLtZJu6lLYQEAAHQrIc2wZdvOrktcNa3Ow7tje+y3njQAANDNLH/FKLFOdBi9jnS73oleesgT60gDAAB0MyHNKE1vX10nesnR76uzc8fEYoP9W9Vh3+5EAwAACGlWIqJ58TEnen0AAADG8Iw0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJvZmDAQBYP9yzZFpZcGN/GXiwr/RvMVjm7jdQZm2zbKJPC2BKENIAAFPI4PJWOfOCOeWqa2eUVk8pPT1Nabdb5cJLZ5V5By0tp520qPT1NhN9mgDrNSENADCF1IheMKM0pVWadqkRPSS2h9NPXjiBZwiw/vOMNADAFHHP4mn1TnTTPBnPI8X22B/DvgFYfUIaAGCKWHBTfx3OPZ7YH89OA7D6hDQAwBQRE4vFM9Hjif1xHACrT0gDAEwRMTv3yGeiO4n9cRwAq09IAwBMEXP3HagTjI0n9sdSWACsPiENADBFzJq5rC5x1Wp1Ht4d22O/9aQBnhrLXwEATCGxTnQYu4503ImeN/eJdaQBeGqENADAFNLX29R1oo89YkmdnTsmFuufPliHfbsTDbBmCGkAgCkoovm4oxZP9GkATEmekQYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgoTdzMACsS9MW3136r59f+h64vwxO37IMHHBIWTZzO/8IAMCEEtIATDqt5YNlznlnlBlXX1FKT09pelql1W7KrEvOL0sPPbwsOuWM0vT2TfRpAgBdSkgDMOnUiJ5/RWmVppT246XVfnJfbA8LT/3cxJ0gANDVPCMNwKQy7d676p3oVtN03B/bY38M+wYAmAhCGoBJpf+Ga+pw7nH19NRnpwEAJoKQBmBSiYnF4pno8cT+OA4AYCJ4RhqASSVm546JxcbTarfrcQDA5HbPkmllwY39ZeDBvtK/xWCZu99AmbXNsrK+E9IATCoD+x9cZ+ceV7upS2EBAJPT4PJWOfOCOeWqa2eUVk88ldWUdrtVLrx0Vpl30NJy2kmLSl/v+G+cT2aGdgMwqSzbdnZd4qppdR7eHdtjv/WkAWDyOjMiesGM0pRWDejly3vqn/H32B7712dCGoBJJ9aJXnrI4fVi2/RsUNq9vaWJ9aRLq26P/QDA5HTP4mn1TnTTrORN8aZV98ew7/WVod0ATDpNb19dJ3rJ0e+rs3PHxGKD/VvVYd/uRAPA5Lbgpv46nLtpr/yY2B/PTh931OKyPhLSAExaEc2Ljzlxok8DAEgYeLBv+JnolYn9cdz6ytBuAAAA1pj+LQbHjegQ++O49ZWQBgAAYI2Zu+/AuMO6Q+yPpbDWV0IaAACANWbWzGV1iatWq/PyVrE99q/P60l7RhoAAIA16rSTFtU/x64jHXei5819Yh3p9ZmQBgAAYI3q623K6ScvLMcesaTOzh0Ti/VPH6zDvtfnO9FDhDQAAABrxaxtlq23S1yNxzPSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAIAEIQ0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAACUIaAAAAEoQ0AAAAJAhpAAAASBDSAAAAkCCkAQAAIEFIAwAAQIKQBgAAgAQhDQAAAAlCGgAAABKENAAAACQIaQAAAEgQ0gAAAJAgpAEAACBBSAMAAECCkAYAAICEVtM0TeYTAAAAoJu5Iw0AAAAJQhoAAAAShDQAAAAkCGkAAABIENIAAACQIKQBAAAgQUgDAABAgpAGAACABCENAAAAZdX9fxZi/lf7ojfPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5) End-to-end sanity check with mock data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create synthetic images and keypoints\n",
    "src = Image.new('RGB', (224,224), color=(200,200,220))\n",
    "tgt = Image.new('RGB', (224,224), color=(210,190,200))\n",
    "src_kps = np.array([[30,30],[60,120],[150,80],[200,200]], dtype=float)\n",
    "gt_kps  = np.array([[40,40],[70,130],[160,90],[210,210]], dtype=float)  # fake GT\n",
    "\n",
    "# Extract features (mock if weights missing)\n",
    "src_feats, _ = feature_extractor.extract(src)  # (1,H,W,D)\n",
    "tgt_feats, _ = feature_extractor.extract(tgt)\n",
    "src_feats = src_feats[0]\n",
    "tgt_feats = tgt_feats[0]\n",
    "H, W, D = src_feats.shape\n",
    "\n",
    "# Map keypoints to feature grid\n",
    "patch = feature_extractor.patch_size\n",
    "def kps_to_feat(kps):\n",
    "    k = kps.copy()\n",
    "    k[:,0] = np.clip(np.round(k[:,0] / patch), 0, W-1)\n",
    "    k[:,1] = np.clip(np.round(k[:,1] / patch), 0, H-1)\n",
    "    return k.astype(int)\n",
    "\n",
    "src_feat_idx = kps_to_feat(src_kps)\n",
    "tgt_feat_idx_gt = kps_to_feat(gt_kps)\n",
    "\n",
    "# Gather source descriptors at keypoints\n",
    "src_desc = src_feats[src_feat_idx[:,1], src_feat_idx[:,0], :]  # (K,D)\n",
    "\n",
    "# Match only for keypoints with configuration\n",
    "matcher = CorrespondenceMatcher(\n",
    "    mutual_nn=False,\n",
    "    use_soft_argmax=USE_SOFT_ARGMAX,\n",
    "    soft_window=SOFT_WINDOW if USE_SOFT_ARGMAX else 7,\n",
    "    soft_tau=SOFT_TAU if USE_SOFT_ARGMAX else 0.05\n",
    ")\n",
    "pred_feat_coords = matcher.match(src_desc[None, None, ...], tgt_feats[None, ...])\n",
    "pred_feat_coords = pred_feat_coords[:len(src_kps)]  # (K,2)\n",
    "\n",
    "# Map feature coords back to image pixels (center of patch)\n",
    "pred_kps = np.stack([pred_feat_coords[:,0]*patch + patch//2,\n",
    "                      pred_feat_coords[:,1]*patch + patch//2], axis=1).astype(float)\n",
    "\n",
    "# Compute PCK\n",
    "alpha_val = 0.1\n",
    "pck_val = pck(pred_kps, gt_kps, alpha=alpha_val, img_wh=(224,224))\n",
    "print(f'PCK@{alpha_val}: {pck_val:.4f} (mock data)')\n",
    "\n",
    "# Visualize\n",
    "fig = visualize(np.array(src), np.array(tgt), src_kps, pred_kps)\n",
    "plt.show()\n",
    "print('✓ End-to-end sanity check completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10266ca",
   "metadata": {},
   "source": [
    "# Auto-detect weights and fallbacks (reference)\n",
    "- Searches for official DINOv3 repo under: `models/dinov3`, `models/dinov3/`, or `dinov3` at project root (added to `sys.path` when present).\n",
    "- Searches for weights in: `models/dinov3_vitb16.pth`, `models/dinov3/dinov3_vitb16.pth`, `checkpoints/dinov3/dinov3_vitb16.pth`, or `weights/dinov3_vitb16.pth`.\n",
    "- If repo+weights exist: loads official ViT-B/16 with `strict=False`; otherwise keeps model stub and uses mock outputs until weights arrive.\n",
    "- If official path missing: falls back to timm DINOv2 ViT-B/14 (`vit_base_patch14_dinov2.lvd142m`, `num_classes=0`).\n",
    "- Feature extractor infers `image_size` and `patch_size` from the loaded model; mock mode triggers only when weights are absent with the official model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
