{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd13a90c",
   "metadata": {},
   "source": [
    "# Merged AML_Project_Dinov3 and DINOv3_Correspondence\n",
    "This notebook programmatically merges your colleague's `AML_Project_Dinov3.ipynb` with your `DINOv3_Correspondence.ipynb`, unifying environment setup, model loading, dataset handling, correspondence matching, evaluation, and tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e05afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded notebooks:\n",
      " - AML_Project_Dinov3.ipynb cells = 9\n",
      " - DINOv3_Correspondence.ipynb cells = 23\n",
      "\n",
      "--- Structure A ---\n",
      "#1 code [python] :: # Mount Google Drive so the SPair-71k dataset can persist be\n",
      "#2 code [python] :: import os import shutil  # Force delete the broken repositor\n",
      "#3 code [python] :: import os import shutil  # Re-define the project path (just \n",
      "#4 code [python] :: import sys import os import importlib import matplotlib.pypl\n",
      "#5 code [python] :: import sys import os import importlib import matplotlib.pypl\n",
      "#6 code [python] :: # === clone_dinov3_repo === %cd /content/drive/MyDrive/AML_P\n",
      "#7 code [python] :: # === download_dinov3_weights === # Carica i pesi DINOv3 dal\n",
      "#8 code [python] :: # === setup_dinov3_model === import torch import sys import \n",
      "#9 code [python] :: # === integrate_dinov3_with_spair === import torch import to\n",
      "\n",
      "--- Structure B ---\n",
      "#1 markdown [markdown] :: ## Section 1: Environment Setup\n",
      "#2 code [python] :: # Detect environment and configure paths import sys import o\n",
      "#3 code [python] :: # Install dependencies import subprocess  print(\"Installing \n",
      "#4 code [python] :: # Import libraries import torch import torch.nn as nn import\n",
      "#5 markdown [markdown] :: ## Section 2: Load DINOv3 Model  ### DINOv3 Setup Options  *\n",
      "#6 code [python] :: # Clone DINOv3 repository (if using official implementation)\n",
      "#7 code [python] :: # Load DINOv3 model with fallback strategies print(\"Loading \n",
      "#8 markdown [markdown] :: ## Section 3: Dense Feature Extraction  DINOv3 feature extra\n",
      "#9 code [python] :: class DINOv3FeatureExtractor:     \"\"\"     Extract dense spat\n",
      "#10 code [python] :: # Test feature extraction print(\"Testing feature extraction.\n",
      "#11 markdown [markdown] :: ## Section 3 (continued): Correspondence Matching\n",
      "#12 code [python] :: class CorrespondenceMatcher:     \"\"\"Match keypoints using de\n",
      "#13 markdown [markdown] :: ## Section 3 (continued): Evaluation Metrics\n",
      "#14 code [python] :: class PCKEvaluator:     \"\"\"PCK (Percentage of Correct Keypoi\n",
      "#15 markdown [markdown] :: ## Dataset Loaders\n",
      "#16 code [python] :: # Dataset setup def setup_datasets(data_root):     \"\"\"Setup \n",
      "#17 code [python] :: # SPair-71k dataset loader from torch.utils.data import Data\n",
      "#18 markdown [markdown] :: ## Visualization and Evaluation Pipeline\n",
      "#19 code [python] :: def visualize_correspondences(src_img, tgt_img, src_kps, pre\n",
      "#20 code [python] :: def evaluate_on_dataset(dataset, feature_extractor, matcher,\n",
      "#21 markdown [markdown] :: ## Run Evaluation  Uncomment to run evaluation on your datas\n",
      "#22 code [python] :: # Load and evaluate # spair_test = SPairDataset( #     root_\n",
      "#23 markdown [markdown] :: ## Summary  ### DINOv3 Implementation Complete ✓  **Implemen\n",
      "\n",
      "Example diff of first code cells (if present):\n",
      "--- \n",
      "+++ \n",
      "@@ -1,15 +1,36 @@\n",
      "-# Mount Google Drive so the SPair-71k dataset can persist between sessions\n",
      "-from google.colab import drive\n",
      "+# Detect environment and configure paths\n",
      "+import sys\n",
      " import os\n",
      "-drive.mount('/content/drive')\n",
      "+from pathlib import Path\n",
      " \n",
      "-# Create a project folder to keep things organized\n",
      "-project_path = '/content/drive/MyDrive/AML_Project'\n",
      "+# Detect Google Colab\n",
      "+try:\n",
      "+    import google.colab\n",
      "+    IN_COLAB = True\n",
      "+    print(\"✓ Running on Google Colab\")\n",
      "+except:\n",
      "+    IN_COLAB = False\n",
      "+    print(\"✓ Running locally\")\n",
      " \n",
      "-if not os.path.exists(project_path):\n",
      "-    os.makedirs(project_path)\n",
      "-    print(f\"Created project directory at {project_path}\")\n",
      "+# Set up paths\n",
      "+if IN_COLAB:\n",
      "+    from google.colab import drive\n",
      "+    drive.mount('/content/drive')\n",
      "+    PROJECT_ROOT = '/content/AMLProject'\n",
      "+    DATA_ROOT = '/content/drive/MyDrive/AMLProject/data'\n",
      " else:\n",
      "-    print(f\"Project directory already exists at {project_path}\")\n",
      "+    PROJECT_ROOT = os.getcwd()\n",
      "+    DATA_ROOT = os.path.join(PROJECT_ROOT, 'data')\n",
      " \n",
      "-%cd {project_path}\n",
      "+# Create necessary directories\n",
      "+CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, 'checkpoints', 'dinov3')\n",
      "+OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'outputs', 'dinov3')\n"
     ]
    }
   ],
   "source": [
    "# 1) Load and Inspect Notebooks\n",
    "import os, json, difflib, nbformat\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(os.getcwd())\n",
    "A = ROOT / 'AML_Project_Dinov3.ipynb'\n",
    "B = ROOT / 'DINOv3_Correspondence.ipynb'\n",
    "\n",
    "def load_nb(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return nbformat.read(f, as_version=4)\n",
    "\n",
    "nbA = load_nb(A)\n",
    "nbB = load_nb(B)\n",
    "print('Loaded notebooks:')\n",
    "print(' -', A.name, 'cells =', len(nbA['cells']))\n",
    "print(' -', B.name, 'cells =', len(nbB['cells']))\n",
    "\n",
    "def summarize(nb):\n",
    "    lines = []\n",
    "    for i, c in enumerate(nb['cells']):\n",
    "        t = c['cell_type']\n",
    "        lang = c.get('metadata', {}).get('language', 'python' if t=='code' else 'markdown')\n",
    "        head = ''.join(c.get('source',''))[:60].replace('\\n',' ')\n",
    "        lines.append(f\"#{i+1} {t} [{lang}] :: {head}\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print('\\n--- Structure A ---')\n",
    "print(summarize(nbA))\n",
    "print('\\n--- Structure B ---')\n",
    "print(summarize(nbB))\n",
    "\n",
    "def diff_cells(srcA, srcB):\n",
    "    return difflib.unified_diff(srcA.splitlines(), srcB.splitlines(), lineterm='')\n",
    "\n",
    "print('\\nExample diff of first code cells (if present):')\n",
    "idxA = next((i for i,c in enumerate(nbA['cells']) if c['cell_type']=='code'), None)\n",
    "idxB = next((i for i,c in enumerate(nbB['cells']) if c['cell_type']=='code'), None)\n",
    "if idxA is not None and idxB is not None:\n",
    "    srcA = ''.join(nbA['cells'][idxA]['source'])\n",
    "    srcB = ''.join(nbB['cells'][idxB]['source'])\n",
    "    for line in list(diff_cells(srcA, srcB))[:40]:\n",
    "        print(line)\n",
    "else:\n",
    "    print('No code cells to diff.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Extract and Normalize Common Utility Functions\n",
    "import os, sys, random, numpy as np, torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "def detect_env():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except Exception:\n",
    "        return 'local'\n",
    "\n",
    "ENV = detect_env()\n",
    "print('Environment:', ENV)\n",
    "\n",
    "def get_paths():\n",
    "    if ENV == 'colab':\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        root = '/content/drive/MyDrive/AMLProject'\n",
    "        data_root = os.path.join(root, 'data')\n",
    "        model_root = os.path.join(root, 'models')\n",
    "        out_root = os.path.join(root, 'outputs')\n",
    "    else:\n",
    "        root = os.getcwd()\n",
    "        data_root = os.path.join(root, 'data')\n",
    "        model_root = os.path.join(root, 'models')\n",
    "        out_root = os.path.join(root, 'outputs')\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "    os.makedirs(model_root, exist_ok=True)\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    return root, data_root, model_root, out_root\n",
    "\n",
    "PROJECT_ROOT, DATA_ROOT, MODEL_ROOT, OUTPUT_ROOT = get_paths()\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "print('DATA_ROOT:', DATA_ROOT)\n",
    "print('MODEL_ROOT:', MODEL_ROOT)\n",
    "print('OUTPUT_ROOT:', OUTPUT_ROOT)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b001c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "DINOv3 repo: /Users/giuliavarga/Desktop/2. AML/Project/AMLProject/models/dinov3\n",
      "DINOv3 weight: None\n",
      "Official DINOv3 unavailable: No module named 'dinov3'\n",
      "Loaded timm DINOv2 fallback.\n",
      "Model type: timm DINOv2 ViT-B/14\n",
      "Feature extractor ready. Mock mode: False image_size: 518 patch: 14\n"
     ]
    }
   ],
   "source": [
    "# 3) Model: Auto-detect weights, fallback, and mock outputs\n",
    "import os, sys, torch, timm\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "# Fallback paths if previous cell not run\n",
    "if 'PROJECT_ROOT' not in globals() or 'MODEL_ROOT' not in globals():\n",
    "    PROJECT_ROOT = os.getcwd()\n",
    "    MODEL_ROOT = os.path.join(PROJECT_ROOT, 'models')\n",
    "    OUTPUT_ROOT = os.path.join(PROJECT_ROOT, 'outputs')\n",
    "    os.makedirs(MODEL_ROOT, exist_ok=True)\n",
    "    os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "    print('Initialized default paths (previous cell not run).')\n",
    "\n",
    "# Device detection (CUDA, MPS, CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Auto-detect official DINOv3 repo and weights\n",
    "def find_dinov3_repo_and_weights():\n",
    "    candidates_repo = [\n",
    "        Path(MODEL_ROOT) / 'dinov3',\n",
    "        Path(PROJECT_ROOT) / 'models' / 'dinov3',\n",
    "        Path(PROJECT_ROOT) / 'dinov3'\n",
    "    ]\n",
    "    repo_path = next((str(p) for p in candidates_repo if p.exists()), None)\n",
    "    if repo_path and repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "    \n",
    "    candidates_weights = [\n",
    "        Path(MODEL_ROOT) / 'dinov3_vitb16.pth',\n",
    "        Path(MODEL_ROOT) / 'dinov3' / 'dinov3_vitb16.pth',\n",
    "        Path(PROJECT_ROOT) / 'models' / 'dinov3_vitb16.pth',\n",
    "        Path(PROJECT_ROOT) / 'models' / 'dinov3' / 'dinov3_vitb16.pth',\n",
    "        Path(PROJECT_ROOT) / 'checkpoints' / 'dinov3' / 'dinov3_vitb16.pth',\n",
    "        Path(PROJECT_ROOT) / 'weights' / 'dinov3_vitb16.pth'\n",
    "    ]\n",
    "    weight_path = next((str(p) for p in candidates_weights if p.exists()), None)\n",
    "    return repo_path, weight_path\n",
    "\n",
    "repo_path, weight_path = find_dinov3_repo_and_weights()\n",
    "print('DINOv3 repo:', repo_path)\n",
    "print('DINOv3 weight:', weight_path)\n",
    "\n",
    "dinov3_model = None\n",
    "model_type = None\n",
    "\n",
    "# Try official DINOv3 if both repo and weights are present\n",
    "try:\n",
    "    if repo_path:\n",
    "        from dinov3.models.vision_transformer import vit_base\n",
    "        dinov3_model = vit_base(patch_size=16)\n",
    "        if weight_path:\n",
    "            ckpt = torch.load(weight_path, map_location='cpu')\n",
    "            missing, unexpected = dinov3_model.load_state_dict(ckpt, strict=False)\n",
    "            print(f'Loaded DINOv3 weights. Missing={len(missing)} Unexpected={len(unexpected)}')\n",
    "            model_type = 'DINOv3 ViT-B/16 (official)'\n",
    "        else:\n",
    "            print('Weights not found; using mock outputs with uninitialized model.')\n",
    "            model_type = 'DINOv3 ViT-B/16 (no weights, mock)'\n",
    "        dinov3_model.to(device).eval()\n",
    "except Exception as e:\n",
    "    print('Official DINOv3 unavailable:', e)\n",
    "\n",
    "# Fallback: timm DINOv2 variant\n",
    "if dinov3_model is None:\n",
    "    try:\n",
    "        dinov3_model = timm.create_model('vit_base_patch14_dinov2.lvd142m', pretrained=True, num_classes=0)\n",
    "        dinov3_model.to(device).eval()\n",
    "        model_type = 'timm DINOv2 ViT-B/14'\n",
    "        print('Loaded timm DINOv2 fallback.')\n",
    "    except Exception as e:\n",
    "        print('Failed timm fallback:', e)\n",
    "        raise\n",
    "\n",
    "print('Model type:', model_type)\n",
    "\n",
    "def _model_image_size(model):\n",
    "    cfg = getattr(model, 'default_cfg', {})\n",
    "    size = cfg.get('input_size', (3, 224, 224))\n",
    "    if isinstance(size, (list, tuple)) and len(size) == 3:\n",
    "        return size[1]\n",
    "    return 224\n",
    "\n",
    "def _model_patch_size(model):\n",
    "    ps = getattr(getattr(model, 'patch_embed', None), 'patch_size', (14, 14))\n",
    "    if isinstance(ps, (list, tuple)) and len(ps) >= 1:\n",
    "        return ps[0]\n",
    "    return 14\n",
    "\n",
    "# Unified feature extraction with optional mock when weights missing\n",
    "class UnifiedFeatureExtractor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.image_size = _model_image_size(model)\n",
    "        self.patch_size = _model_patch_size(model)\n",
    "        self.feat_dim = getattr(getattr(model, 'num_features', None), 'real', None) or getattr(model, 'num_features', 768)\n",
    "        import torchvision.transforms as transforms\n",
    "        from PIL import Image\n",
    "        self.Image = Image\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(self.image_size, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(self.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.use_mock = (model_type is not None and 'mock' in model_type)\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        if isinstance(img, self.Image.Image):\n",
    "            pil = img\n",
    "        else:\n",
    "            pil = self.Image.fromarray(img)\n",
    "        return self.transforms(pil).unsqueeze(0).to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def extract(self, img, normalize=True):\n",
    "        x = self.preprocess(img)\n",
    "        if self.use_mock:\n",
    "            grid = (self.image_size // self.patch_size)\n",
    "            torch.manual_seed(42)\n",
    "            feats = torch.randn(grid*grid, self.feat_dim)\n",
    "            feats = feats.view(1, grid, grid, self.feat_dim)\n",
    "        else:\n",
    "            out = self.model.forward_features(x)\n",
    "            if isinstance(out, dict) and 'x_norm_patchtokens' in out:\n",
    "                tokens = out['x_norm_patchtokens']\n",
    "            elif isinstance(out, dict) and 'x_norm_clstoken' in out and 'x_norm_patchtokens' in out:\n",
    "                tokens = out['x_norm_patchtokens']\n",
    "            else:\n",
    "                tokens = out[:, 1:, :] if out.dim() == 3 else out\n",
    "            grid = int(tokens.shape[1] ** 0.5)\n",
    "            feats = tokens.view(1, grid, grid, self.feat_dim)\n",
    "        if normalize:\n",
    "            feats = F.normalize(feats, p=2, dim=-1)\n",
    "        info = {\n",
    "            'feature_size': (feats.shape[2], feats.shape[1]),\n",
    "            'processed_size': (self.image_size, self.image_size),\n",
    "            'patch_size': self.patch_size,\n",
    "            'feat_dim': self.feat_dim\n",
    "        }\n",
    "        return feats.cpu(), info\n",
    "\n",
    "feature_extractor = UnifiedFeatureExtractor(dinov3_model, device)\n",
    "print('Feature extractor ready. Mock mode:', feature_extractor.use_mock, 'image_size:', feature_extractor.image_size, 'patch:', feature_extractor.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "168a5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matcher, PCK, and visualization ready.\n"
     ]
    }
   ],
   "source": [
    "# 4) Correspondence matching, PCK, and visualization (works with mock)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CorrespondenceMatcher:\n",
    "    def __init__(self, mutual_nn=False):\n",
    "        self.mutual_nn = mutual_nn\n",
    "\n",
    "    def match(self, src_feats, tgt_feats):\n",
    "        H, W, D = tgt_feats.shape[1], tgt_feats.shape[2], tgt_feats.shape[3]\n",
    "        src_flat = src_feats.view(-1, D).numpy()\n",
    "        tgt_flat = tgt_feats.view(-1, D).numpy()\n",
    "        sim = src_flat @ tgt_flat.T\n",
    "        best = np.argmax(sim, axis=1)\n",
    "        y = best // W\n",
    "        x = best % W\n",
    "        return np.stack([x, y], axis=1)\n",
    "\n",
    "def pck(pred_kps, gt_kps, alpha=0.1, img_wh=(224,224)):\n",
    "    if len(pred_kps)==0 or len(gt_kps)==0:\n",
    "        return 0.0\n",
    "    d = np.linalg.norm(pred_kps - gt_kps, axis=1)\n",
    "    norm = np.sqrt(img_wh[0]**2 + img_wh[1]**2)\n",
    "    thr = alpha*norm\n",
    "    return float((d<=thr).mean())\n",
    "\n",
    "def visualize(src_img, tgt_img, src_kps, pred_kps):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "    ax[0].imshow(src_img)\n",
    "    ax[0].scatter(src_kps[:,0], src_kps[:,1], c='r', s=40)\n",
    "    ax[0].set_title('Source')\n",
    "    ax[0].axis('off')\n",
    "    ax[1].imshow(tgt_img)\n",
    "    ax[1].scatter(pred_kps[:,0], pred_kps[:,1], c='b', s=40)\n",
    "    ax[1].set_title('Target (Pred)')\n",
    "    ax[1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "print('Matcher, PCK, and visualization ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19838d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOCK PCK@0.10: 100.00%\n",
      "✓ Sanity check complete (mock).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5) End-to-end sanity check with mock data\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create synthetic images and keypoints\n",
    "src = Image.new('RGB', (224,224), color=(200,200,220))\n",
    "tgt = Image.new('RGB', (224,224), color=(210,190,200))\n",
    "src_kps = np.array([[30,30],[60,120],[150,80],[200,200]], dtype=float)\n",
    "gt_kps  = np.array([[40,40],[70,130],[160,90],[210,210]], dtype=float)  # fake GT\n",
    "\n",
    "# Extract features (mock if weights missing)\n",
    "src_feats, _ = feature_extractor.extract(src)  # (1,H,W,D)\n",
    "tgt_feats, _ = feature_extractor.extract(tgt)\n",
    "src_feats = src_feats[0]\n",
    "tgt_feats = tgt_feats[0]\n",
    "H, W, D = src_feats.shape\n",
    "\n",
    "# Map keypoints to feature grid\n",
    "patch = feature_extractor.patch_size\n",
    "def kps_to_feat(kps):\n",
    "    k = kps.copy()\n",
    "    k[:,0] = np.clip(np.round(k[:,0] / patch), 0, W-1)\n",
    "    k[:,1] = np.clip(np.round(k[:,1] / patch), 0, H-1)\n",
    "    return k.astype(int)\n",
    "\n",
    "src_feat_idx = kps_to_feat(src_kps)\n",
    "tgt_feat_idx_gt = kps_to_feat(gt_kps)\n",
    "\n",
    "# Gather source descriptors at keypoints\n",
    "src_desc = src_feats[src_feat_idx[:,1], src_feat_idx[:,0], :]  # (K,D)\n",
    "\n",
    "# Match only for keypoints\n",
    "matcher = CorrespondenceMatcher(mutual_nn=False)\n",
    "pred_feat_coords = matcher.match(src_desc[None, None, ...], tgt_feats[None, ...])\n",
    "pred_feat_coords = pred_feat_coords[:len(src_kps)]  # (K,2)\n",
    "\n",
    "# Map feature coords back to image pixels (center of patch)\n",
    "pred_kps = np.stack([pred_feat_coords[:,0]*patch + patch//2,\n",
    "                      pred_feat_coords[:,1]*patch + patch//2], axis=1).astype(float)\n",
    "\n",
    "# Compute PCK\n",
    "score = pck(pred_kps, gt_kps, alpha=0.1, img_wh=(224,224))\n",
    "print(f'MOCK PCK@0.10: {score*100:.2f}%')\n",
    "\n",
    "# Visualize\n",
    "fig = visualize(src, tgt, src_kps, pred_kps)\n",
    "print('✓ Sanity check complete (mock).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10266ca",
   "metadata": {},
   "source": [
    "# Auto-detect weights and fallbacks (reference)\n",
    "- Searches for official DINOv3 repo under: `models/dinov3`, `models/dinov3/`, or `dinov3` at project root (added to `sys.path` when present).\n",
    "- Searches for weights in: `models/dinov3_vitb16.pth`, `models/dinov3/dinov3_vitb16.pth`, `checkpoints/dinov3/dinov3_vitb16.pth`, or `weights/dinov3_vitb16.pth`.\n",
    "- If repo+weights exist: loads official ViT-B/16 with `strict=False`; otherwise keeps model stub and uses mock outputs until weights arrive.\n",
    "- If official path missing: falls back to timm DINOv2 ViT-B/14 (`vit_base_patch14_dinov2.lvd142m`, `num_classes=0`).\n",
    "- Feature extractor infers `image_size` and `patch_size` from the loaded model; mock mode triggers only when weights are absent with the official model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
